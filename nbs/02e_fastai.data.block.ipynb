{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from typing import Any,Callable\n",
    "from inspect import isfunction,ismethod\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.torch_core import *\n",
    "from fastai.data.transforms import *\n",
    "import torchdata.datapipes as dp\n",
    "from collections import deque\n",
    "from fastai.imports import *\n",
    "# Local modules\n",
    "from fastrl.fastai.data.loop.core import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "> High level API to quickly get your data in a `DataLoader`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63392058-2c52-4282-94f7-3c01400845d9",
   "metadata": {},
   "source": [
    "## Transform Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189134b-414d-4a46-a7ce-5d7a75594655",
   "metadata": {},
   "source": [
    "> Note: We will first validate the lower level API on a dqn before making the data block. This is going to be a naive implimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c518d8b6-858c-41a3-a9cc-23fefea2ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _merge_grouper(o):\n",
    "    if isinstance(o, LambdaType): return id(o)\n",
    "    elif isinstance(o, type): return o\n",
    "    elif (isfunction(o) or ismethod(o)): return o.__qualname__\n",
    "    return o.__class__\n",
    "\n",
    "def _merge_tfms(*tfms):\n",
    "    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n",
    "    g = groupby(concat(*tfms), _merge_grouper)\n",
    "    return L(v[-1] for k,v in g.items()).map(instantiate)\n",
    "\n",
    "def _zip(x): return L(x).zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec802048-caeb-4149-adb5-e57592ded1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    def __init__(self, \n",
    "        type_tfms:list=None, # One or more `Transform`s for converting types. These will be re-called if workers!=0 for the dataloader.\n",
    "        item_tfms:list=None, # `ItemTransform`s, applied on an item\n",
    "        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch\n",
    "        cbs:list=None, # `Callback`s for use in dataloaders\n",
    "        dl_type:DataLoader2=None, # Task specific `TfmdDL`, defaults to `TfmdDL`\n",
    "        dls_kwargs:dict=None, # Additional arguments to be passed to `DataLoaders`\n",
    "    ):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  =            L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.cbs        =            L(cbs)\n",
    "        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c441a8d-111c-4045-82d4-91fa086b0d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type_tfms': (#0) [],\n",
       " 'item_tfms': (#0) [],\n",
       " 'batch_tfms': (#0) [],\n",
       " 'cbs': (#0) [],\n",
       " 'dl_type': None,\n",
       " 'dls_kwargs': {}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformBlock().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c25da7-6dae-4f33-82fb-5ae633ded351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def simple_iter_loader_loop(\n",
    "    items:Iterable,\n",
    "    cbs:Optional[List[Callback]]=None,\n",
    "    type_tfms:Optional[Transform]=None,\n",
    "    item_tfms:Optional[Transform]=None,\n",
    "    batch_tfms:Optional[Transform]=None,\n",
    "    bs:int=2,\n",
    "):\n",
    "    type_tfms = ifnone(type_tfms,L())\n",
    "    pipe = dp.map.SequenceWrapper(items)\n",
    "    pipe = TypeTransformLoop(pipe, type_tfms=type_tfms)\n",
    "    pipe = dp.iter.MapToIterConverter(pipe) # Will intialize the gym object, which will be an issue when doing multiproc\n",
    "    pipe = ItemTransformLoop(pipe, item_tfms=ifnone(item_tfms,L()))\n",
    "    pipe = pipe.batch(bs)\n",
    "    pipe = BatchTransformLoop(pipe, batch_tfms=ifnone(batch_tfms,L()))\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b69adc7-2512-450d-9dc3-c12c1d5901ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBlock(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        blocks:List[TransformBlock]=None, # Transform blocks to use \n",
    "        loader_loop:Callable=None,\n",
    "        dl_type=None\n",
    "    ):\n",
    "        store_attr(but='loader_loop')\n",
    "        self.loader_loop = ifnone(loader_loop,default_loader_loop)\n",
    "        blocks = L(self.blocks if blocks is None else blocks)\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        self.type_tfms = blocks.attrgot('type_tfms', L())\n",
    "\n",
    "        self.cbs = blocks.attrgot('cbs', L())\n",
    "        self.item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        for b in blocks:\n",
    "            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n",
    "        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "\n",
    "    def datapipes(\n",
    "        self,\n",
    "        source:Any,\n",
    "        bs=2,\n",
    "        n=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return L(self.loader_loop(\n",
    "            source,\n",
    "            cbs=cbs,\n",
    "            type_tfms=type_tfms,\n",
    "            item_tfms=self.item_tfms,\n",
    "            batch_tfms=self.batch_tfms,\n",
    "            bs=bs,\n",
    "            n=n,\n",
    "            **kwargs\n",
    "        ) for type_tfms,cbs in zip(self.type_tfms,self.cbs))\n",
    "        \n",
    "    def dataloaders(\n",
    "        self,\n",
    "        source:Any,\n",
    "        n_workers=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pipes = self.datapipes(source,**kwargs)\n",
    "        return L(pipes).map(DataLoader2,num_workers=n_workers,**self.dls_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d782b9-cc27-4e7c-a017-d7bd71fcb4b4",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09946787-463d-483f-821d-bd65af92b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3525921f-fe45-49ba-b37f-fbe4de26e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def simple_iter_loader_loop(\n",
    "    items:Iterable,\n",
    "    cbs:Optional[List[Callback]]=None,\n",
    "    type_tfms:Optional[Transform]=None,\n",
    "    item_tfms:Optional[Transform]=None,\n",
    "    batch_tfms:Optional[Transform]=None,\n",
    "    bs:int=2,\n",
    "    n:int=1,\n",
    "):\n",
    "    pipe = dp.map.SequenceWrapper(items)\n",
    "    pipe = TypeTransformLoop(pipe, type_tfms=ifnone(type_tfms,L()))\n",
    "    pipe = dp.map.InMemoryCacheHolder(pipe)\n",
    "    pipe = dp.iter.MapToIterConverter(pipe) # Will intialize the gym object, which will be an issue when doing multiproc\n",
    "    pipe = pipe.cycle(count=n)\n",
    "    pipe = ItemTransformLoop(pipe, item_tfms=ifnone(item_tfms,L())).attach_callbacks(cbs)\n",
    "    pipe = pipe.batch(bs)\n",
    "    pipe = BatchTransformLoop(pipe, batch_tfms=ifnone(batch_tfms,L()))\n",
    "    \n",
    "    if len(cbs)!=0: default_constructor(pipe,cbs)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb74ef4f-e4c8-4362-8db2-dca0a853ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Flatten(dp.iter.IterDataPipe):\n",
    "    \n",
    "    def __init__(self, source_datapipe, **kwargs) -> None:\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @callback_iter\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if not is_listy(o):\n",
    "                raise Exception(f'Expected listy object got {type(o)}\\n{o}')\n",
    "            for oo in o: yield oo\n",
    "            \n",
    "class NStepPipe(dp.iter.IterDataPipe):\n",
    "    \n",
    "    def __init__(self, source_datapipe, n=1, **kwargs) -> None:\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.n = n\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @callback_iter\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for o in self.source_datapipe:\n",
    "            if not type(o)==dict:\n",
    "                raise Exception(f'Expected dict object generated from `make_step` got {type(o)}\\n{o}')\n",
    "        \n",
    "            buffer.append(o)\n",
    "            if not o['done'] and len(buffer)<self.n: continue\n",
    "            \n",
    "            while o['done'] and len(buffer)!=0:\n",
    "                yield tuple(buffer)\n",
    "                buffer.pop(0)\n",
    "                \n",
    "            if not o['done']:\n",
    "                yield tuple(buffer)\n",
    "                buffer.pop(0)\n",
    "                \n",
    "class NSkipPipe(dp.iter.IterDataPipe):\n",
    "    \n",
    "    def __init__(self, source_datapipe, n=1, **kwargs) -> None:\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.n = n\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @callback_iter\n",
    "    def __iter__(self):\n",
    "        skip_idx = 0\n",
    "        for o in self.source_datapipe:\n",
    "            if not type(o)==dict:\n",
    "                raise Exception(f'Expected dict object generated from `make_step` got {type(o)}\\n{o}')\n",
    "            \n",
    "            skip_idx += 1 # Be aware of the ordering here. we want to always show the first step when we can. \n",
    "            if skip_idx%self.n==0 or o['done']: \n",
    "                yield o\n",
    "                if o['done']: skip_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "14877e71-cc13-47a0-9d56-9c16e9849d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NStepCallback(Callback):\n",
    "    \"A list of data pipes that have an associated job.\"\n",
    "    call_on=L(ItemTransformLoop)\n",
    "    exclude_under=L()\n",
    "    \n",
    "    def __init__(self,nsteps=1,nskip=1):\n",
    "        store_attr()\n",
    "        self.pipes = L(partial(NSkipPipe,n=nskip),\n",
    "                       partial(NStepPipe, n=nsteps),\n",
    "                       Flatten\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0a61a9a-a7ce-4960-ba95-ca387c0a5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_step(\n",
    "    state,\n",
    "    next_state,\n",
    "    done,\n",
    "    action,\n",
    "    env_id\n",
    "):\n",
    "    return dict(state=state,next_state=next_state,done=done,action=action,env_id=env_id)\n",
    "\n",
    "class GymTypeTransform(Transform):\n",
    "    def encodes(self,o): return gym.make(o)\n",
    "    \n",
    "class GymStepTransform(Transform):\n",
    "    def encodes(self,o:gym.Env):\n",
    "        if getattr(o,'is_done',True):\n",
    "            state = o.reset(seed=getattr(self,'seed',0))\n",
    "            o.is_done = False\n",
    "        else:\n",
    "            state = o.state\n",
    "        next_state,action,done,_ = o.step(0)\n",
    "        \n",
    "        if done: o.is_done = True\n",
    "        o.state = next_state\n",
    "        \n",
    "        return make_step(state,next_state,done,action,env_id=id(o))\n",
    "    \n",
    "GymTransformBlock = TransformBlock(\n",
    "    type_tfms = GymTypeTransform,\n",
    "    item_tfms = (GymStepTransform,ToTensor),\n",
    "    cbs = NStepCallback(nsteps=3,nskip=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1884a330-79a2-4c18-a97e-00693ebcdb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.NStepCallback object at 0x7f0469d6bd50>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock,\n",
    "    loader_loop=simple_iter_loader_loop\n",
    ")\n",
    "pipes = block.datapipes(['CartPole-v1']*1,n=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7441d041-1654-460c-b2a4-2b360655eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data.graph import traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6df64ef3-89cd-4d1c-8c11-c37383362ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{BatchTransformLoop: {BatcherIterDataPipe: {Flatten: {NStepPipe: {NSkipPipe: {ItemTransformLoop[<__main__.NStepCallback object at 0x7f0469d6bd50>]: {CyclerIterDataPipe: {MapToIterConverterIterDataPipe: {InMemoryCacheHolderMapDataPipe: {TypeTransformLoop: {SequenceWrapperMapDataPipe: {}}}}}}}}}}}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(pipes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02362585-0910-4e6b-8332-b45d4930fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'state': array([ 0.01323574, -0.21745604, -0.04686959,  0.22950698], dtype=float32), 'next_state': array([ 0.00888662, -0.411878  , -0.04227945,  0.50704503], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([ 6.4906175e-04, -6.0637945e-01, -3.2138553e-02,  7.8611010e-01],\n",
      "      dtype=float32), 'next_state': array([-0.01147853, -0.8010455 , -0.01641635,  1.0685112 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([ 6.4906175e-04, -6.0637945e-01, -3.2138553e-02,  7.8611010e-01],\n",
      "      dtype=float32), 'next_state': array([-0.01147853, -0.8010455 , -0.01641635,  1.0685112 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.07124098, -1.3866119 ,  0.06507833,  1.9527254 ], dtype=float32), 'next_state': array([-0.09897321, -1.582362  ,  0.10413284,  2.2648487 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.07124098, -1.3866119 ,  0.06507833,  1.9527254 ], dtype=float32), 'next_state': array([-0.09897321, -1.582362  ,  0.10413284,  2.2648487 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.13062045, -1.7782916 ,  0.14942981,  2.5877106 ], dtype=float32), 'next_state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.07124098, -1.3866119 ,  0.06507833,  1.9527254 ], dtype=float32), 'next_state': array([-0.09897321, -1.582362  ,  0.10413284,  2.2648487 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.13062045, -1.7782916 ,  0.14942981,  2.5877106 ], dtype=float32), 'next_state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'next_state': array([-0.20567098, -2.169928  ,  0.2596264 ,  3.2684884 ], dtype=float32), 'done': True, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.13062045, -1.7782916 ,  0.14942981,  2.5877106 ], dtype=float32), 'next_state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'next_state': array([-0.20567098, -2.169928  ,  0.2596264 ,  3.2684884 ], dtype=float32), 'done': True, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'next_state': array([-0.20567098, -2.169928  ,  0.2596264 ,  3.2684884 ], dtype=float32), 'done': True, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([ 0.01323574, -0.21745604, -0.04686959,  0.22950698], dtype=float32), 'next_state': array([ 0.00888662, -0.411878  , -0.04227945,  0.50704503], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([ 6.4906175e-04, -6.0637945e-01, -3.2138553e-02,  7.8611010e-01],\n",
      "      dtype=float32), 'next_state': array([-0.01147853, -0.8010455 , -0.01641635,  1.0685112 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([ 6.4906175e-04, -6.0637945e-01, -3.2138553e-02,  7.8611010e-01],\n",
      "      dtype=float32), 'next_state': array([-0.01147853, -0.8010455 , -0.01641635,  1.0685112 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.07124098, -1.3866119 ,  0.06507833,  1.9527254 ], dtype=float32), 'next_state': array([-0.09897321, -1.582362  ,  0.10413284,  2.2648487 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.02749944, -0.9959465 ,  0.00495387,  1.3559971 ], dtype=float32), 'next_state': array([-0.04741837, -1.1911303 ,  0.03207381,  1.6502256 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n",
      "[{'state': array([-0.07124098, -1.3866119 ,  0.06507833,  1.9527254 ], dtype=float32), 'next_state': array([-0.09897321, -1.582362  ,  0.10413284,  2.2648487 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}, {'state': array([-0.13062045, -1.7782916 ,  0.14942981,  2.5877106 ], dtype=float32), 'next_state': array([-0.16618629, -1.9742345 ,  0.20118402,  2.9221187 ], dtype=float32), 'done': False, 'action': 1.0, 'env_id': 139656932269648}]\n"
     ]
    }
   ],
   "source": [
    "for i,o in enumerate(pipes[0]):\n",
    "    print(o)\n",
    "    if i>10:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30383a9f-7822-4e49-a23f-131f68a47700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.NStepCallback object at 0x7f0469d6bd50>]\n"
     ]
    }
   ],
   "source": [
    "dls = block.dataloaders(['CartPole-v1']*1,n=10,n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38da3c24-8809-4c68-b25b-5ee37cd9a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'state': tensor([[ 0.0132, -0.2175, -0.0469,  0.2295]]), 'next_state': tensor([[ 0.0089, -0.4119, -0.0423,  0.5070]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}, {'state': tensor([[ 6.4906e-04, -6.0638e-01, -3.2139e-02,  7.8611e-01]]), 'next_state': tensor([[-0.0115, -0.8010, -0.0164,  1.0685]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}]\n",
      "[{'state': tensor([[-0.0275, -0.9959,  0.0050,  1.3560]]), 'next_state': tensor([[-0.0474, -1.1911,  0.0321,  1.6502]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}, {'state': tensor([[ 6.4906e-04, -6.0638e-01, -3.2139e-02,  7.8611e-01]]), 'next_state': tensor([[-0.0115, -0.8010, -0.0164,  1.0685]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}]\n",
      "[{'state': tensor([[-0.0275, -0.9959,  0.0050,  1.3560]]), 'next_state': tensor([[-0.0474, -1.1911,  0.0321,  1.6502]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}, {'state': tensor([[-0.0712, -1.3866,  0.0651,  1.9527]]), 'next_state': tensor([[-0.0990, -1.5824,  0.1041,  2.2648]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}]\n",
      "[{'state': tensor([[-0.0275, -0.9959,  0.0050,  1.3560]]), 'next_state': tensor([[-0.0474, -1.1911,  0.0321,  1.6502]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}, {'state': tensor([[-0.0712, -1.3866,  0.0651,  1.9527]]), 'next_state': tensor([[-0.0990, -1.5824,  0.1041,  2.2648]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}]\n",
      "[{'state': tensor([[-0.1306, -1.7783,  0.1494,  2.5877]]), 'next_state': tensor([[-0.1662, -1.9742,  0.2012,  2.9221]]), 'done': tensor([False]), 'action': tensor([1.], dtype=torch.float64), 'env_id': tensor([139656926889296])}]\n"
     ]
    }
   ],
   "source": [
    "for o in dls[0]:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0492ee1f-b889-4fc7-b9a9-1f44afe956ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|export\n",
    "# @docs\n",
    "# @funcs_kwargs\n",
    "# class DataBlock():\n",
    "#     \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n",
    "#     blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n",
    "#     _methods = 'get_items splitter get_y get_x'.split()\n",
    "#     _msg = \"If you wanted to compose several transforms in your getter don't forget to wrap them in a `Pipeline`.\"\n",
    "#     def __init__(self, \n",
    "#         blocks:list=None, # One or more `TransformBlock`s\n",
    "#         dl_type:DataLoader2=None, # Task specific `TfmdDL`, defaults to `block`'s dl_type or`TfmdDL`\n",
    "#         n_inp:int=None, # Number of inputs\n",
    "#         item_tfms:list=None, # `ItemTransform`s, applied on an item \n",
    "#         batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch\n",
    "#         **kwargs, \n",
    "#     ):\n",
    "#         blocks = L(self.blocks if blocks is None else blocks)\n",
    "#         blocks = L(b() if callable(b) else b for b in blocks)\n",
    "#         self.type_tfms = blocks.attrgot('type_tfms', L())\n",
    "#         self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "#         self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "#         for b in blocks:\n",
    "#             if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "#         if dl_type is not None: self.dl_type = dl_type\n",
    "#         self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n",
    "#         self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "\n",
    "#         self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n",
    "\n",
    "#         if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n",
    "#         self.new(item_tfms, batch_tfms)\n",
    "\n",
    "#     def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n",
    "#         lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n",
    "\n",
    "#     def new(self, \n",
    "#         item_tfms:list=None, # `ItemTransform`s, applied on an item\n",
    "#         batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch \n",
    "#     ):\n",
    "#         self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "#         self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "#         return self\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_columns(cls, \n",
    "#         blocks:list =None, # One or more `TransformBlock`s\n",
    "#         getters:list =None, # Getter functions applied to results of `get_items`\n",
    "#         get_items:callable=None, # A function to get items\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n",
    "#         get_items = _zip if get_items is None else compose(get_items, _zip)\n",
    "#         return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n",
    "\n",
    "#     def datasets(self, \n",
    "#         source, # The data source\n",
    "#         verbose:bool=False, # Show verbose messages\n",
    "#     ) -> Datasets:\n",
    "#         self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "#         items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "#         splits = (self.splitter or RandomSplitter())(items)\n",
    "#         pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "#         return default_loader_loop(items=source)\n",
    "\n",
    "#     def dataloaders(self, \n",
    "#         source, # The data source\n",
    "#         path:str='.', # Data source and default `Learner` path \n",
    "#         verbose:bool=False, # Show verbose messages\n",
    "#         **kwargs\n",
    "#     ) -> DataLoaders:\n",
    "#         dsets = self.datasets(source, verbose=verbose)\n",
    "#         kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "#         return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n",
    "\n",
    "#     _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n",
    "#                  datasets=\"Create a `Datasets` object from `source`\",\n",
    "#                  dataloaders=\"Create a `DataLoaders` object from `source`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbdev.cli import *\n",
    "    make_readme()\n",
    "    notebook2script(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3e5f5-5451-4a88-9f95-f2d4ec06e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
