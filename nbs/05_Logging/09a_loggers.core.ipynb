{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Optional,List,Any,Iterable,Union\n",
    "from collections import deque\n",
    "from multiprocessing import Queue\n",
    "from queue import Empty\n",
    "import logging\n",
    "# Third party libs\n",
    "from fastcore.all import add_docs,merge,ifnone\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import master_bar,progress_bar\n",
    "from torchdata.dataloader2.graph import find_dps,traverse_dps,list_dps\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from IPython.display import display,HTML\n",
    "import numpy as np\n",
    "# Local modules\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.core import Record,StepTypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Utilities used for handling log messages and display over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "def not_record(data:Any):\n",
    "    \"Intended for use with dp.iter.Filter\"\n",
    "    return type(data)!=Record\n",
    "def is_record(data:Any):\n",
    "    \"Intended for use with dp.iter.Filter\"\n",
    "    return type(data)==Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7dbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.all import test_eq\n",
    "from fastrl.core import test_in,test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pipe = dp.iter.IterableWrapper([\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1))\n",
    "]).filter(not_record)\n",
    "\n",
    "test_eq(len(list(input_pipe)),5)\n",
    "test_in(torch.ones((1,1)),list(input_pipe))\n",
    "test_out(Record('loss',0.5),list(input_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3287207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_RECORD_CATCH_LIST = []\n",
    "\n",
    "class RecordCatchBufferOverflow(Exception):\n",
    "    def __init__(self,msg, *args, **kwargs):\n",
    "        msg=f\"\"\"_RECORD_CATCH_QUEUE got larger than {msg}.\n",
    "        \n",
    "        Make sure that `RecordDumper` or `dump_records` is being called\n",
    "        at some point in the pipeline, in the current process. Reference\n",
    "        documentation for examples.\n",
    "        \"\"\"\n",
    "        super().__init__(msg, *args, **kwargs)\n",
    "\n",
    "\n",
    "def _clear_record_catch_list():\n",
    "    while _RECORD_CATCH_LIST:\n",
    "        yield _RECORD_CATCH_LIST.pop(0)\n",
    "\n",
    "@functional_datapipe(\"catch_records\")\n",
    "class RecordCatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_datapipe,\n",
    "            # Max size of _RECORD_CATCH_LIST before raising in exception.\n",
    "            # Important to avoid memory leaks, and indicates that `dump_records`\n",
    "            # is not being called or used.\n",
    "            buffer_size=1000,\n",
    "            # If True, instead of appending to _RECORD_CATCH_LIST, \n",
    "            #  drop the record so it does not continue thorugh the \n",
    "            # pipeline.\n",
    "            drop:bool=False\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer_size = buffer_size\n",
    "        self.drop = drop\n",
    "        if _RECORD_CATCH_LIST and not self.drop:\n",
    "            _logger.debug(\n",
    "                \"Clearing _RECORD_CATCH_LIST since it is not empty: %s elements\",\n",
    "                len(_RECORD_CATCH_LIST)\n",
    "            )\n",
    "            _RECORD_CATCH_LIST.clear()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                if not self.drop:                   \n",
    "                    _RECORD_CATCH_LIST.append(o)\n",
    "                    if len(_RECORD_CATCH_LIST) > self.buffer_size:\n",
    "                        raise RecordCatchBufferOverflow(self.buffer_size)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "@functional_datapipe(\"dump_records\")\n",
    "class RecordDumper(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_datapipe=None\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.source_datapipe is None:\n",
    "            yield from _clear_record_catch_list()\n",
    "        else:\n",
    "            for o in self.source_datapipe:\n",
    "                yield from _clear_record_catch_list()\n",
    "                yield o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import ExceptionExpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd450f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RecordCatcher DataPipe\n",
    "input_pipe_with_catcher = dp.iter.IterableWrapper([\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss', 0.5),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss', 0.5),\n",
    "    torch.ones((1,1))\n",
    "])\n",
    "input_pipe_with_catcher = RecordCatcher(\n",
    "    input_pipe_with_catcher, \n",
    "    buffer_size=1000\n",
    ")\n",
    "\n",
    "# Check if BufferOverflow is raised when exceeded\n",
    "large_buffer = [Record('loss', 0.5) for _ in range(1001)]\n",
    "catcher_large = RecordCatcher(dp.iter.IterableWrapper(large_buffer), buffer_size=1000)\n",
    "with ExceptionExpected(RecordCatchBufferOverflow): \n",
    "    list(catcher_large)\n",
    "\n",
    "# Test the RecordDumper DataPipe\n",
    "for _ in _clear_record_catch_list():pass\n",
    "input_pipe_with_dumper = RecordDumper(input_pipe_with_catcher)\n",
    "dumped_records = list(input_pipe_with_dumper)\n",
    "print(dumped_records)\n",
    "test_eq(len(dumped_records), 7)  # 5 tensors + 2 records\n",
    "test_eq(dumped_records.count(Record('loss', 0.5)), 2)\n",
    "test_eq(dumped_records.count(torch.ones((1,1))), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(object):\n",
    "    debug:bool\n",
    "    buffer:list\n",
    "    source_datapipe:dp.iter.IterDataPipe\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while self.buffer: yield self.buffer.pop(0)\n",
    "    \n",
    "    # def reset(self):\n",
    "        # Note: trying to decide if this is really needed.\n",
    "        # if self.debug:\n",
    "        #     print(self,' resetting buffer.')\n",
    "        # if self._snapshot_state!=_SnapshotState.Restored:\n",
    "        #     self.buffer = []\n",
    "        \n",
    "add_docs(\n",
    "LoggerBase,\n",
    "\"\"\"The `LoggerBase` class is an iterface for datapipes that also collect `Record` objects\n",
    "for logging purposes.\n",
    "\"\"\",\n",
    "dequeue=\"Empties the `self.buffer` yielding each of its contents.\"\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9c084-75c7-44ce-a45c-b17adf411545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "\n",
    "traverse_dps(logger_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fceafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(object):\n",
    "    debug:bool=False\n",
    "    title:Optional[str] = None\n",
    "    main_buffers:Optional[List] = None        \n",
    "\n",
    "    def enqueue_title(self):\n",
    "        \"Sends a empty `Record` to tell all the `LoggerBase`s of the `LogCollector's` existance.\"\n",
    "        record = Record(self.title,None)\n",
    "        if self.main_buffers is not None:\n",
    "            for q in self.main_buffers: \n",
    "                q.append(record)\n",
    "        return record\n",
    "    \n",
    "    def enqueue_value(\n",
    "        self,\n",
    "        value:Any\n",
    "    ):\n",
    "        \"Sends a `Record` with `value` to all `LoggerBase`s\"\n",
    "        record = Record(self.title,value)\n",
    "        if self.main_buffers is not None:\n",
    "            for q in self.main_buffers:\n",
    "                q.append(Record(self.title,value))\n",
    "        return record\n",
    "\n",
    "    def reset(self):\n",
    "        if self.main_buffers is None:\n",
    "            if self.debug: print(f'Resetting {self}')\n",
    "            logger_bases = list_dps(traverse_dps(self))\n",
    "            logger_bases = [o for o in logger_bases if isinstance(o,LoggerBase)]\n",
    "            self.main_buffers = [o.buffer for o in logger_bases]\n",
    "            self.enqueue_title()\n",
    "\n",
    "add_docs(\n",
    "LogCollector,\n",
    "\"\"\"`LogCollector` specifically manages finding and attaching itself to\n",
    "`LoggerBase`s found earlier in the pipeline.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(LoggerBase,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            yield o\n",
    "\n",
    "class B(LogCollector,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            self.reset()\n",
    "            self.enqueue_value(o)\n",
    "            yield o\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "collector = B(logger_base)\n",
    "\n",
    "# Collect data from collector to trigger the enqueue methods\n",
    "data_collected = list(collector)\n",
    "\n",
    "# Check if data is passed through\n",
    "test_eq(data_collected, [1, 2, 3, 4])\n",
    "\n",
    "# Check if logger_base has received records\n",
    "all_records = logger_base.buffer\n",
    "\n",
    "# Check if titles and values are recorded correctly\n",
    "expected_records = [\n",
    "    Record(name=None, value=None),  # The title is recorded first\n",
    "    Record(name=None, value=1),\n",
    "    # Record(title=None, value=None),  # The title is recorded every time before the value\n",
    "    Record(name=None, value=2),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=3),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=4)\n",
    "]\n",
    "\n",
    "test_eq(all_records, expected_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpochCollector(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "    title:str='epoch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            # Epochs is the number of times we iterate, and exhaust `source_datapipe`.\n",
    "            # This is expected behavior of more traditional dataset iteration where\n",
    "            # an epoch is a single full run through of a dataset.\n",
    "            epochs:int=0\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_buffers is None:\n",
    "            yield Record(self.title,None)\n",
    "        for i in range(self.epochs):\n",
    "            # self.reset() \n",
    "            self.epoch = i\n",
    "            yield from self.source_datapipe\n",
    "            yield Record(self.title,self.epoch)\n",
    "            \n",
    "add_docs(\n",
    "EpochCollector,\n",
    "\"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "\n",
    "# Create an EpocherCollector with 3 epochs and two loggers\n",
    "epochs = 3\n",
    "collector = EpochCollector(source_datapipe=source_datapipe, epochs=epochs)\n",
    "collector = collector.catch_records()\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each epoch\n",
    "test_eq(data, [1, 2, 3, 4, 5]*epochs)\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                self.buffer.append(o)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "loggers = RecordDumper()\n",
    "dp1,dp2 = loggers.fork(2)\n",
    "\n",
    "logger1 = A(dp1)\n",
    "logger2 = A(dp2)\n",
    "for _ in logger1: pass\n",
    "for _ in logger2: pass\n",
    "\n",
    "# Test whether the epoch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], [None]+list(range(epochs)))\n",
    "test_eq([record.value for record in logger2.buffer], [None]+list(range(epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(dp.iter.IterDataPipe, LogCollector):\n",
    "    title:str='batch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.batches = (\n",
    "            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)\n",
    "        )\n",
    "        self.batch = 0\n",
    "        \n",
    "    def batch_on_pipe_get_batches(self,batch_on_pipe):\n",
    "        pipe = find_dp(traverse_dps(self.source_datapipe),batch_on_pipe)\n",
    "        if hasattr(pipe,'batches'):\n",
    "            return pipe.batches\n",
    "        elif hasattr(pipe,'limit'):\n",
    "            return pipe.limit\n",
    "        else:\n",
    "            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')\n",
    "\n",
    "    def __iter__(self): \n",
    "        self.batch = 0\n",
    "        if self.main_buffers is None:\n",
    "            yield Record(self.title,None)\n",
    "        for data in self.source_datapipe: \n",
    "            yield data\n",
    "            if not_record(data):\n",
    "                record = Record(self.title,self.batch)\n",
    "                self.batch += 1\n",
    "                yield record\n",
    "            if self.batch>=self.batches: \n",
    "                break\n",
    "\n",
    "add_docs(\n",
    "BatchCollector,\n",
    "\"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\",\n",
    "batch_on_pipe_get_batches=\"Gets the number of batches from `batch_on_pipe`\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "\n",
    "# Create an EpocherCollector with 3 epochs and two loggers\n",
    "batches = 3\n",
    "collector = BatchCollector(source_datapipe=source_datapipe, batches=batches)\n",
    "collector = collector.catch_records()\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each batch\n",
    "test_eq(data, list(source_datapipe)[:batches])\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                self.buffer.append(o)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "loggers = RecordDumper()\n",
    "dp1,dp2 = loggers.fork(2)\n",
    "\n",
    "logger1 = A(dp1)\n",
    "logger2 = A(dp2)\n",
    "for _ in logger1: pass\n",
    "for _ in logger2: pass\n",
    "\n",
    "# Test whether the batch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], [None]+list(range(0, batches)))\n",
    "test_eq([record.value for record in logger2.buffer], [None]+list(range(0, batches)))\n",
    "\n",
    "# Test behavior with batch_on_pipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "class B(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,batches):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.batches = batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.source_datapipe\n",
    "\n",
    "source_datapipe = B(source_datapipe,batches=4)\n",
    "collector_with_pipe = BatchCollector(source_datapipe=source_datapipe, batch_on_pipe=B)\n",
    "test_eq(collector_with_pipe.batches, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#TODO(josiahls): Put this in a different module, maybe in jupyter visualizers\n",
    "# so logger.core doesn't force the user to need tqdm or ipython installed.\n",
    "class ProgressBarLogger(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # An iterable that yields `Any` data, which will be passed through,\n",
    "            # of `Record`  objects\n",
    "            source_datapipe:Iterable[Union[Any,Record]], \n",
    "            # For automatic pipe attaching, we can designate which pipe this should be\n",
    "            # referneced for information on which epoch we are on\n",
    "            epoch_on_pipe:dp.iter.IterDataPipe=EpochCollector,\n",
    "            # For automatic pipe attaching, we can designate which pipe this should be\n",
    "            # referneced for information on which batch we are on\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=BatchCollector,\n",
    "            # Whether to close the progress bars at end of iter\n",
    "            close_bars:bool = False\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        self.close_bars = close_bars\n",
    "        \n",
    "        self.metrics_df = pd.DataFrame()\n",
    "        self.current_row = pd.Series()\n",
    "        self._table_ref = None\n",
    "\n",
    "    def update_dataframe(self):\n",
    "        new_df = pd.DataFrame([self.current_row])\n",
    "        self.metrics_df = pd.concat([self.metrics_df, new_df], axis=0, ignore_index=True).fillna(0)\n",
    "        # Display without index and keep the progress bars persistent\n",
    "        html_str = self.metrics_df.to_html(index=False)\n",
    "\n",
    "        # Check if the table is being displayed for the first time\n",
    "        if self._table_ref is None:\n",
    "            self._table_ref = display(HTML(html_str),display_id=True)\n",
    "        else:\n",
    "            self._table_ref.update(HTML(html_str))\n",
    "\n",
    "    def __iter__(self):\n",
    "        epocher = find_dp(traverse_dps(self),self.epoch_on_pipe)\n",
    "        batcher = find_dp(traverse_dps(self),self.batch_on_pipe)\n",
    "        master_pbar = tqdm(total=epocher.epochs, desc=\"Epochs\", position=0, leave=False)\n",
    "        batch_pbar = tqdm(total=batcher.batches, desc=\"Batches\", position=1, leave=False)\n",
    "        \n",
    "        for data in self.source_datapipe:\n",
    "            if is_record(data):\n",
    "                self.current_row[data.name] = data.value\n",
    "                if data.name == \"batch\" and data.value is not None:\n",
    "                    batch_pbar.update(1)\n",
    "                if data.name == \"epoch\" and data.value is not None:\n",
    "                    self.update_dataframe()\n",
    "                    self.current_row = pd.Series()  # Reset for next epoch\n",
    "                    master_pbar.update(1)\n",
    "                    batch_pbar.reset()\n",
    "            else:\n",
    "                yield data\n",
    "        if self.close_bars:\n",
    "            batch_pbar.close()\n",
    "            master_pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf81b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "logging.basicConfig(level='WARNING')\n",
    "_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e374e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper(list(range(10)))\n",
    "pipe = pipe.map(lambda o:(sleep(0.2),o)[1])\n",
    "pipe = BatchCollector(pipe,batches=10)\n",
    "pipe = EpochCollector(pipe,epochs=3)\n",
    "pipe = ProgressBarLogger(source_datapipe=pipe)\n",
    "list(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(dp.iter.IterDataPipe, LogCollector):\n",
    "    title:str='reward'\n",
    "\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def make_record(self,step:StepTypes.types):\n",
    "        reward = step.reward.detach().numpy()\n",
    "        if len(reward.shape)!=0:\n",
    "            reward = reward[0]\n",
    "        return Record(self.title,reward)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            if not_record(steps):\n",
    "                if isinstance(steps,dp.DataChunk):\n",
    "                    for step in steps:\n",
    "                        yield self.make_record(step)\n",
    "                else:\n",
    "                    yield self.make_record(steps)\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.core import SimpleStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper([SimpleStep(reward=torch.ones(1)) for _ in range(5)])\n",
    "pipe = BatchCollector(pipe,batches=4)\n",
    "pipe = EpochCollector(pipe,epochs=3)\n",
    "pipe = RewardCollector(pipe)\n",
    "pipe = ProgressBarLogger(source_datapipe=pipe)\n",
    "list(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpisodeCollector(dp.iter.IterDataPipe):\n",
    "    title:str='episode'\n",
    "\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "    \n",
    "    def make_episode(self,step): \n",
    "        try:\n",
    "            v = step.episode_n.cpu().detach().numpy()\n",
    "            if len(v.shape)==1: \n",
    "                v = v[0]\n",
    "            return Record(self.title,v)\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting episode_n which is unexpected: \\n{step}')\n",
    "            raise\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            if not_record(steps):\n",
    "                if isinstance(steps,dp.DataChunk):\n",
    "                    for step in steps:\n",
    "                        yield self.make_episode(step)\n",
    "                else:\n",
    "                    yield self.make_episode(steps)\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "EpisodeCollector,\n",
    "\"\"\"Collects the `episode_n` field from steps.\"\"\",\n",
    "make_episode=\"Moves the `episode_n` tensor to numpy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e96b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RollingTerminatedRewardCollector(dp.iter.IterDataPipe):\n",
    "    title:str='rolling_reward'\n",
    "\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         rolling_length:int=100\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.rolling_rewards = deque([],maxlen=rolling_length)\n",
    "        \n",
    "    def step2terminated(self,step): return bool(step.terminated)\n",
    "\n",
    "    def make_reward(self,step): \n",
    "        try:\n",
    "            v = step.total_reward.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return float(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting reward which is unexpected: \\n{step}')\n",
    "            raise\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            if not_record(steps):\n",
    "                if isinstance(steps,dp.DataChunk):\n",
    "                    for step in steps:\n",
    "                        if self.step2terminated(step):\n",
    "                            self.rolling_rewards.append(self.make_reward(step))\n",
    "                            yield Record(self.title,np.average(self.rolling_rewards))\n",
    "                elif self.step2terminated(steps):\n",
    "                    self.rolling_rewards.append(self.make_reward(steps))\n",
    "                    yield Record(self.title,np.average(self.rolling_rewards))\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "RollingTerminatedRewardCollector,\n",
    "\"\"\"Collects the `total_reward` field from steps if `terminated` is true and \n",
    "logs a rolling average of size `rolling_length`.\"\"\",\n",
    "make_reward=\"Moves the `total_reward` tensor to numpy.\",\n",
    "step2terminated=\"Casts the `terminated` field in steps to a bool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d858f-afa2-4601-8195-e2d513375b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2.dataloader2 import DataLoader2\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "# from fastrl.data.dataloader2 import *\n",
    "# import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "import gymnasium as gym\n",
    "# from fastrl.pipes.map.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkPipe(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "            self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "          for o in self.source_datapipe:\n",
    "                assert isinstance(o,SimpleStep),f'Got: {o} , {type(o)}'\n",
    "                yield o\n",
    "\n",
    "class Printer(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "            self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "          for o in self.source_datapipe:\n",
    "                yield o\n",
    "\n",
    "envs = ['CartPole-v1'] * 10\n",
    "\n",
    "\n",
    "pipe = dp.iter.IterableWrapper(envs)\n",
    "pipe = pipe.map(gym.make)\n",
    "pipe = pipe.pickleable_in_memory_cache()\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "pipe = EpisodeCollector(pipe)\n",
    "pipe = RollingTerminatedRewardCollector(pipe)\n",
    "pipe = RewardCollector(pipe).catch_records()\n",
    "pipe = pipe.header(limit=10)\n",
    "# pipe = Printer(pipe)\n",
    "\n",
    "pipe = WorkPipe(pipe)\n",
    "\n",
    "pipe = BatchCollector(pipe,batch_on_pipe=dp.iter.Header)\n",
    "pipe = EpochCollector(pipe,epochs=5).dump_records()\n",
    "pipe = ProgressBarLogger(pipe,batch_on_pipe=BatchCollector)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "steps = list(pipe)\n",
    "len(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac4294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
