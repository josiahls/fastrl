{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Optional,List,Any,Iterable\n",
    "from collections import deque\n",
    "# Third party libs\n",
    "from fastcore.all import add_docs,merge,ifnone\n",
    "# from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import master_bar,progress_bar\n",
    "from torchdata.dataloader2.graph import find_dps,traverse_dps,list_dps\n",
    "# from torch.utils.data.datapipes._hook_iterator import _SnapshotState\n",
    "import numpy as np\n",
    "# Local modules\n",
    "# from fastrl.core import *\n",
    "from fastrl.pipes.core import find_dp,find_dps\n",
    "from fastrl.core import StepType,Record\n",
    "# from fastrl.torch_core import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Utilities used for handling log messages and display over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def not_record(data:Any):\n",
    "    \"Intended for use with dp.iter.Filter\"\n",
    "    return type(data)!=Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7dbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.all import test_eq\n",
    "from fastrl.core import test_in,test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa5e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pipe = dp.iter.IterableWrapper([\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1))\n",
    "]).filter(not_record)\n",
    "\n",
    "test_eq(len(list(input_pipe)),5)\n",
    "test_in(torch.ones((1,1)),list(input_pipe))\n",
    "test_out(Record('loss',0.5),list(input_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(object):\n",
    "    debug:bool\n",
    "    buffer:list\n",
    "    source_datapipe:dp.iter.IterDataPipe\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while self.buffer: yield self.buffer.pop(0)\n",
    "    \n",
    "    # def reset(self):\n",
    "        # Note: trying to decide if this is really needed.\n",
    "        # if self.debug:\n",
    "        #     print(self,' resetting buffer.')\n",
    "        # if self._snapshot_state!=_SnapshotState.Restored:\n",
    "        #     self.buffer = []\n",
    "        \n",
    "add_docs(\n",
    "    LoggerBase,\n",
    "    \"\"\"The `LoggerBase` class is an iterface for datapipes that also collect `Record` objects\n",
    "    for logging purposes.\n",
    "    \"\"\",\n",
    "    dequeue=\"Empties the `self.buffer` yielding each of its contents.\"\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c9c084-75c7-44ce-a45c-b17adf411545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{140355346068912: (A, {})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "\n",
    "traverse_dps(logger_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(object):\n",
    "    debug:bool=False\n",
    "    title:Optional[str] = None\n",
    "    main_buffers:Optional[List] = None        \n",
    "\n",
    "    def enqueue_title(self):\n",
    "        \"Sends a empty `Record` to tell all the `LoggerBase`s of the `LogCollector's` existance.\"\n",
    "        for q in self.main_buffers: \n",
    "            q.append(Record(self.title,None))\n",
    "    \n",
    "    def enqueue_value(\n",
    "        self,\n",
    "        value:Any\n",
    "    ):\n",
    "        \"Sends a `Record` with `value` to all `LoggerBase`s\"\n",
    "        for q in self.main_buffers:\n",
    "            q.append(Record(self.title,value))\n",
    "\n",
    "    def reset(self):\n",
    "        if self.main_buffers is None:\n",
    "            if self.debug: print(f'Resetting {self}')\n",
    "            logger_bases = list_dps(traverse_dps(self))\n",
    "            logger_bases = [o for o in logger_bases if isinstance(o,LoggerBase)]\n",
    "            self.main_buffers = [o.buffer for o in logger_bases]\n",
    "            self.enqueue_title()\n",
    "\n",
    "add_docs(\n",
    "LogCollector,\n",
    "\"\"\"`LogCollector` specifically manages finding and attaching itself to\n",
    "`LoggerBase`s found earlier in the pipeline.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eec7cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(LoggerBase,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            yield o\n",
    "\n",
    "class B(LogCollector,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            self.reset()\n",
    "            self.enqueue_value(o)\n",
    "            yield o\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "collector = B(logger_base)\n",
    "\n",
    "# Collect data from collector to trigger the enqueue methods\n",
    "data_collected = list(collector)\n",
    "\n",
    "# Check if data is passed through\n",
    "test_eq(data_collected, [1, 2, 3, 4])\n",
    "\n",
    "# Check if logger_base has received records\n",
    "records = logger_base.buffer\n",
    "\n",
    "# Check if titles and values are recorded correctly\n",
    "expected_records = [\n",
    "    Record(name=None, value=None),  # The title is recorded first\n",
    "    Record(name=None, value=1),\n",
    "    # Record(title=None, value=None),  # The title is recorded every time before the value\n",
    "    Record(name=None, value=2),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=3),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=4)\n",
    "]\n",
    "\n",
    "test_eq(records, expected_records)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bb15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpocherCollector(dp.iter.IterDataPipe,LogCollector):\n",
    "    debug:bool=False\n",
    "    title:str='epoch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            # Epochs is the number of times we iterate, and exhaust `source_datapipe`.\n",
    "            # This is expected behavior of more traditional dataset iteration where\n",
    "            # an epoch is a single full run through of a dataset.\n",
    "            epochs:int=0\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        for i in range(self.epochs):\n",
    "            self.reset() \n",
    "            self.epoch = i\n",
    "            self.enqueue_value(self.epoch)\n",
    "            yield from self.source_datapipe\n",
    "            \n",
    "add_docs(\n",
    "EpocherCollector,\n",
    "\"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f8d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.source_datapipe\n",
    "\n",
    "logger1 = A([])\n",
    "logger2 = A([])\n",
    "\n",
    "source_datapipe = source_datapipe.concat(logger1,logger2)\n",
    "\n",
    "# Create an EpocherCollector with 3 epochs and two loggers\n",
    "epochs = 3\n",
    "collector = EpocherCollector(source_datapipe=source_datapipe, epochs=epochs)\n",
    "collector.main_buffers = [logger1.buffer, logger2.buffer]\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each epoch\n",
    "test_eq(data, [1, 2, 3, 4, 5]*epochs)\n",
    "\n",
    "# Test whether the epoch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], list(range(epochs)))\n",
    "test_eq([record.value for record in logger2.buffer], list(range(epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e725d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(LogCollector):\n",
    "    title:str='batch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.batches = (\n",
    "            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)\n",
    "        )\n",
    "        self.batch = 0\n",
    "        \n",
    "    def batch_on_pipe_get_batches(self,batch_on_pipe):\n",
    "        pipe = find_dp(traverse_dps(self.source_datapipe),batch_on_pipe)\n",
    "        if hasattr(pipe,'batches'):\n",
    "            return pipe.batches\n",
    "        elif hasattr(pipe,'limit'):\n",
    "            return pipe.limit\n",
    "        else:\n",
    "            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')\n",
    "\n",
    "    def __iter__(self): \n",
    "        self.batch = 0\n",
    "        for batch,record in enumerate(self.source_datapipe): \n",
    "            yield record\n",
    "            if type(record)!=Record:\n",
    "                self.batch += 1\n",
    "                self.enqueue_value(self.batch)\n",
    "            if self.batch>=self.batches: \n",
    "                break\n",
    "\n",
    "add_docs(\n",
    "BatchCollector,\n",
    "\"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\",\n",
    "batch_on_pipe_get_batches=\"Gets the number of batches from `batch_on_pipe`\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da2ab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.source_datapipe\n",
    "        \n",
    "logger1 = A([])\n",
    "logger2 = A([])\n",
    "\n",
    "# Create a BatchCollector with 3 batches and two loggers\n",
    "batches = 3\n",
    "collector = BatchCollector(source_datapipe=source_datapipe, batches=batches)\n",
    "collector.main_buffers = [logger1.buffer, logger2.buffer]\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each batch\n",
    "test_eq(data, source_datapipe[:batches])\n",
    "\n",
    "# Test whether the batch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], list(range(1, batches+1)))\n",
    "test_eq([record.value for record in logger2.buffer], list(range(1, batches+1)))\n",
    "\n",
    "# Test behavior with batch_on_pipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "class B(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,batches):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.batches = batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.source_datapipe\n",
    "\n",
    "source_datapipe = B(source_datapipe,batches=4)\n",
    "collector_with_pipe = BatchCollector(source_datapipe=source_datapipe, batch_on_pipe=B)\n",
    "test_eq(collector_with_pipe.batches, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(LoggerBase):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "                 # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "                 # first be able to reference its queues.\n",
    "                 source_datapipe=None, \n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which epoch we are on\n",
    "                 epoch_on_pipe:dp.iter.IterDataPipe=EpochCollector,\n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which batch we are on\n",
    "                 batch_on_pipe:dp.iter.IterDataPipe=BatchCollector\n",
    "                ):\n",
    "        super().__init__(source_datapipe=source_datapipe)\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        epocher = find_dp(traverse_dps(self),self.epoch_on_pipe)\n",
    "        batcher = find_dp(traverse_dps(self),self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        i = 0\n",
    "        for record in self.source_datapipe:\n",
    "            if self.filter_record(record):\n",
    "                self.buffer.append(record)\n",
    "                # We only want to start setting up logging when the data loader starts producing \n",
    "                # real data.\n",
    "                continue\n",
    "   \n",
    "            if i==0:\n",
    "                self.attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "                if self.debug: print('Got initial values: ',self.attached_collectors)\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                pbar.update(0)\n",
    "                    \n",
    "            attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "            if self.debug: print('Got running values: ',self.attached_collectors)\n",
    "\n",
    "            if attached_collectors:\n",
    "                self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "            \n",
    "            if 'batch' in attached_collectors: \n",
    "                pbar.update(attached_collectors['batch'])\n",
    "                \n",
    "            if 'epoch' in attached_collectors:\n",
    "                mbar.update(attached_collectors['epoch'])\n",
    "                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "                \n",
    "            i+=1  \n",
    "            yield record\n",
    "\n",
    "        attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "\n",
    "        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e374e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_pipe = dp.iter.IterableWrapper(range(10, 100))\n",
    "batch_pipe = dp.iter.IterableWrapper(range(10, 100))\n",
    "\n",
    "pbl = ProgressBarLogger(source_datapipe=None, epoch_on_pipe=epoch_pipe, batch_on_pipe=batch_pipe)\n",
    "\n",
    "test_eq(pbl.epoch_on_pipe, epoch_pipe)\n",
    "test_eq(pbl.batch_on_pipe, batch_pipe)\n",
    "test_eq(pbl.attached_collectors, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(LogCollector):\n",
    "    title:str='reward'\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            # if i==0: self.push_title('reward')\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_buffers: q.append(Record('reward',step.reward.detach().numpy()))\n",
    "            else:\n",
    "                for q in self.main_buffers: q.append(Record('reward',steps.reward.detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8aca2e-de22-43d2-b606-d2f0554502df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b456fac-9fdd-40a8-868a-5b97117c252a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpisodeCollector(LogCollector):\n",
    "    title:str='episode'\n",
    "    \n",
    "    def episode_detach(self,step): \n",
    "        try:\n",
    "            v = step.episode_n.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return int(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting episode_n which is unexpected: \\n{step}')\n",
    "            raise\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            # if i==0: self.push_title('episode')\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_buffers: q.append(Record('episode',self.episode_detach(step)))\n",
    "            else:\n",
    "                for q in self.main_buffers: q.append(Record('episode',self.episode_detach(steps)))\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "EpisodeCollector,\n",
    "\"\"\"Collects the `episode_n` field from steps.\"\"\",\n",
    "episode_detach=\"Moves the `episode_n` tensor to numpy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e96b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RollingTerminatedRewardCollector(LogCollector):\n",
    "    debug:bool=False\n",
    "    title:str='rolling_reward'\n",
    "\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         rolling_length:int=100\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.rolling_rewards = deque([],maxlen=rolling_length)\n",
    "        \n",
    "    def step2terminated(self,step): return bool(step.terminated)\n",
    "\n",
    "    def reward_detach(self,step): \n",
    "        try:\n",
    "            v = step.total_reward.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return float(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting reward which is unexpected: \\n{step}')\n",
    "            raise\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            if self.debug: print(f'RollingTerminatedRewardCollector: ',steps)\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    if self.step2terminated(step):\n",
    "                        self.rolling_rewards.append(self.reward_detach(step))\n",
    "                        for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))\n",
    "            elif self.step2terminated(steps):\n",
    "                self.rolling_rewards.append(self.reward_detach(steps))\n",
    "                for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "RollingTerminatedRewardCollector,\n",
    "\"\"\"Collects the `total_reward` field from steps if `terminated` is true and \n",
    "logs a rolling average of size `rolling_length`.\"\"\",\n",
    "reward_detach=\"Moves the `total_reward` tensor to numpy.\",\n",
    "step2terminated=\"Casts the `terminated` field in steps to a bool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d858f-afa2-4601-8195-e2d513375b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2.dataloader2 import DataLoader2\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "# from fastrl.data.dataloader2 import *\n",
    "# import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "import gymnasium as gym\n",
    "# from fastrl.pipes.map.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastrl_user/fastrl/fastrl/envs/gym.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state=torch.tensor(step.next_state),\n"
     ]
    }
   ],
   "source": [
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "logger_base = ProgressBarLogger(batch_on_pipe=BatchCollector,epoch_on_pipe=EpocherCollector)\n",
    "\n",
    "pipe = dp.iter.IterableWrapper(envs)\n",
    "pipe = pipe.map(gym.make)\n",
    "pipe = pipe.pickleable_in_memory_cache()\n",
    "pipe = LoggerBasePassThrough(pipe,[logger_base])\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "pipe = RewardCollector(pipe)\n",
    "# pipe = InputInjester(pipe)\n",
    "# pipe = TestSync(pipe)\n",
    "pipe = pipe.header(limit=10)\n",
    "\n",
    "\n",
    "pipe = BatchCollector(pipe,batch_on_pipe=dp.iter.Header)\n",
    "pipe = EpocherCollector(pipe,epochs=5)\n",
    "pipe = logger_base.connect_source_datapipe(pipe)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "steps = list(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d76a55-09d6-4360-932e-0f5f9a36aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = DataLoader2(\n",
    "#     pipe,\n",
    "#     reading_service=PrototypeMultiProcessingReadingService(\n",
    "#         num_workers = 1,\n",
    "#         protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "#         protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "#         pipe_type = item_input_pipe_type,\n",
    "#         eventloop = SpawnProcessForDataPipeline\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # dl = logger_base.connect_source_datapipe(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c2a7-155d-4ccd-be2b-0aaa2492b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|export\n",
    "# class ActionPublish(dp.iter.IterDataPipe):\n",
    "#     def __init__(self,\n",
    "#             source_datapipe, # Pretend this is in the middle of a learner training segment\n",
    "#             dls\n",
    "#         ):\n",
    "#         self.source_datapipe = source_datapipe\n",
    "#         self.dls = dls\n",
    "#         self.protocol_clients = []\n",
    "#         self._expect_response = []\n",
    "#         self.initialized = False\n",
    "        \n",
    "#     def __iter__(self): \n",
    "#         for step in self.source_datapipe:\n",
    "#             if not self.initialized:\n",
    "#                 for dl in self.dls:\n",
    "#                     # dataloader.IterableWrapperIterDataPipe._IterateQueueDataPipes,[QueueWrappers]\n",
    "#                     for q_wrapper in dl.datapipe.iterable.datapipes:\n",
    "#                         self.protocol_clients.append(q_wrapper.protocol)\n",
    "#                         self._expect_response.append(False)\n",
    "#                 self.initialized = True\n",
    "            \n",
    "#             if isinstance(step,StepType):\n",
    "#                 for i,client in enumerate(self.protocol_clients):\n",
    "#                     if self._expect_response[i]: \n",
    "#                         client.get_response_input_item()\n",
    "#                     else:\n",
    "#                         client.request_input_item(\n",
    "#                             'action_augmentation',value=100\n",
    "#                         )\n",
    "\n",
    "#             yield step\n",
    "#         self.protocol_clients = []\n",
    "#         self._expect_response = []\n",
    "# add_docs(\n",
    "#     ActionPublish,\n",
    "#     \"\"\"Publishes an action augmentation to the dataloader.\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7e6af-18a4-47c8-8b96-1f50b3725f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|hide\n",
    "# #|eval: false\n",
    "# learn_pipe = ActionPublish(dl,[dl])\n",
    "\n",
    "# for o in learn_pipe:pass\n",
    "#     # print('Final Output',o)\n",
    "\n",
    "# # for i,o in enumerate(dl):\n",
    "# #     learn_pipe.source_datapipe.append(o)\n",
    "    \n",
    "# #     if i==0: print(dl.datapipe)\n",
    "# #     print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca97251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CacheLoggerBase(LoggerBase):\n",
    "    \"Short lived logger base meant to dump logs\"\n",
    "    def reset(self):\n",
    "        # This logger will be exhausted frequently if used in an agent.\n",
    "        # We need to get the buffer alive so we dont lose reference\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        print('Iterating through buffer of len: ',len(self.buffer))\n",
    "        yield from self.buffer\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac4294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
