{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Optional,List,Any,Iterable\n",
    "from collections import deque\n",
    "from multiprocessing import Queue\n",
    "from queue import Empty\n",
    "import logging\n",
    "# Third party libs\n",
    "from fastcore.all import add_docs,merge,ifnone\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import master_bar,progress_bar\n",
    "from torchdata.dataloader2.graph import find_dps,traverse_dps,list_dps\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "import numpy as np\n",
    "# Local modules\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.core import Record,StepType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Utilities used for handling log messages and display over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b464f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "def not_record(data:Any):\n",
    "    \"Intended for use with dp.iter.Filter\"\n",
    "    return type(data)!=Record\n",
    "def is_record(data:Any):\n",
    "    \"Intended for use with dp.iter.Filter\"\n",
    "    return type(data)==Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7dbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.all import test_eq\n",
    "from fastrl.core import test_in,test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa5e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pipe = dp.iter.IterableWrapper([\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss',0.5),\n",
    "    torch.ones((1,1))\n",
    "]).filter(not_record)\n",
    "\n",
    "test_eq(len(list(input_pipe)),5)\n",
    "test_in(torch.ones((1,1)),list(input_pipe))\n",
    "test_out(Record('loss',0.5),list(input_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3287207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_RECORD_CATCH_LIST = []\n",
    "\n",
    "class RecordCatchBufferOverflow(Exception):\n",
    "    def __init__(self,msg, *args, **kwargs):\n",
    "        msg=f\"\"\"_RECORD_CATCH_QUEUE got larger than {msg}.\n",
    "        \n",
    "        Make sure that `RecordDumper` or `dump_records` is being called\n",
    "        at some point in the pipeline, in the current process. Reference\n",
    "        documentation for examples.\n",
    "        \"\"\"\n",
    "        super().__init__(msg, *args, **kwargs)\n",
    "\n",
    "\n",
    "def _clear_record_catch_list():\n",
    "    while _RECORD_CATCH_LIST:\n",
    "        yield _RECORD_CATCH_LIST.pop(0)\n",
    "\n",
    "@functional_datapipe(\"catch_records\")\n",
    "class RecordCatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_datapipe,\n",
    "            buffer_size=1000,\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer_size = buffer_size\n",
    "        if _RECORD_CATCH_LIST:\n",
    "            _logger.warning(\n",
    "                \"Clearing _RECORD_CATCH_LIST since it is not empty: %s elements\",\n",
    "                len(_RECORD_CATCH_LIST)\n",
    "            )\n",
    "            _RECORD_CATCH_LIST.clear()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                _RECORD_CATCH_LIST.append(o)\n",
    "                if len(_RECORD_CATCH_LIST) > self.buffer_size:\n",
    "                    raise RecordCatchBufferOverflow(self.buffer_size)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "@functional_datapipe(\"dump_records\")\n",
    "class RecordDumper(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_datapipe=None\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.source_datapipe is None:\n",
    "            yield from _clear_record_catch_list()\n",
    "        else:\n",
    "            for o in self.source_datapipe:\n",
    "                yield from _clear_record_catch_list()\n",
    "                yield o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b52d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import ExceptionExpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd450f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.]]), tensor([[1.]]), tensor([[1.]]), Record(name='loss', value=0.5), tensor([[1.]]), Record(name='loss', value=0.5), tensor([[1.]])]\n"
     ]
    }
   ],
   "source": [
    "# Test the RecordCatcher DataPipe\n",
    "input_pipe_with_catcher = dp.iter.IterableWrapper([\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss', 0.5),\n",
    "    torch.ones((1,1)),\n",
    "    Record('loss', 0.5),\n",
    "    torch.ones((1,1))\n",
    "])\n",
    "input_pipe_with_catcher = RecordCatcher(\n",
    "    input_pipe_with_catcher, \n",
    "    buffer_size=1000\n",
    ")\n",
    "\n",
    "# Check if BufferOverflow is raised when exceeded\n",
    "large_buffer = [Record('loss', 0.5) for _ in range(1001)]\n",
    "catcher_large = RecordCatcher(dp.iter.IterableWrapper(large_buffer), buffer_size=1000)\n",
    "with ExceptionExpected(RecordCatchBufferOverflow): \n",
    "    list(catcher_large)\n",
    "\n",
    "# Test the RecordDumper DataPipe\n",
    "for _ in _clear_record_catch_list():pass\n",
    "input_pipe_with_dumper = RecordDumper(input_pipe_with_catcher)\n",
    "dumped_records = list(input_pipe_with_dumper)\n",
    "print(dumped_records)\n",
    "test_eq(len(dumped_records), 7)  # 5 tensors + 2 records\n",
    "test_eq(dumped_records.count(Record('loss', 0.5)), 2)\n",
    "test_eq(dumped_records.count(torch.ones((1,1))), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(object):\n",
    "    debug:bool\n",
    "    buffer:list\n",
    "    source_datapipe:dp.iter.IterDataPipe\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while self.buffer: yield self.buffer.pop(0)\n",
    "    \n",
    "    # def reset(self):\n",
    "        # Note: trying to decide if this is really needed.\n",
    "        # if self.debug:\n",
    "        #     print(self,' resetting buffer.')\n",
    "        # if self._snapshot_state!=_SnapshotState.Restored:\n",
    "        #     self.buffer = []\n",
    "        \n",
    "add_docs(\n",
    "LoggerBase,\n",
    "\"\"\"The `LoggerBase` class is an iterface for datapipes that also collect `Record` objects\n",
    "for logging purposes.\n",
    "\"\"\",\n",
    "dequeue=\"Empties the `self.buffer` yielding each of its contents.\"\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c9c084-75c7-44ce-a45c-b17adf411545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{140132222481888: (A, {})}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "\n",
    "traverse_dps(logger_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35df43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(object):\n",
    "    debug:bool=False\n",
    "    title:Optional[str] = None\n",
    "    main_buffers:Optional[List] = None        \n",
    "\n",
    "    def enqueue_title(self):\n",
    "        \"Sends a empty `Record` to tell all the `LoggerBase`s of the `LogCollector's` existance.\"\n",
    "        record = Record(self.title,None)\n",
    "        if self.main_buffers is not None:\n",
    "            for q in self.main_buffers: \n",
    "                q.append(record)\n",
    "        return record\n",
    "    \n",
    "    def enqueue_value(\n",
    "        self,\n",
    "        value:Any\n",
    "    ):\n",
    "        \"Sends a `Record` with `value` to all `LoggerBase`s\"\n",
    "        record = Record(self.title,value)\n",
    "        if self.main_buffers is not None:\n",
    "            for q in self.main_buffers:\n",
    "                q.append(Record(self.title,value))\n",
    "        return record\n",
    "\n",
    "    def reset(self):\n",
    "        if self.main_buffers is None:\n",
    "            if self.debug: print(f'Resetting {self}')\n",
    "            logger_bases = list_dps(traverse_dps(self))\n",
    "            logger_bases = [o for o in logger_bases if isinstance(o,LoggerBase)]\n",
    "            self.main_buffers = [o.buffer for o in logger_bases]\n",
    "            self.enqueue_title()\n",
    "\n",
    "add_docs(\n",
    "LogCollector,\n",
    "\"\"\"`LogCollector` specifically manages finding and attaching itself to\n",
    "`LoggerBase`s found earlier in the pipeline.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec7cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(LoggerBase,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            yield o\n",
    "\n",
    "class B(LogCollector,dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            self.reset()\n",
    "            self.enqueue_value(o)\n",
    "            yield o\n",
    "\n",
    "logger_base = A([1,2,3,4])\n",
    "collector = B(logger_base)\n",
    "\n",
    "# Collect data from collector to trigger the enqueue methods\n",
    "data_collected = list(collector)\n",
    "\n",
    "# Check if data is passed through\n",
    "test_eq(data_collected, [1, 2, 3, 4])\n",
    "\n",
    "# Check if logger_base has received records\n",
    "all_records = logger_base.buffer\n",
    "\n",
    "# Check if titles and values are recorded correctly\n",
    "expected_records = [\n",
    "    Record(name=None, value=None),  # The title is recorded first\n",
    "    Record(name=None, value=1),\n",
    "    # Record(title=None, value=None),  # The title is recorded every time before the value\n",
    "    Record(name=None, value=2),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=3),\n",
    "    # Record(title=None, value=None),\n",
    "    Record(name=None, value=4)\n",
    "]\n",
    "\n",
    "test_eq(all_records, expected_records)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpochCollector(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "    title:str='epoch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            # Epochs is the number of times we iterate, and exhaust `source_datapipe`.\n",
    "            # This is expected behavior of more traditional dataset iteration where\n",
    "            # an epoch is a single full run through of a dataset.\n",
    "            epochs:int=0\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_buffers is None:\n",
    "            yield Record(self.title,None)\n",
    "        for i in range(self.epochs):\n",
    "            self.reset() \n",
    "            self.epoch = i\n",
    "            yield from self.source_datapipe\n",
    "            yield Record(self.title,self.epoch)\n",
    "            \n",
    "add_docs(\n",
    "EpochCollector,\n",
    "\"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f8d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "\n",
    "# Create an EpocherCollector with 3 epochs and two loggers\n",
    "epochs = 3\n",
    "collector = EpochCollector(source_datapipe=source_datapipe, epochs=epochs)\n",
    "collector = collector.catch_records()\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each epoch\n",
    "test_eq(data, [1, 2, 3, 4, 5]*epochs)\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                self.buffer.append(o)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "loggers = RecordDumper()\n",
    "dp1,dp2 = loggers.fork(2)\n",
    "\n",
    "logger1 = A(dp1)\n",
    "logger2 = A(dp2)\n",
    "for _ in logger1: pass\n",
    "for _ in logger2: pass\n",
    "\n",
    "# Test whether the epoch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], [None]+list(range(epochs)))\n",
    "test_eq([record.value for record in logger2.buffer], [None]+list(range(epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e725d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(dp.iter.IterDataPipe, LogCollector):\n",
    "    title:str='batch'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.iteration_started = False\n",
    "        self.batches = (\n",
    "            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)\n",
    "        )\n",
    "        self.batch = 0\n",
    "        \n",
    "    def batch_on_pipe_get_batches(self,batch_on_pipe):\n",
    "        pipe = find_dp(traverse_dps(self.source_datapipe),batch_on_pipe)\n",
    "        if hasattr(pipe,'batches'):\n",
    "            return pipe.batches\n",
    "        elif hasattr(pipe,'limit'):\n",
    "            return pipe.limit\n",
    "        else:\n",
    "            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')\n",
    "\n",
    "    def __iter__(self): \n",
    "        self.batch = 0\n",
    "        if self.main_buffers is None:\n",
    "            yield Record(self.title,None)\n",
    "        for data in self.source_datapipe: \n",
    "            yield data\n",
    "            if type(data)!=Record:\n",
    "                record = Record(self.title,self.batch)\n",
    "                self.batch += 1\n",
    "                yield record\n",
    "            if self.batch>=self.batches: \n",
    "                break\n",
    "\n",
    "add_docs(\n",
    "BatchCollector,\n",
    "\"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\",\n",
    "batch_on_pipe_get_batches=\"Gets the number of batches from `batch_on_pipe`\",\n",
    "reset=\"Grabs buffers from all logger bases in the pipeline.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2ab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mock source_datapipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "\n",
    "# Create an EpocherCollector with 3 epochs and two loggers\n",
    "batches = 3\n",
    "collector = BatchCollector(source_datapipe=source_datapipe, batches=batches)\n",
    "collector = collector.catch_records()\n",
    "\n",
    "# Define a function to collect data from the collector\n",
    "def collect_data(collector):\n",
    "    return [item for item in collector]\n",
    "\n",
    "# Collect data\n",
    "data = collect_data(collector)\n",
    "\n",
    "# Test whether the source_datapipe data was yielded correctly for each batch\n",
    "test_eq(data, list(source_datapipe)[:batches])\n",
    "\n",
    "# Define some mock LoggerBases with buffers\n",
    "class A(dp.iter.IterDataPipe,LoggerBase):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe:\n",
    "            if is_record(o):\n",
    "                self.buffer.append(o)\n",
    "            else:\n",
    "                yield o\n",
    "\n",
    "loggers = RecordDumper()\n",
    "dp1,dp2 = loggers.fork(2)\n",
    "\n",
    "logger1 = A(dp1)\n",
    "logger2 = A(dp2)\n",
    "for _ in logger1: pass\n",
    "for _ in logger2: pass\n",
    "\n",
    "# Test whether the batch was correctly pushed to the main buffers of the logger bases\n",
    "test_eq([record.value for record in logger1.buffer], [None]+list(range(0, batches)))\n",
    "test_eq([record.value for record in logger2.buffer], [None]+list(range(0, batches)))\n",
    "\n",
    "# Test behavior with batch_on_pipe\n",
    "source_datapipe = dp.iter.IterableWrapper([1, 2, 3, 4, 5])\n",
    "\n",
    "class B(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,batches):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.batches = batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.source_datapipe\n",
    "\n",
    "source_datapipe = B(source_datapipe,batches=4)\n",
    "collector_with_pipe = BatchCollector(source_datapipe=source_datapipe, batch_on_pipe=B)\n",
    "test_eq(collector_with_pipe.batches, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "            # first be able to reference its queues.\n",
    "            source_datapipe=None, \n",
    "            # Number of iterations that have data before we start setting up the logging.\n",
    "            warmup:int=10,\n",
    "            # For automatic pipe attaching, we can designate which pipe this should be\n",
    "            # referneced for information on which epoch we are on\n",
    "            epoch_on_pipe:dp.iter.IterDataPipe=EpochCollector,\n",
    "            # For automatic pipe attaching, we can designate which pipe this should be\n",
    "            # referneced for information on which batch we are on\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=BatchCollector\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "        self.warmup = warmup\n",
    "        self.buffer = []\n",
    "\n",
    "    def dequeue(self): \n",
    "        while self.buffer: yield self.buffer.pop(0)\n",
    "\n",
    "    def update_bars(self,pbar,mbar,all_records):\n",
    "        if 'batch' in all_records: \n",
    "            pbar.update(all_records['batch'])\n",
    "\n",
    "        if 'epoch' in all_records:\n",
    "            mbar.update(all_records['epoch'])\n",
    "            collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "            mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "                \n",
    "    def __iter__(self):\n",
    "        epocher = find_dp(traverse_dps(self),self.epoch_on_pipe)\n",
    "        batcher = find_dp(traverse_dps(self),self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        counter = 0\n",
    "        for record in self.source_datapipe:\n",
    "            if is_record(record):\n",
    "                self.buffer.append(record)\n",
    "                counter += 1\n",
    "                # We only want to start setting up logging when the data loader starts producing \n",
    "                # real data.\n",
    "                continue\n",
    "            elif counter < self.warmup:\n",
    "                counter += 1\n",
    "                yield record\n",
    "                continue\n",
    "            \n",
    "            all_records = list(self.dequeue())\n",
    "            if self.attached_collectors is None:\n",
    "                self.attached_collectors = {o.name:o.value for o in all_records if o.value is None}\n",
    "                _logger.debug('Got initial values: ',self.attached_collectors)\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                pbar.update(0)\n",
    "                    \n",
    "            all_records = {o.name:o.value for o in all_records if o.value is not None}\n",
    "            _logger.debug('Got running values: ',self.attached_collectors)\n",
    "\n",
    "            if all_records:\n",
    "                self.attached_collectors = merge(self.attached_collectors,all_records)\n",
    "                self.update_bars(pbar,mbar,all_records)\n",
    "            yield record\n",
    "\n",
    "        all_records = {o.name:o.value for o in self.dequeue()}\n",
    "        if all_records: \n",
    "            self.attached_collectors = merge(self.attached_collectors,all_records)\n",
    "            self.update_bars(pbar,mbar,all_records)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf81b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e374e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = dp.iter.IterableWrapper(list(range(4)))\n",
    "pipe = BatchCollector(pipe,batches=4)\n",
    "pipe = EpochCollector(pipe,epochs=3)\n",
    "pipe = ProgressBarLogger(source_datapipe=pipe)\n",
    "list(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(dp.iter.IterDataPipe, LogCollector):\n",
    "    title:str='reward'\n",
    "\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def make_record(self,step:StepType):\n",
    "        reward = step.reward.detach().numpy()\n",
    "        if len(reward.shape)!=0:\n",
    "            reward = reward[0]\n",
    "        return Record(self.title,reward)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            # print(steps)\n",
    "            if is_record(steps):\n",
    "                yield steps\n",
    "                continue\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    yield self.make_record(step)\n",
    "            else:\n",
    "                yield self.make_record(steps)\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8838d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.core import SimpleStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa21b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>reward</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = dp.iter.IterableWrapper([SimpleStep(reward=torch.ones(1)) for _ in range(5)])\n",
    "pipe = BatchCollector(pipe,batches=4)\n",
    "pipe = EpochCollector(pipe,epochs=3)\n",
    "pipe = RewardCollector(pipe)\n",
    "pipe = ProgressBarLogger(source_datapipe=pipe)\n",
    "list(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1fc8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpisodeCollector(dp.iter.IterDataPipe):\n",
    "    title:str='episode'\n",
    "\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "    \n",
    "    def make_episode(self,step): \n",
    "        try:\n",
    "            v = step.episode_n.cpu().detach().numpy()\n",
    "            if len(v.shape)==1: \n",
    "                v = v[0]\n",
    "            return Record(self.title,v)\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting episode_n which is unexpected: \\n{step}')\n",
    "            raise\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            # if i==0: self.push_title('episode')\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    yield self.make_episode(step)\n",
    "            else:\n",
    "                yield self.make_episode(steps)\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "EpisodeCollector,\n",
    "\"\"\"Collects the `episode_n` field from steps.\"\"\",\n",
    "make_episode=\"Moves the `episode_n` tensor to numpy.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1e96b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RollingTerminatedRewardCollector(dp.iter.IterDataPipe):\n",
    "    title:str='rolling_reward'\n",
    "\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         rolling_length:int=100\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        self.rolling_rewards = deque([],maxlen=rolling_length)\n",
    "        \n",
    "    def step2terminated(self,step): return bool(step.terminated)\n",
    "\n",
    "    def make_reward(self,step): \n",
    "        try:\n",
    "            v = step.total_reward.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return float(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting reward which is unexpected: \\n{step}')\n",
    "            raise\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    if self.step2terminated(step):\n",
    "                        self.rolling_rewards.append(self.make_reward(step))\n",
    "                        yield Record(self.title,np.average(self.rolling_rewards))\n",
    "            elif self.step2terminated(steps):\n",
    "                self.rolling_rewards.append(self.make_reward(steps))\n",
    "                yield Record(self.title,np.average(self.rolling_rewards))\n",
    "            yield steps\n",
    "\n",
    "add_docs(\n",
    "RollingTerminatedRewardCollector,\n",
    "\"\"\"Collects the `total_reward` field from steps if `terminated` is true and \n",
    "logs a rolling average of size `rolling_length`.\"\"\",\n",
    "make_reward=\"Moves the `total_reward` tensor to numpy.\",\n",
    "step2terminated=\"Casts the `terminated` field in steps to a bool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e5d858f-afa2-4601-8195-e2d513375b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2.dataloader2 import DataLoader2\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "# from fastrl.data.dataloader2 import *\n",
    "# import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "import gymnasium as gym\n",
    "# from fastrl.pipes.map.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>reward</th>\n",
       "      <th>rolling_reward</th>\n",
       "      <th>episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WorkPipe(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "            self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "          for o in self.source_datapipe:\n",
    "                assert isinstance(o,SimpleStep),f'Got: {o} , {type(o)}'\n",
    "                yield o\n",
    "\n",
    "class Printer(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "            self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "          for o in self.source_datapipe:\n",
    "                yield o\n",
    "\n",
    "envs = ['CartPole-v1'] * 10\n",
    "\n",
    "\n",
    "pipe = dp.iter.IterableWrapper(envs)\n",
    "pipe = pipe.map(gym.make)\n",
    "pipe = pipe.pickleable_in_memory_cache()\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "pipe = EpisodeCollector(pipe).catch_records()\n",
    "pipe = RollingTerminatedRewardCollector(pipe).catch_records()\n",
    "pipe = RewardCollector(pipe).catch_records()\n",
    "pipe = pipe.header(limit=10)\n",
    "# pipe = Printer(pipe)\n",
    "\n",
    "pipe = WorkPipe(pipe)\n",
    "\n",
    "pipe = BatchCollector(pipe,batch_on_pipe=dp.iter.Header)\n",
    "pipe = EpochCollector(pipe,epochs=5).dump_records()\n",
    "pipe = ProgressBarLogger(pipe,batch_on_pipe=BatchCollector,epoch_on_pipe=EpochCollector,warmup=5)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "steps = list(pipe)\n",
    "len(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d76a55-09d6-4360-932e-0f5f9a36aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = DataLoader2(\n",
    "#     pipe,\n",
    "#     reading_service=PrototypeMultiProcessingReadingService(\n",
    "#         num_workers = 1,\n",
    "#         protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "#         protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "#         pipe_type = item_input_pipe_type,\n",
    "#         eventloop = SpawnProcessForDataPipeline\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # dl = logger_base.connect_source_datapipe(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a203c2a7-155d-4ccd-be2b-0aaa2492b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|export\n",
    "# class ActionPublish(dp.iter.IterDataPipe):\n",
    "#     def __init__(self,\n",
    "#             source_datapipe, # Pretend this is in the middle of a learner training segment\n",
    "#             dls\n",
    "#         ):\n",
    "#         self.source_datapipe = source_datapipe\n",
    "#         self.dls = dls\n",
    "#         self.protocol_clients = []\n",
    "#         self._expect_response = []\n",
    "#         self.initialized = False\n",
    "        \n",
    "#     def __iter__(self): \n",
    "#         for step in self.source_datapipe:\n",
    "#             if not self.initialized:\n",
    "#                 for dl in self.dls:\n",
    "#                     # dataloader.IterableWrapperIterDataPipe._IterateQueueDataPipes,[QueueWrappers]\n",
    "#                     for q_wrapper in dl.datapipe.iterable.datapipes:\n",
    "#                         self.protocol_clients.append(q_wrapper.protocol)\n",
    "#                         self._expect_response.append(False)\n",
    "#                 self.initialized = True\n",
    "            \n",
    "#             if isinstance(step,StepType):\n",
    "#                 for i,client in enumerate(self.protocol_clients):\n",
    "#                     if self._expect_response[i]: \n",
    "#                         client.get_response_input_item()\n",
    "#                     else:\n",
    "#                         client.request_input_item(\n",
    "#                             'action_augmentation',value=100\n",
    "#                         )\n",
    "\n",
    "#             yield step\n",
    "#         self.protocol_clients = []\n",
    "#         self._expect_response = []\n",
    "# add_docs(\n",
    "#     ActionPublish,\n",
    "#     \"\"\"Publishes an action augmentation to the dataloader.\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ec7e6af-18a4-47c8-8b96-1f50b3725f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|hide\n",
    "# #|eval: false\n",
    "# learn_pipe = ActionPublish(dl,[dl])\n",
    "\n",
    "# for o in learn_pipe:pass\n",
    "#     # print('Final Output',o)\n",
    "\n",
    "# # for i,o in enumerate(dl):\n",
    "# #     learn_pipe.source_datapipe.append(o)\n",
    "    \n",
    "# #     if i==0: print(dl.datapipe)\n",
    "# #     print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nbdev/export.py:54: UserWarning: Notebook '/home/fastrl_user/fastrl/nbs/07_Agents/02_Continuous/12u_agents.ppo.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac4294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
