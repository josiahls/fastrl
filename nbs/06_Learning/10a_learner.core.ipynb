{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp learner.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from typing import List,Union,Dict,Optional,Iterable\n",
    "# Third party libs\n",
    "from fastcore.all import add_docs\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2.graph import list_dps \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchdata.dataloader2 import DataLoader2\n",
    "from torchdata.dataloader2.graph import traverse_dps,DataPipeGraph,DataPipe\n",
    "# Local modules\n",
    "from fastrl.torch_core import evaluating\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.loggers.core import Record,EpochCollector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Learner Core\n",
    "> Core DataPipes for building Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984a53f1-93ee-4540-8b8d-50071090280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LearnerBase(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            # The base NN that we getting raw action values out of.\n",
    "            # This can either be a `nn.Module` or a dict of multiple `nn.Module`s\n",
    "            # For multimodel training\n",
    "            model:Union[nn.Module,Dict[str,nn.Module]], \n",
    "            # The dataloaders to read data from for training. This can be a single\n",
    "            # DataLoader2 or an iterable that yields from a DataLoader2.\n",
    "            fit_dls:Union[DataLoader2,Iterable], \n",
    "            # The dataloaders to read data from for validation. This can be a single\n",
    "            # DataLoader2 or an iterable that yields from a DataLoader2.\n",
    "            val_dls:Optional[Union[DataLoader2,Iterable]]=None, \n",
    "            # By default for reinforcement learning, we want to keep the workers\n",
    "            # alive so that simluations are not being shutdown / restarted.\n",
    "            # Epochs are expected to be handled semantically via tracking the number \n",
    "            # of batches.\n",
    "            infinite_dls:bool=True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.fit_iterable = fit_dls\n",
    "        self.val_iterable = val_dls\n",
    "        self.learner_base = self\n",
    "        self.infinite_dls = infinite_dls\n",
    "        self._dls = None\n",
    "        self._ended = False\n",
    "        self._validating = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {k:v for k,v in self.__dict__.items() if k not in ['_dls']}\n",
    "        # TODO: Needs a better way to serialize / deserialize states.\n",
    "        # state['iterable'] = [d.state_dict() for d in state['iterable']]\n",
    "        if dp.iter.IterDataPipe.getstate_hook is not None:\n",
    "            return dp.iter.IterDataPipe.getstate_hook(state)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # state['iterable'] = [d.from_state_dict() for d in state['iterable']]\n",
    "        for k,v in state.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "    def end(self):\n",
    "        self._ended = True\n",
    "   \n",
    "    def __iter__(self):\n",
    "        self._ended = False\n",
    "        for data in (self.val_iterable if self._validating else self.fit_iterable):\n",
    "            if self._ended:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "add_docs(\n",
    "LearnerBase,\n",
    "\"Combines models,dataloaders, and optimizers together for running a training pipeline.\",\n",
    "reset=\"\"\"If `infinite_dls` is false, then all dls will be reset, otherwise they will be\n",
    "kept alive.\"\"\",\n",
    "end=\"When called, will cause the Learner to stop iterating and cleanup.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742c9cc6-4be3-4606-b07d-12d30da2144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LearnerHead(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.learner_base = find_dp(traverse_dps(self.source_datapipe),LearnerBase)\n",
    "\n",
    "    def __iter__(self): yield from self.source_datapipe\n",
    "    \n",
    "    def fit(self,epochs):\n",
    "        epocher = find_dp(traverse_dps(self),EpochCollector)\n",
    "        epocher.epochs = epochs\n",
    "        \n",
    "        for iteration in self: \n",
    "            pass\n",
    "\n",
    "    def validate(self,epochs=1,show=True) -> DataPipe:\n",
    "        with evaluating(self.learner_base.model):\n",
    "            try:\n",
    "                self.learner_base._validating = True\n",
    "                epocher = find_dp(traverse_dps(self),EpochCollector)\n",
    "                epocher.epochs = epochs\n",
    "                for iteration in self: \n",
    "                    pass\n",
    "            finally:\n",
    "                self.learner_base._validating = False\n",
    "\n",
    "            if show:\n",
    "                pipes = list_dps(traverse_dps(self.learner_base.val_iterable))\n",
    "                for pipe in pipes:\n",
    "                    if hasattr(pipe,'show'):\n",
    "                        return pipe.show() \n",
    "        \n",
    "add_docs(\n",
    "LearnerHead,\n",
    "\"\"\"\n",
    "\"\"\",\n",
    "fit=\"Runs the `LearnerHead` pipeline for `epochs`\",\n",
    "validate=\"\"\"If there is more than 1 dl, then run 1 epoch of that dl based on \n",
    "`dl_idx` and returns the original datapipe for displaying.\"\"\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2bac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders\n",
    "from fastrl.loggers.core import EpochCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d2a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concated Dataloaders\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n",
      "Validating Concated Dataloaders:\n",
      "20 21 22 23 24 25 26 27 28 29 \n",
      "Muxed Dataloaders\n",
      "0 10 1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 \n",
      "0 10 1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 \n",
      "0 10 1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 \n",
      "0 10 1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 \n",
      "0 10 1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 \n",
      "Validating Muxed Dataloaders:\n",
      "20 21 22 23 24 25 26 27 28 29 \n"
     ]
    }
   ],
   "source": [
    "class Printer(dp.iter.IterDataPipe):\n",
    "    def __init__(self,pipe): \n",
    "        self.pipe = pipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.pipe:\n",
    "            print(o, end=\" \")\n",
    "            yield o\n",
    "        print()\n",
    "\n",
    "def TestLearner(dls,val_dls):\n",
    "    learner = LearnerBase(nn.Module(),dls,val_dls=val_dls)\n",
    "    learner = Printer(learner)\n",
    "    learner = EpochCollector(learner)\n",
    "    learner = LearnerHead(learner)\n",
    "    return learner\n",
    "\n",
    "dls = dataloaders((\n",
    "        dp.iter.IterableWrapper(range(10)),\n",
    "        dp.iter.IterableWrapper(range(10,20))\n",
    "    ),\n",
    "    do_concat=True\n",
    ")\n",
    "(dl3,) = dataloaders(dp.iter.IterableWrapper(range(20,30)))\n",
    "print('Concated Dataloaders')\n",
    "\n",
    "learn = TestLearner(dls,dl3)\n",
    "learn.fit(5)\n",
    "\n",
    "print(\"Validating Concated Dataloaders:\")\n",
    "learn.validate(1)  # using one epoch for validation by default\n",
    "\n",
    "dls = dataloaders((\n",
    "        dp.iter.IterableWrapper(range(10)),\n",
    "        dp.iter.IterableWrapper(range(10,20))\n",
    "    ),\n",
    "    do_multiplex=True\n",
    ")\n",
    "(dl3,) = dataloaders(dp.iter.IterableWrapper(range(20,30)))\n",
    "print('Muxed Dataloaders')\n",
    "\n",
    "learn = TestLearner(dls,dl3)\n",
    "learn.fit(5)\n",
    "\n",
    "print(\"Validating Muxed Dataloaders:\")\n",
    "learn.validate(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a51d58-04f6-454b-a783-ad667f4b2f17",
   "metadata": {},
   "source": [
    "> Warning: Pickling the LearnerBase will exclude the '_dls','opt','iterable' fields since\n",
    "these aren't easily picklable (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef47785f-7041-4590-9a55-43bb50e986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.agents.dqn.basic import DQN\n",
    "from fastrl.agents.core import AgentBase,AgentHead,StepFieldSelector,SimpleModelRunner,NumpyConverter\n",
    "from fastrl.agents.discrete import ArgMaxer,PyPrimativeConverter\n",
    "from fastrl.envs.gym import GymDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a48ec65-b385-4dea-87c8-f58c202d2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "# Setup the agent\n",
    "agent = AgentBase(model,[])\n",
    "agent = StepFieldSelector(agent,field='state')\n",
    "# All the things that make this agent unique and special\n",
    "# In this instance, all this module does is pass the action directly through to the model.\n",
    "agent = SimpleModelRunner(agent)\n",
    "agent = ArgMaxer(agent,only_idx=True)\n",
    "agent = NumpyConverter(agent)\n",
    "agent = PyPrimativeConverter(agent)\n",
    "# Bring everything together into the AgentHead where actions will be passed and then run through the pipeline\n",
    "agent = AgentHead(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8243aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "251e2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the DataBlock\n",
    "def gym_block(num_workers=0,vis=False):\n",
    "    pipe = GymDataPipe(['CartPole-v1']*1,agent=agent,nsteps=1,nskips=1,firstlast=False,include_images=True,n=100,bs=1)\n",
    "    if vis:\n",
    "        pipe = VSCodeDataPipe(pipe)\n",
    "    return pipe\n",
    "\n",
    "dls = dataloaders((gym_block(),gym_block(vis=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff58d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestLearner(model,dls):\n",
    "    learner = LearnerBase(model,dls[0],dls[1])\n",
    "    learner = EpochCollector(learner)\n",
    "    learner = LearnerHead(learner)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe530bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c6e24da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearnerHead"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = TestLearner(model,dls)\n",
    "\n",
    "out = pickle.dumps(learner)\n",
    "pickle.loads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f664a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80a492b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7402b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class StepBatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            device=None\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.device = device\n",
    "        \n",
    "    def vstack_by_fld(self,batch,fld):\n",
    "        try:\n",
    "            if self.device is None: return torch.vstack(tuple(getattr(step,fld) for step in batch))\n",
    "            return torch.vstack(tuple(getattr(step,fld) for step in batch)).to(torch.device(self.device))\n",
    "        except RuntimeError as e:\n",
    "            print(f'Failed to stack {fld} given batch: {batch}')\n",
    "            raise\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            cls = batch[0].__class__\n",
    "            yield cls(**{fld:self.vstack_by_fld(batch,fld) for fld in cls._fields})\n",
    "\n",
    "add_docs(\n",
    "StepBatcher,\n",
    "\"Converts multiple `StepType` into a single `StepType` with the fields concated.\",\n",
    "vstack_by_fld=\"vstacks a `fld` in `batch`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nbdev/export.py:54: UserWarning: Notebook '/home/fastrl_user/fastrl/nbs/07_Agents/02_Continuous/12u_agents.ppo.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
