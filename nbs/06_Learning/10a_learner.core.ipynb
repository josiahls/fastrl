{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp learner.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from typing import List,Union,Dict,Optional,Iterable,Tuple\n",
    "# Third party libs\n",
    "from fastcore.all import add_docs\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2.graph import list_dps \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchdata.dataloader2 import DataLoader2\n",
    "from torchdata.dataloader2.graph import traverse_dps,DataPipeGraph,DataPipe\n",
    "# Local modules\n",
    "from fastrl.torch_core import evaluating\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.loggers.core import Record,EpochCollector,BatchCollector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Learner Core\n",
    "> Core DataPipes for building Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a53f1-93ee-4540-8b8d-50071090280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LearnerBase(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            # The base NN that we getting raw action values out of.\n",
    "            # This can either be a `nn.Module` or a dict of multiple `nn.Module`s\n",
    "            # For multimodel training\n",
    "            model:Union[nn.Module,Dict[str,nn.Module]], \n",
    "            # The dataloaders to read data from for training. This can be a single\n",
    "            # DataLoader2 or an iterable that yields from a DataLoader2.\n",
    "            dls:Union[DataLoader2,Iterable], \n",
    "            # By default for reinforcement learning, we want to keep the workers\n",
    "            # alive so that simluations are not being shutdown / restarted.\n",
    "            # Epochs are expected to be handled semantically via tracking the number \n",
    "            # of batches.\n",
    "            infinite_dls:bool=True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.iterable = dls\n",
    "        self.learner_base = self\n",
    "        self.infinite_dls = infinite_dls\n",
    "        self._dls = None\n",
    "        self._ended = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = {k:v for k,v in self.__dict__.items() if k not in ['_dls']}\n",
    "        # TODO: Needs a better way to serialize / deserialize states.\n",
    "        # state['iterable'] = [d.state_dict() for d in state['iterable']]\n",
    "        if dp.iter.IterDataPipe.getstate_hook is not None:\n",
    "            return dp.iter.IterDataPipe.getstate_hook(state)\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # state['iterable'] = [d.from_state_dict() for d in state['iterable']]\n",
    "        for k,v in state.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "    def end(self):\n",
    "        self._ended = True\n",
    "   \n",
    "    def __iter__(self):\n",
    "        self._ended = False\n",
    "        for data in self.iterable:\n",
    "            if self._ended:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "add_docs(\n",
    "LearnerBase,\n",
    "\"Combines models,dataloaders, and optimizers together for running a training pipeline.\",\n",
    "reset=\"\"\"If `infinite_dls` is false, then all dls will be reset, otherwise they will be\n",
    "kept alive.\"\"\",\n",
    "end=\"When called, will cause the Learner to stop iterating and cleanup.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c9cc6-4be3-4606-b07d-12d30da2144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LearnerHead(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_datapipes:Tuple[dp.iter.IterDataPipe],\n",
    "            model\n",
    "        ):\n",
    "        if not isinstance(source_datapipes,tuple):\n",
    "            self.source_datapipes = (source_datapipes,)\n",
    "        else:\n",
    "            self.source_datapipes = source_datapipes\n",
    "        self.dp_idx = 0\n",
    "        self.model = model\n",
    "\n",
    "    def __iter__(self): yield from self.source_datapipes[self.dp_idx]\n",
    "    \n",
    "    def fit(self,epochs):\n",
    "        self.dp_idx = 0\n",
    "        epocher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),EpochCollector)\n",
    "        epocher.epochs = epochs\n",
    "        if isinstance(self.model,tuple):\n",
    "            for m in self.model: \n",
    "                m.train()\n",
    "        else:\n",
    "            self.model.train()\n",
    "        for _ in self: pass\n",
    "\n",
    "    def validate(self,epochs=1,batches=100,show=True) -> DataPipe:\n",
    "        self.dp_idx = 1\n",
    "        epocher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),EpochCollector)\n",
    "        epocher.epochs = epochs\n",
    "        batcher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),BatchCollector)\n",
    "        batcher.batches = batches\n",
    "        with evaluating(self.model):\n",
    "            for _ in self: pass\n",
    "            if show:\n",
    "                pipes = list_dps(traverse_dps(self.source_datapipes[self.dp_idx]))\n",
    "                for pipe in pipes:\n",
    "                    if hasattr(pipe,'show'):\n",
    "                        return pipe.show() \n",
    "        \n",
    "add_docs(\n",
    "LearnerHead,\n",
    "\"\"\"\n",
    "\"\"\",\n",
    "fit=\"Runs the `LearnerHead` pipeline for `epochs`\",\n",
    "validate=\"\"\"If there is more than 1 dl, then run 1 epoch of that dl based on \n",
    "`dl_idx` and returns the original datapipe for displaying.\"\"\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders\n",
    "from fastrl.loggers.core import EpochCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Printer(dp.iter.IterDataPipe):\n",
    "    def __init__(self,pipe): \n",
    "        self.pipe = pipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for o in self.pipe:\n",
    "            print(o, end=\" \")\n",
    "            yield o\n",
    "        print()\n",
    "\n",
    "def TestLearner(train_dl,valid_dls):\n",
    "    model = nn.Module()\n",
    "    learner = LearnerBase(model,train_dl)\n",
    "    learner = Printer(learner)\n",
    "    learner = EpochCollector(learner)\n",
    "\n",
    "    val_learner = LearnerBase(model,valid_dls)\n",
    "    val_learner = Printer(val_learner)\n",
    "    val_learner = BatchCollector(val_learner,batches=1000)\n",
    "    val_learner = EpochCollector(val_learner)\n",
    "\n",
    "    learner = LearnerHead((learner,val_learner),model)\n",
    "    return learner\n",
    "\n",
    "dls = dataloaders((\n",
    "        dp.iter.IterableWrapper(range(10)),\n",
    "        dp.iter.IterableWrapper(range(10,20))\n",
    "    ),\n",
    "    do_concat=True\n",
    ")\n",
    "(dl3,) = dataloaders(dp.iter.IterableWrapper(range(20,30)))\n",
    "print('Concated Dataloaders')\n",
    "\n",
    "learn = TestLearner(dls,dl3)\n",
    "learn.fit(5)\n",
    "\n",
    "print(\"Validating Concated Dataloaders:\")\n",
    "learn.validate(1)  # using one epoch for validation by default\n",
    "\n",
    "dls = dataloaders((\n",
    "        dp.iter.IterableWrapper(range(10)),\n",
    "        dp.iter.IterableWrapper(range(10,20))\n",
    "    ),\n",
    "    do_multiplex=True\n",
    ")\n",
    "(dl3,) = dataloaders(dp.iter.IterableWrapper(range(20,30)))\n",
    "print('Muxed Dataloaders')\n",
    "\n",
    "learn = TestLearner(dls,dl3)\n",
    "learn.fit(5)\n",
    "\n",
    "print(\"Validating Muxed Dataloaders:\")\n",
    "learn.validate(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a51d58-04f6-454b-a783-ad667f4b2f17",
   "metadata": {},
   "source": [
    "> Warning: Pickling the LearnerBase will exclude the '_dls','opt','iterable' fields since\n",
    "these aren't easily picklable (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47785f-7041-4590-9a55-43bb50e986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.agents.dqn.basic import DQN\n",
    "from fastrl.agents.core import AgentBase,AgentHead,StepFieldSelector,SimpleModelRunner,NumpyConverter\n",
    "from fastrl.agents.discrete import ArgMaxer,PyPrimativeConverter\n",
    "from fastrl.envs.gym import GymDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48ec65-b385-4dea-87c8-f58c202d2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "# Setup the agent\n",
    "agent = AgentBase(model,[])\n",
    "agent = StepFieldSelector(agent,field='state')\n",
    "# All the things that make this agent unique and special\n",
    "# In this instance, all this module does is pass the action directly through to the model.\n",
    "agent = SimpleModelRunner(agent)\n",
    "agent = ArgMaxer(agent,only_idx=True)\n",
    "agent = NumpyConverter(agent)\n",
    "agent = PyPrimativeConverter(agent)\n",
    "# Bring everything together into the AgentHead where actions will be passed and then run through the pipeline\n",
    "agent = AgentHead(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8243aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the DataBlock\n",
    "def gym_block(num_workers=0,vis=False):\n",
    "    pipe = GymDataPipe(['CartPole-v1']*1,agent=agent,nsteps=1,nskips=1,firstlast=False,include_images=True,n=100,bs=1)\n",
    "    if vis:\n",
    "        pipe = VSCodeDataPipe(pipe)\n",
    "    return pipe\n",
    "\n",
    "train_dl = dataloaders((gym_block(),gym_block(vis=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestLearner(model,train_dl,valid_dls):\n",
    "    learner = LearnerBase(model,train_dl)\n",
    "    learner = Printer(learner)\n",
    "    learner = EpochCollector(learner)\n",
    "\n",
    "    val_learner = LearnerBase(model,valid_dls)\n",
    "    val_learner = Printer(val_learner)\n",
    "    val_learner = BatchCollector(val_learner,batches=100)\n",
    "    val_learner = EpochCollector(val_learner)\n",
    "\n",
    "    learner = LearnerHead((learner,val_learner),model)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe530bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = TestLearner(model,train_dl,train_dl)\n",
    "\n",
    "out = pickle.dumps(learner)\n",
    "pickle.loads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f664a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a492b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class StepBatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            device=None\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.device = device\n",
    "        \n",
    "    def vstack_by_fld(self,batch,fld):\n",
    "        try:\n",
    "            t = torch.vstack(tuple(getattr(step,fld) for step in batch))\n",
    "            if self.device is not None:\n",
    "                t = t.to(torch.device(self.device))\n",
    "            t.requires_grad = False\n",
    "            return t\n",
    "        except RuntimeError as e:\n",
    "            print(f'Failed to stack {fld} given batch: {batch}')\n",
    "            raise\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            cls = batch[0].__class__\n",
    "            batched_step = cls(**{fld:self.vstack_by_fld(batch,fld) for fld in cls._fields})\n",
    "            yield batched_step \n",
    "\n",
    "add_docs(\n",
    "StepBatcher,\n",
    "\"Converts multiple `StepType` into a single `StepType` with the fields concated.\",\n",
    "vstack_by_fld=\"vstacks a `fld` in `batch`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6a924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
