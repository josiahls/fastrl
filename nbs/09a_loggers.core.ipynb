{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os,typing\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import *\n",
    "from torchdata.dataloader2.graph import find_dps,traverse\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Utilities used for handling log messages and display over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb554e2-de75-43e5-8fdf-25196d0803a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(dp.iter.IterDataPipe):\n",
    "    \n",
    "    def __init__(self,source_datapipe=None,do_filter=True):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.buffer = []\n",
    "        self.do_filter = do_filter\n",
    "        \n",
    "    def connect_source_datapipe(self,pipe):\n",
    "        self.source_datapipe = pipe\n",
    "        return self\n",
    "    \n",
    "    def filter_record(self,record):\n",
    "        return type(record)==Record and self.do_filter\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while self.buffer: yield self.buffer.pop(0)\n",
    "    \n",
    "    def reset(self):\n",
    "        # We can chain multiple `LoggerBase`s together, but if we do this, we dont want the \n",
    "        # first one in the chain filtering out the Records before the others!\n",
    "        if issubclass(type(self.source_datapipe),LoggerBase):\n",
    "            self.source_datapipe.do_filter = False\n",
    "        self.buffer = []\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "add_docs(\n",
    "    LoggerBase,\n",
    "    \"\"\"The `LoggerBase` class outlines simply the `buffer`. \n",
    "    It works in combo with `LogCollector` datapipe which will add to the `buffer`.\n",
    "    \n",
    "    `LoggerBase` also filters out the log records to as to not disrupt the training pipeline\"\"\",\n",
    "    filter_record=\"Returns True of `record` is actually a record and that `self` actually is set to filter.\",\n",
    "    connect_source_datapipe=\"\"\"`LoggerBase` does not need to be part of a `DataPipeGraph` \n",
    "    when its initialized, so this method allows for inserting into a `DataPipeGraph` later on.\"\"\",\n",
    "    reset=\"\"\"Checks if `self.source_datapipe` is also a logger base, and if so will tell `self.source_datapipe`\n",
    "    not to filter out the log records.\"\"\",\n",
    "    dequeue=\"Empties the `self.buffer` yielding each of its contents.\"\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8a78b5-01fc-436c-9602-4cd2779d1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_base = LoggerBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c9c084-75c7-44ce-a45c-b17adf411545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{140047756116816: (LoggerBase, {})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(logger_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase] # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = [o.buffer for o in logger_bases]\n",
    "        \n",
    "    def __iter__(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(LoggerBase):\n",
    "    def __init__(self,\n",
    "                 # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "                 # first be able to reference its queues.\n",
    "                 source_datapipe=None, \n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which epoch we are on\n",
    "                 epoch_on_pipe:dp.iter.IterDataPipe=None,\n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which batch we are on\n",
    "                 batch_on_pipe:dp.iter.IterDataPipe=None\n",
    "                ):\n",
    "        super().__init__(source_datapipe=source_datapipe)\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        epocher = find_dp(traverse(self),self.epoch_on_pipe)\n",
    "        batcher = find_dp(traverse(self),self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        for i,record in enumerate(self.source_datapipe):\n",
    "            if self.filter_record(record):\n",
    "                self.buffer.append(record)\n",
    "                # We only want to start setting up logging when the data loader starts producing \n",
    "                # real data.\n",
    "                continue\n",
    "                \n",
    "            if i==0:\n",
    "                self.attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                    \n",
    "            attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "\n",
    "            if attached_collectors:\n",
    "                self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "            \n",
    "            if 'batch' in attached_collectors:\n",
    "                pbar.update(attached_collectors['batch'])\n",
    "                \n",
    "            if 'epoch' in attached_collectors:\n",
    "                mbar.update(attached_collectors['epoch'])\n",
    "                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "                \n",
    "            if self.filter_record(record): continue\n",
    "            yield record\n",
    "\n",
    "        attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "\n",
    "        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(LogCollector):\n",
    "    def __iter__(self):\n",
    "        for q in self.main_buffers: q.append(Record('reward',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_buffers: q.append(Record('reward',step.reward.detach().numpy()))\n",
    "            else:\n",
    "                for q in self.main_buffers: q.append(Record('reward',steps.reward.detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8aca2e-de22-43d2-b606-d2f0554502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpocherCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            epochs:int=0,\n",
    "            logger_bases:List[LoggerBase]=None # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = [o.buffer for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_buffers is not None and not self.iteration_started:\n",
    "            for q in self.main_buffers: q.append(Record('epoch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        for i in range(self.epochs): \n",
    "            self.epoch = i\n",
    "            if self.main_buffers is not None:\n",
    "                for q in self.main_buffers: q.append(Record('epoch',self.epoch))\n",
    "            yield from self.source_datapipe\n",
    "            \n",
    "add_docs(\n",
    "    EpocherCollector,\n",
    "    \"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b456fac-9fdd-40a8-868a-5b97117c252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            logger_bases:List[LoggerBase], # `LoggerBase`s that we want to send metrics to\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = [o.buffer for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.batches = (\n",
    "            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)\n",
    "        )\n",
    "        self.batch = 0\n",
    "        \n",
    "    def batch_on_pipe_get_batches(self,batch_on_pipe):\n",
    "        pipe = find_dp(traverse(self.source_datapipe),batch_on_pipe)\n",
    "        if hasattr(pipe,'batches'):\n",
    "            return pipe.batches\n",
    "        elif hasattr(pipe,'limit'):\n",
    "            return pipe.limit\n",
    "        else:\n",
    "            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_buffers is not None and not self.iteration_started:\n",
    "            for q in self.main_buffers: q.append(Record('batch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        self.batch = 0\n",
    "        for batch,record in enumerate(self.source_datapipe): \n",
    "            yield record\n",
    "            self.batch = batch\n",
    "            if self.main_buffers is not None:\n",
    "                for q in self.main_buffers: q.append(Record('batch',batch))\n",
    "                \n",
    "add_docs(\n",
    "    BatchCollector,\n",
    "    \"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\",\n",
    "    batch_on_pipe_get_batches=\"Gets the number of batches from `batch_on_pipe`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d965bd6c-209f-4f66-b0a2-b5fc3defd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TestSync(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actions_augments = []\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for step in self.source_datapipe:\n",
    "            # print('Got step: ',step)\n",
    "            if isinstance(step,GetInputItemRequest):\n",
    "                # print('augmenting!!!!!')\n",
    "                self.actions_augments.append(step.value)\n",
    "                continue\n",
    "            elif self.actions_augments:\n",
    "                step = step.__class__(**{fld:getattr(step,fld)+self.actions_augments.pop(0) \n",
    "                                         if fld=='action' else \n",
    "                                         getattr(step,fld) for fld in step._fields})\n",
    "            yield step\n",
    "add_docs(\n",
    "    TestSync,\n",
    "    \"\"\"Tests getting values from data loader requests.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5d858f-afa2-4601-8195-e2d513375b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloader2_ext import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "\n",
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "logger_base = ProgressBarLogger(batch_on_pipe=BatchCollector,epoch_on_pipe=EpocherCollector)\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "pipe = RewardCollector(pipe,[logger_base])\n",
    "pipe = InputInjester(pipe)\n",
    "pipe = TestSync(pipe)\n",
    "pipe = pipe.header(limit=10)\n",
    "\n",
    "\n",
    "pipe = BatchCollector(pipe,[logger_base],batch_on_pipe=dp.iter.Header)\n",
    "pipe = EpocherCollector(pipe,epochs=5,logger_bases=[logger_base])\n",
    "pipe = logger_base.connect_source_datapipe(pipe)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "steps = list(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fc5521e-1326-4552-8593-91d30861bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchdata.dataloader2.dataloader2 import DataLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d76a55-09d6-4360-932e-0f5f9a36aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader2(\n",
    "    pipe,\n",
    "    reading_service=PrototypeMultiProcessingReadingService(\n",
    "        num_workers = 1,\n",
    "        protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "        protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "        pipe_type = item_input_pipe_type,\n",
    "        eventloop = SpawnProcessForDataPipeline\n",
    "    )\n",
    ")\n",
    "\n",
    "# dl = logger_base.connect_source_datapipe(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c2a7-155d-4ccd-be2b-0aaa2492b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastrl.core import StepType\n",
    "\n",
    "class ActionPublish(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe, # Pretend this is in the middle of a learner training segment\n",
    "            dls\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.dls = dls\n",
    "        self.protocol_clients = []\n",
    "        self._expect_response = []\n",
    "        self.initialized = False\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for step in self.source_datapipe:\n",
    "            if not self.initialized:\n",
    "                for dl in self.dls:\n",
    "                    # dataloader.IterableWrapperIterDataPipe._IterateQueueDataPipes,[QueueWrappers]\n",
    "                    for q_wrapper in dl.datapipe.iterable.datapipes:\n",
    "                        self.protocol_clients.append(q_wrapper.protocol)\n",
    "                        self._expect_response.append(False)\n",
    "                self.initialized = True\n",
    "            \n",
    "            if isinstance(step,StepType):\n",
    "                for i,client in enumerate(self.protocol_clients):\n",
    "                    if self._expect_response[i]: \n",
    "                        client.get_response_input_item()\n",
    "                    else:\n",
    "                        client.request_input_item(\n",
    "                            'action_augmentation',value=100\n",
    "                        )\n",
    "\n",
    "            yield step\n",
    "        self.protocol_clients = []\n",
    "        self._expect_response = []\n",
    "add_docs(\n",
    "    ActionPublish,\n",
    "    \"\"\"Publishes an action augmentation to the dataloader.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7e6af-18a4-47c8-8b96-1f50b3725f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_pipe = ActionPublish(dl,[dl])\n",
    "\n",
    "for o in learn_pipe:pass\n",
    "    # print('Final Output',o)\n",
    "\n",
    "# for i,o in enumerate(dl):\n",
    "#     learn_pipe.source_datapipe.append(o)\n",
    "    \n",
    "#     if i==0: print(dl.datapipe)\n",
    "#     print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4eb072-4b4f-4213-908b-7100e06ae102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290e1eb-6f1c-48a1-8b04-6ac0a5902d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
