{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os,typing\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import *\n",
    "# Local modules\n",
    "from fastrl.pipes.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Core Utilties for logging in fastrl using Callbacks and DataPipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe=None):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = self.initialize_queue()\n",
    "        \n",
    "    def initialize_queue(self):\n",
    "        \"If the start method is `spawn` then the queue will need to be managed using a Manager.\"\n",
    "        if get_start_method()=='spawn':\n",
    "            self.manager = Manager()\n",
    "            return self.manager.Queue()\n",
    "        else:\n",
    "            return Queue()\n",
    "        \n",
    "        \n",
    "    def connect_source_datapipe(self,pipe):\n",
    "        self.source_datapipe = pipe\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1713a8-0c6d-4c72-953d-8edede20684b",
   "metadata": {},
   "source": [
    "The `LoggerBase` class outlines simply the `main_queue`. It works in combo with `LogCollector` datapipe which will add to the `main_queue` even across processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase] # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases]\n",
    "        \n",
    "    def __iter__(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. This can be put in \n",
    "full on pipelines even across workers.\n",
    "\n",
    "The manual process might not be desireable though, so I'm thinking of revising the `add_cbs_to_pipes` where\n",
    "the pipe itself literally says if it should go before, after, exclude from.\n",
    "\n",
    "This seems... easier than defining a callback, and keeps a level of flatness to the api.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317033b3-e2e5-43a3-89e8-fd474fbb5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Record(typing.NamedTuple):\n",
    "    name:str\n",
    "    value:typing.Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(LoggerBase):\n",
    "    def __init__(self,\n",
    "                 # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "                 # first be able to reference its queues.\n",
    "                 source_datapipe=None, \n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which epoch we are on\n",
    "                 epoch_on_pipe:dp.iter.IterDataPipe=None,\n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which batch we are on\n",
    "                 batch_on_pipe:dp.iter.IterDataPipe=None\n",
    "                ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = self.initialize_queue()\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while not self.main_queue.empty(): yield self.main_queue.get()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        epocher = find_pipe_instance(self,self.epoch_on_pipe)\n",
    "        batcher = find_pipe_instance(self,self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        for i,record in enumerate(self.source_datapipe):\n",
    "            if i==0:\n",
    "                self.attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                    \n",
    "            attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "            \n",
    "            if attached_collectors:\n",
    "                self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "            \n",
    "            if 'batch' in attached_collectors:\n",
    "                pbar.update(attached_collectors['batch'])\n",
    "                \n",
    "            if 'epoch' in attached_collectors:\n",
    "                mbar.update(attached_collectors['epoch'])\n",
    "                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "            yield record\n",
    "\n",
    "        attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "\n",
    "        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(LogCollector):\n",
    "    def __iter__(self):\n",
    "        for q in self.main_queues: q.put(Record('reward',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_queues: q.put(Record('reward',step.reward.detach().numpy()))\n",
    "            else:\n",
    "                for q in self.main_queues: q.put(Record('reward',steps.reward.detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8aca2e-de22-43d2-b606-d2f0554502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpocherCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            epochs:int=0,\n",
    "            logger_bases:List[LoggerBase]=None # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_queues is not None and not self.iteration_started:\n",
    "            for q in self.main_queues: q.put(Record('epoch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        for i in range(self.epochs): \n",
    "            self.epoch = i\n",
    "            if self.main_queues is not None:\n",
    "                for q in self.main_queues: q.put(Record('epoch',self.epoch))\n",
    "            yield from self.source_datapipe\n",
    "            \n",
    "add_docs(\n",
    "    EpocherCollector,\n",
    "    \"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b456fac-9fdd-40a8-868a-5b97117c252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            logger_bases:List[LoggerBase], # `LoggerBase`s that we want to send metrics to\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.batches = ifnone(\n",
    "            batches,\n",
    "            find_pipe_instance(self.source_datapipe,pipe_cls=batch_on_pipe).batches\n",
    "        )\n",
    "        self.batch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_queues is not None and not self.iteration_started:\n",
    "            for q in self.main_queues: q.put(Record('batch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        self.batch = 0\n",
    "        for batch,record in enumerate(self.source_datapipe): \n",
    "            yield record\n",
    "            self.batch = batch\n",
    "            if self.main_queues is not None:\n",
    "                for q in self.main_queues: q.put(Record('batch',batch))\n",
    "                \n",
    "add_docs(\n",
    "    BatchCollector,\n",
    "    \"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "\n",
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "logger_base = ProgressBarLogger(batch_on_pipe=BatchCollector,epoch_on_pipe=EpocherCollector)\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle(count=(18*len(envs))) \n",
    "pipe = EpocherCollector(pipe,epochs=5)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "\n",
    "pipe = RewardCollector(pipe,[logger_base])\n",
    "\n",
    "# pipe = logger_base.connect_source_datapipe(pipe)\n",
    "# pipe = add_cbs_to_pipes(pipe,L(cb))\n",
    "# steps = list(pipe)\n",
    "steps = list(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8dae91-6f28-4256-975d-48783866322e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
