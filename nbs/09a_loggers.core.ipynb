{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os,typing\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import *\n",
    "from torchdata.dataloader2.graph import find_dps,traverse\n",
    "# Local modules\n",
    "from fastrl.pipes.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Utilities used for handling log messages and display over multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(dp.iter.IterDataPipe):\n",
    "    \n",
    "    def __init__(self,source_datapipe=None):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = self.initialize_queue()\n",
    "        \n",
    "    def initialize_queue(self):\n",
    "        \"If the start method is `spawn` then the queue will need to be managed using a Manager.\"\n",
    "        # if get_start_method()=='spawn':\n",
    "        self.manager = Manager()\n",
    "        return self.manager.Queue()\n",
    "        # else:\n",
    "        #     return Queue() \n",
    "        \n",
    "    def __getstate__(self):\n",
    "        \"LoggerBase will not allow passing queues when being serialized.\"\n",
    "        return {k:v for k,v in self.__dict__.items() if k not in ['main_queue','manager']}\n",
    "        \n",
    "    def connect_source_datapipe(self,pipe):\n",
    "        self.source_datapipe = pipe\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8a78b5-01fc-436c-9602-4cd2779d1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_base = LoggerBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c9c084-75c7-44ce-a45c-b17adf411545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{139920640893584: (LoggerBase, {})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(logger_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1713a8-0c6d-4c72-953d-8edede20684b",
   "metadata": {},
   "source": [
    "The `LoggerBase` class outlines simply the `main_queue`. It works in combo with `LogCollector` datapipe which will add to the `main_queue` even across processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase] # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases]\n",
    "        \n",
    "    def __iter__(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. This can be put in \n",
    "full on pipelines even across workers.\n",
    "\n",
    "The manual process might not be desireable though, so I'm thinking of revising the `add_cbs_to_pipes` where\n",
    "the pipe itself literally says if it should go before, after, exclude from.\n",
    "\n",
    "This seems... easier than defining a callback, and keeps a level of flatness to the api.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317033b3-e2e5-43a3-89e8-fd474fbb5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Record(typing.NamedTuple):\n",
    "    name:str\n",
    "    value:typing.Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(LoggerBase):\n",
    "    def __init__(self,\n",
    "                 # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "                 # first be able to reference its queues.\n",
    "                 source_datapipe=None, \n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which epoch we are on\n",
    "                 epoch_on_pipe:dp.iter.IterDataPipe=None,\n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which batch we are on\n",
    "                 batch_on_pipe:dp.iter.IterDataPipe=None\n",
    "                ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = self.initialize_queue()\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while not self.main_queue.empty(): yield self.main_queue.get()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        epocher = find_dp(traverse(self),self.epoch_on_pipe)\n",
    "        batcher = find_dp(traverse(self),self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        for i,record in enumerate(self.source_datapipe):\n",
    "            if i==0:\n",
    "                self.attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                    \n",
    "            attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "\n",
    "            if attached_collectors:\n",
    "                self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "            \n",
    "            if 'batch' in attached_collectors:\n",
    "                pbar.update(attached_collectors['batch'])\n",
    "                \n",
    "            if 'epoch' in attached_collectors:\n",
    "                mbar.update(attached_collectors['epoch'])\n",
    "                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "            yield record\n",
    "\n",
    "        attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "\n",
    "        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(LogCollector):\n",
    "    def __iter__(self):\n",
    "        for q in self.main_queues: q.put(Record('reward',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_queues: q.put(Record('reward',step.reward.detach().numpy()))\n",
    "            else:\n",
    "                for q in self.main_queues: q.put(Record('reward',steps.reward.detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8aca2e-de22-43d2-b606-d2f0554502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpocherCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            epochs:int=0,\n",
    "            logger_bases:List[LoggerBase]=None # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_queues is not None and not self.iteration_started:\n",
    "            for q in self.main_queues: q.put(Record('epoch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        for i in range(self.epochs): \n",
    "            self.epoch = i\n",
    "            if self.main_queues is not None:\n",
    "                for q in self.main_queues: q.put(Record('epoch',self.epoch))\n",
    "            yield from self.source_datapipe\n",
    "            \n",
    "add_docs(\n",
    "    EpocherCollector,\n",
    "    \"\"\"Tracks the number of epochs that the pipeline is currently on.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b456fac-9fdd-40a8-868a-5b97117c252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            logger_bases:List[LoggerBase], # `LoggerBase`s that we want to send metrics to\n",
    "            batches:Optional[int]=None,\n",
    "            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance\n",
    "            # and try to grab a `batches` field from there.\n",
    "            batch_on_pipe:dp.iter.IterDataPipe=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases] if logger_bases is not None else None\n",
    "        self.iteration_started = False\n",
    "        self.batches = (\n",
    "            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)\n",
    "        )\n",
    "        self.batch = 0\n",
    "        \n",
    "    def batch_on_pipe_get_batches(self,batch_on_pipe):\n",
    "        pipe = find_dp(traverse(self.source_datapipe),batch_on_pipe)\n",
    "        if hasattr(pipe,'batches'):\n",
    "            return pipe.batches\n",
    "        elif hasattr(pipe,'limit'):\n",
    "            return pipe.limit\n",
    "        else:\n",
    "            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')\n",
    "\n",
    "    def __iter__(self): \n",
    "        if self.main_queues is not None and not self.iteration_started:\n",
    "            for q in self.main_queues: q.put(Record('batch',None))\n",
    "            self.iteration_started = True\n",
    "            \n",
    "        self.batch = 0\n",
    "        for batch,record in enumerate(self.source_datapipe): \n",
    "            yield record\n",
    "            self.batch = batch\n",
    "            if self.main_queues is not None:\n",
    "                for q in self.main_queues: q.put(Record('batch',batch))\n",
    "                \n",
    "add_docs(\n",
    "    BatchCollector,\n",
    "    \"\"\"Tracks the number of batches that the pipeline is currently on.\"\"\",\n",
    "    batch_on_pipe_get_batches=\"Gets the number of batches from `batch_on_pipe`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d965bd6c-209f-4f66-b0a2-b5fc3defd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TestSync(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actions_augments = []\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for step in self.source_datapipe:\n",
    "            # print('Got step: ',step)\n",
    "            if isinstance(step,GetInputItemRequest):\n",
    "                # print('augmenting!!!!!')\n",
    "                self.actions_augments.append(step.value)\n",
    "                continue\n",
    "            elif self.actions_augments:\n",
    "                step = step.__class__(**{fld:getattr(step,fld)+self.actions_augments.pop(0) \n",
    "                                         if fld=='action' else \n",
    "                                         getattr(step,fld) for fld in step._fields})\n",
    "            yield step\n",
    "add_docs(\n",
    "    TestSync,\n",
    "    \"\"\"Tests getting values from data loader requests.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e5d858f-afa2-4601-8195-e2d513375b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloader2_ext import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "\n",
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "logger_base = ProgressBarLogger(batch_on_pipe=BatchCollector,epoch_on_pipe=EpocherCollector)\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "pipe = RewardCollector(pipe,[logger_base])\n",
    "pipe = InputInjester(pipe)\n",
    "pipe = TestSync(pipe)\n",
    "pipe = pipe.header(limit=10)\n",
    "\n",
    "pipe = BatchCollector(pipe,[logger_base],batch_on_pipe=dp.iter.Header)\n",
    "pipe = EpocherCollector(pipe,epochs=5,logger_bases=[logger_base])\n",
    "# Turn off the seed so that some envs end before others...\n",
    "steps = list(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fc5521e-1326-4552-8593-91d30861bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchdata.dataloader2.dataloader2 import DataLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d76a55-09d6-4360-932e-0f5f9a36aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader2(\n",
    "    pipe,\n",
    "    reading_service=PrototypeMultiProcessingReadingService(\n",
    "        num_workers = 1,\n",
    "        protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "        protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "        pipe_type = item_input_pipe_type,\n",
    "        eventloop = SpawnProcessForDataPipeline\n",
    "    )\n",
    ")\n",
    "\n",
    "# dl = logger_base.connect_source_datapipe(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a203c2a7-155d-4ccd-be2b-0aaa2492b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastrl.core import StepType\n",
    "\n",
    "class ActionPublish(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe, # Pretend this is in the middle of a learner training segment\n",
    "            dls\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.dls = dls\n",
    "        self.protocol_clients = []\n",
    "        self._expect_response = []\n",
    "        self.initialized = False\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for step in self.source_datapipe:\n",
    "            if not self.initialized:\n",
    "                for dl in self.dls:\n",
    "                    # dataloader.IterableWrapperIterDataPipe._IterateQueueDataPipes,[QueueWrappers]\n",
    "                    for q_wrapper in dl.datapipe.iterable.datapipes:\n",
    "                        self.protocol_clients.append(q_wrapper.protocol)\n",
    "                        self._expect_response.append(False)\n",
    "                self.initialized = True\n",
    "            \n",
    "            if isinstance(step,StepType):\n",
    "                for i,client in enumerate(self.protocol_clients):\n",
    "                    if self._expect_response[i]: \n",
    "                        client.get_response_input_item()\n",
    "                    else:\n",
    "                        print('sending augmentation')\n",
    "                        client.request_input_item(\n",
    "                            'action_augmentation',value=100\n",
    "                        )\n",
    "\n",
    "            yield step\n",
    "        self.protocol_clients = []\n",
    "        self._expect_response = []\n",
    "add_docs(\n",
    "    ActionPublish,\n",
    "    \"\"\"Publishes an action augmentation to the dataloader.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec7e6af-18a4-47c8-8b96-1f50b3725f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.1161, -0.7581,  0.1456,  1.1927]), action=tensor(0.), next_state=tensor([-0.1313, -0.9548,  0.1694,  1.5272]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(11.), env_id=tensor(139920349538000), proc_id=tensor(12766), step_n=tensor(11), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0470, -0.1752,  0.0025,  0.2634]), action=tensor(100.), next_state=tensor([ 0.0435, -0.3704,  0.0078,  0.5569]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0094, -0.5514,  0.0610,  0.8810]), action=tensor(101.), next_state=tensor([-0.0016, -0.3571,  0.0787,  0.6081]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920329125648), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0078,  0.6182,  0.0155, -0.8223]), action=tensor(100.), next_state=tensor([ 0.0202,  0.4229, -0.0010, -0.5248]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920328912592), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0230, -0.2328, -0.0022,  0.2280]), action=tensor(101.), next_state=tensor([ 0.0183, -0.0377,  0.0024, -0.0654]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920328910608), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0812,  0.6094, -0.0450, -0.8870]), action=tensor(100.), next_state=tensor([ 0.0934,  0.4149, -0.0628, -0.6088]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139922571154384), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0238,  0.6053, -0.0404, -0.8842]), action=tensor(100.), next_state=tensor([ 0.0360,  0.4108, -0.0581, -0.6045]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139922570282192), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0192,  0.2244,  0.0035, -0.2537]), action=tensor(100.), next_state=tensor([-0.0148,  0.0293, -0.0015,  0.0401]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920640413840), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0213,  0.1713,  0.0210, -0.2959]), action=tensor(100.), next_state=tensor([ 0.0247, -0.0241,  0.0151,  0.0033]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920640415760), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0449,  0.2424, -0.0362, -0.3759]), action=tensor(100.), next_state=tensor([-0.0400,  0.0478, -0.0437, -0.0948]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(139920349566544), proc_id=tensor(12766), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.1504, -1.1515,  0.2000,  1.8677]), action=tensor(100.), next_state=tensor([-0.1734, -1.3482,  0.2373,  2.2152]), terminated=tensor(True), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(13.), env_id=tensor(139920349538000), proc_id=tensor(12766), step_n=tensor(13), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0435, -0.3704,  0.0078,  0.5569]), action=tensor(100.), next_state=tensor([ 0.0361, -0.5656,  0.0189,  0.8520]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0016, -0.3571,  0.0787,  0.6081]), action=tensor(100.), next_state=tensor([-0.0088, -0.5533,  0.0908,  0.9245]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920329125648), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0202,  0.4229, -0.0010, -0.5248]), action=tensor(101.), next_state=tensor([ 0.0287,  0.6180, -0.0115, -0.8178]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920328912592), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0183, -0.0377,  0.0024, -0.0654]), action=tensor(101.), next_state=tensor([ 0.0176,  0.1574,  0.0011, -0.3573]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920328910608), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0934,  0.4149, -0.0628, -0.6088]), action=tensor(100.), next_state=tensor([ 0.1017,  0.2207, -0.0750, -0.3366]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139922571154384), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0360,  0.4108, -0.0581, -0.6045]), action=tensor(101.), next_state=tensor([ 0.0442,  0.6067, -0.0702, -0.9149]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139922570282192), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0148,  0.0293, -0.0015,  0.0401]), action=tensor(101.), next_state=tensor([-0.0142,  0.2244, -0.0007, -0.2531]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920640413840), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0247, -0.0241,  0.0151,  0.0033]), action=tensor(101.), next_state=tensor([ 0.0242,  0.1708,  0.0152, -0.2845]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920640415760), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0400,  0.0478, -0.0437, -0.0948]), action=tensor(101.), next_state=tensor([-0.0391,  0.2435, -0.0456, -0.4009]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(139920349566544), proc_id=tensor(12766), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0248, -0.7610,  0.0360,  1.1506]), action=tensor(100.), next_state=tensor([ 0.0096, -0.9566,  0.0590,  1.4543]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0088, -0.5533,  0.0908,  0.9245]), action=tensor(101.), next_state=tensor([-0.0198, -0.3595,  0.1093,  0.6617]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920329125648), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0287,  0.6180, -0.0115, -0.8178]), action=tensor(100.), next_state=tensor([ 0.0410,  0.4231, -0.0278, -0.5287]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920328912592), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0176,  0.1574,  0.0011, -0.3573]), action=tensor(101.), next_state=tensor([ 0.0207,  0.3525, -0.0061, -0.6497]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920328910608), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.1017,  0.2207, -0.0750, -0.3366]), action=tensor(101.), next_state=tensor([ 0.1061,  0.4168, -0.0817, -0.6519]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139922571154384), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0442,  0.6067, -0.0702, -0.9149]), action=tensor(100.), next_state=tensor([ 0.0563,  0.4126, -0.0885, -0.6451]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139922570282192), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0142,  0.2244, -0.0007, -0.2531]), action=tensor(101.), next_state=tensor([-0.0097,  0.4195, -0.0058, -0.5460]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920640413840), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0242,  0.1708,  0.0152, -0.2845]), action=tensor(100.), next_state=tensor([ 0.0276, -0.0245,  0.0095,  0.0129]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920640415760), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0391,  0.2435, -0.0456, -0.4009]), action=tensor(100.), next_state=tensor([-0.0342,  0.0491, -0.0536, -0.1230]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(139920349566544), proc_id=tensor(12766), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0096, -0.9566,  0.0590,  1.4543]), action=tensor(101.), next_state=tensor([-0.0095, -0.7622,  0.0881,  1.1806]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0095, -0.7622,  0.0881,  1.1806]), action=tensor(100.), next_state=tensor([-0.0248, -0.9583,  0.1117,  1.4996]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(11.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(11), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0270, -0.1660,  0.1225,  0.4053]), action=tensor(101.), next_state=tensor([-0.0303,  0.0272,  0.1307,  0.1537]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920329125648), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0410,  0.4231, -0.0278, -0.5287]), action=tensor(100.), next_state=tensor([ 0.0495,  0.2283, -0.0384, -0.2449]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920328912592), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0207,  0.3525, -0.0061, -0.6497]), action=tensor(100.), next_state=tensor([ 0.0278,  0.1575, -0.0191, -0.3589]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920328910608), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.1061,  0.4168, -0.0817, -0.6519]), action=tensor(101.), next_state=tensor([ 0.1145,  0.6129, -0.0947, -0.9692]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139922571154384), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0563,  0.4126, -0.0885, -0.6451]), action=tensor(100.), next_state=tensor([ 0.0646,  0.2188, -0.1014, -0.3816]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139922570282192), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0097,  0.4195, -0.0058, -0.5460]), action=tensor(101.), next_state=tensor([-0.0013,  0.6148, -0.0167, -0.8405]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920640413840), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0276, -0.0245,  0.0095,  0.0129]), action=tensor(101.), next_state=tensor([ 0.0272,  0.1705,  0.0098, -0.2768]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920640415760), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0342,  0.0491, -0.0536, -0.1230]), action=tensor(101.), next_state=tensor([-0.0332,  0.2449, -0.0560, -0.4321]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(139920349566544), proc_id=tensor(12766), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0248, -0.9583,  0.1117,  1.4996]), action=tensor(101.), next_state=tensor([-0.0439, -0.7647,  0.1417,  1.2437]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(12.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(12), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0439, -0.7647,  0.1417,  1.2437]), action=tensor(100.), next_state=tensor([-0.0592, -0.9614,  0.1665,  1.5772]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(13.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(13), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0298, -0.1696,  0.1337,  0.4845]), action=tensor(101.), next_state=tensor([-0.0332,  0.0234,  0.1434,  0.2368]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(12.), env_id=tensor(139920329125648), proc_id=tensor(12766), step_n=tensor(12), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0495,  0.2283, -0.0384, -0.2449]), action=tensor(101.), next_state=tensor([ 0.0540,  0.4240, -0.0433, -0.5495]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920328912592), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0278,  0.1575, -0.0191, -0.3589]), action=tensor(101.), next_state=tensor([ 0.0309,  0.3529, -0.0263, -0.6575]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920328910608), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.1145,  0.6129, -0.0947, -0.9692]), action=tensor(101.), next_state=tensor([ 0.1267,  0.8092, -0.1141, -1.2900]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139922571154384), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0646,  0.2188, -0.1014, -0.3816]), action=tensor(100.), next_state=tensor([ 0.0689,  0.0252, -0.1090, -0.1225]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139922570282192), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0013,  0.6148, -0.0167, -0.8405]), action=tensor(100.), next_state=tensor([ 0.0110,  0.4199, -0.0335, -0.5531]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920640413840), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([ 0.0272,  0.1705,  0.0098, -0.2768]), action=tensor(100.), next_state=tensor([ 0.0306, -0.0248,  0.0042,  0.0190]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920640415760), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0332,  0.2449, -0.0560, -0.4321]), action=tensor(101.), next_state=tensor([-0.0283,  0.4408, -0.0647, -0.7419]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(139920349566544), proc_id=tensor(12766), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n",
      "sending augmentation\n",
      "Final Output SimpleStep(state=tensor([-0.0592, -0.9614,  0.1665,  1.5772]), action=tensor(101.), next_state=tensor([-0.0785, -0.7686,  0.1981,  1.3408]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(14.), env_id=tensor(139920640344720), proc_id=tensor(12766), step_n=tensor(14), episode_n=tensor(1), image=tensor([0.]))\n",
      "Final Output action_augmentation\n"
     ]
    }
   ],
   "source": [
    "learn_pipe = ActionPublish(dl,[dl])\n",
    "\n",
    "for o in learn_pipe:\n",
    "    print('Final Output',o)\n",
    "\n",
    "# for i,o in enumerate(dl):\n",
    "#     learn_pipe.source_datapipe.append(o)\n",
    "    \n",
    "#     if i==0: print(dl.datapipe)\n",
    "#     print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4eb072-4b4f-4213-908b-7100e06ae102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290e1eb-6f1c-48a1-8b04-6ac0a5902d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
