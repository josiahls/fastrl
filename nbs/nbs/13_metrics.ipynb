{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.metrics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "import ptan\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Metrics\n",
    "\n",
    "> Metrics for tracking the progress of reinforcement learning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AvgEpisodeRewardMetric(Metric):\n",
    "    def __init__(self,experience_cls:ExperienceFirstLast,always_extend=False):\n",
    "        self.experience_cls=experience_cls\n",
    "        self.always_extend=always_extend\n",
    "        self.rolling_rewards=deque([0],maxlen=100)\n",
    "        \n",
    "    def accumulate(self,learn):\n",
    "#         yb=learn.yb\n",
    "#         print(len(yb),len(yb[0]),yb)\n",
    "        if type(learn.yb[0][0])!=ExperienceFirstLast:\n",
    "            yb=[]\n",
    "            for i in range(len(learn.xb[0])):\n",
    "    #             print(learn.yb)\n",
    "                yb.append(self.experience_cls(learn.xb[0][i].cpu().detach(),*(learn.yb[j][i].cpu().detach() for j in range(len(learn.yb)))))\n",
    "        else:\n",
    "            yb=learn.yb[0]\n",
    "#             print(yb[-1])\n",
    "#         yb=[ExperienceFirstLast([0],*(yb[k][i] for k in range(len(yb)))) for i in range(len(yb[0]))]\n",
    "#         yb=[for yb.items()]\n",
    "#         print([o.done for o in yb if o.done])\n",
    "#         print([float(o.episode_reward) for o in yb  if o.done])\n",
    "        if len([float(o.episode_reward) for o in yb if o.done and int(o.episode_reward)!=0])==0:return\n",
    "#         print([o.episode_reward for o in yb if o.done])\n",
    "        if not self.always_extend:\n",
    "            r=[np.average([float(o.episode_reward) for o in yb if o.done and int(o.episode_reward)!=0])]\n",
    "        else:\n",
    "            r=[float(o.episode_reward) for o in yb if o.done and int(o.episode_reward)!=0]\n",
    "#         print([y for y in yb if y.absolute_end])\n",
    "#         for r in rewards:\n",
    "        if len(r)!=0:self.rolling_rewards.extend(r)\n",
    "#         print(len(rewards))\n",
    "#         if len(rewards)!=0:self.r=sum(rewards)/len(rewards)\n",
    "        \n",
    "    @property\n",
    "    def value(self):return np.mean(self.rolling_rewards) if len(self.rolling_rewards)!=1 else self.rolling_rewards[0]\n",
    "    @property\n",
    "    def name(self):return 'avg_episode_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.actorcritic.a3c_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process DataFitProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/fastrl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/project/fastrl/fastrl/async_data.py\", line 43, in template_data_fit\n",
      "    ifnone(pipe_out,noopo).send('Hello')\n",
      "NameError: name 'pipe_out' is not defined\n"
     ]
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "\n",
    "block=AsyncExperienceBlock(\n",
    "    experience_block=partial(FirstLastExperienceBlock,a=0,seed=0,dls_kwargs={'bs':1,'n_steps':4,'num_workers':0,\n",
    "                                                                             'verbose':False,'indexed':True,'shuffle_train':False}),\n",
    "    n_processes=1,\n",
    "    n=128\n",
    ")\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x[0],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,bs=3)\n",
    "\n",
    "model=LinearA2C((4,),2)\n",
    "agent=ActorCriticAgent(model=model)\n",
    "learner=A3CLearner(dls,agent=agent,cbs=[A3CTrainer],reward_steps=4,\n",
    "                   metrics=[AvgEpisodeRewardMetric()])\n",
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 05c_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/13_metrics.ipynb\n",
      "converting: /opt/project/fastrl/nbs/20c_qlearning.dqn_target.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export2html import *\n",
    "from nbdev.export import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
