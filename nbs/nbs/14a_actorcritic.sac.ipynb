{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC\n",
    "\n",
    "> Soft Actor Critic\n",
    "\n",
    "Most of the code pre-refactor is thanks to: https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state.float(), action.float()], 1)\n",
    "\n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean = nn.Linear(hidden_dim, num_actions)\n",
    "        self.noise = torch.Tensor(num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = 1.\n",
    "            self.action_bias = 0.\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = torch.tanh(self.mean(x)) * self.action_scale + self.action_bias\n",
    "        return mean\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean = self.forward(state)\n",
    "        noise = self.noise.normal_(0., std=0.1)\n",
    "        noise = noise.clamp(-0.25, 0.25)\n",
    "        action = mean + noise\n",
    "        return action, torch.tensor(0.), mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        self.noise = self.noise.to(device)\n",
    "        return super(DeterministicPolicy, self).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_log_gaussian(mean, log_std, t):\n",
    "    quadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
    "    l = mean.shape\n",
    "    log_z = log_std\n",
    "    z = l[-1] * math.log(2 * math.pi)\n",
    "    log_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
    "    return log_p\n",
    "\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "    if dim is None:\n",
    "        inputs = inputs.view(-1)\n",
    "        dim = 0\n",
    "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "    if not keepdim:\n",
    "        outputs = outputs.squeeze(dim)\n",
    "    return outputs\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pprint import pprint\n",
    "\n",
    "class SAC(BaseAgent):\n",
    "    def __init__(self, num_inputs, action_space,gamma=0.99,tau=0.005,alpha=0.2,policy='gaussian',\n",
    "                automatic_entropy_tuning=True,target_update_interval=1,hidden_size=100,lr=0.0003):\n",
    "        \n",
    "        self.action_space=action_space\n",
    "        self.warming_up=False\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.policy_type = policy\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        self.device=default_device()\n",
    "        \n",
    "        # qf\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"gaussian\":\n",
    "            # Target Entropy = −dim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper\n",
    "            if self.automatic_entropy_tuning is True:\n",
    "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
    "                self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "                self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        else:\n",
    "            self.alpha = 0\n",
    "            self.automatic_entropy_tuning = False\n",
    "            self.policy = DeterministicPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "            \n",
    "        self.updates=0\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        if self.warming_up: return self.action_space.sample()\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if evaluate is False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def __call__(self,s,asl):\n",
    "        action,asl= self.select_action(s),asl\n",
    "        if len(action.shape)==1: action=action.reshape(1,-1)\n",
    "        return action,asl\n",
    "\n",
    "    def update_parameters(self, *yb, learn):\n",
    "        # Sample a batch from memory\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch, mask_batch = learn.memory.sample(batch_size=batch_size)\n",
    "        batch=learn.sample_yb\n",
    "#         pprint(batch)\n",
    "        state_batch=torch.stack([o.state.to(device=default_device()) for o in batch]).float()\n",
    "        next_state_batch=torch.stack([o.last_state.to(device=default_device()) for o in batch]).float()\n",
    "        action_batch=torch.stack([o.action.to(device=default_device()) for o in batch]).float()\n",
    "        reward_batch=torch.stack([o.reward.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        mask_batch=torch.stack([o.done.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        \n",
    "#         print(state_batch.shape,next_state_batch.shape,action_batch.shape,reward_batch.shape,mask_batch.shape)\n",
    "#         state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "#         next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "#         action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "#         reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "#         mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + (1-mask_batch) * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if self.updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        self.updates+=1\n",
    "#         print(self.updates)\n",
    "\n",
    "        return qf1_loss+ qf2_loss+ policy_loss+ alpha_loss+ alpha_tlogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperienceReplay(Callback):\n",
    "    def __init__(self,sz=100,bs=128,starting_els=1,max_steps=1):\n",
    "        store_attr()\n",
    "        self.queue=deque(maxlen=int(sz))\n",
    "        self.max_steps=max_steps\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.warming_up=True\n",
    "        while len(self.queue)<self.starting_els:\n",
    "            for i,o in enumerate(self.dls.train):\n",
    "                batch=[ExperienceFirstLast(state=o[0][i],action=o[1][i],reward=o[2][i],\n",
    "                                    last_state=o[3][i], done=(o[4][i] and self.max_steps!=o[6][i]),episode_reward=o[5][i],steps=o[6][i])\n",
    "                                    for i in range(len(o[0]))]\n",
    "#                 print(self.max_steps,max([o.steps for o in batch]))\n",
    "                for _b in batch: self.queue.append(_b)\n",
    "                if len(self.queue)>self.starting_els:break\n",
    "        self.learn.agent.warming_up=False\n",
    "\n",
    "#     def after_epoch(self):\n",
    "#         print(len(self.queue))\n",
    "    def before_batch(self):\n",
    "#         print(len(self.queue))\n",
    "        b=list(self.learn.xb)+list(self.learn.yb)\n",
    "        batch=[ExperienceFirstLast(state=b[0][i],action=b[1][i],reward=b[2][i],\n",
    "                                last_state=b[3][i], done=(b[4][i] and self.max_steps!=b[6][i]),episode_reward=b[5][i],\n",
    "                                steps=b[6][i])\n",
    "                                for i in range(len(b[0]))]\n",
    "        for _b in batch: self.queue.append(_b)\n",
    "        idxs=np.random.randint(0,len(self.queue), self.bs)\n",
    "        self.learn.sample_yb=[self.queue[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SACCriticTrainer(Callback):\n",
    "    def after_batch(self): \n",
    "        self.learn.dls.bs=1\n",
    "        for d in self.learn.dls.loaders: d.bs=1\n",
    "        \n",
    "    def after_loss(self):raise CancelBatchException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "class SACLearner(AgentLearner):\n",
    "    def __init__(self,dls,agent=None,reward_scale=2,**kwargs):\n",
    "        store_attr()\n",
    "#         print(type(self.agent))\n",
    "        super().__init__(dls,agent=agent,\n",
    "                         loss_func=partial(self.agent.update_parameters,learn=self),\n",
    "                         model=agent.policy,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-5.657963</td>\n",
       "      <td>23.428571</td>\n",
       "      <td>None</td>\n",
       "      <td>23.428571</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-9.433179</td>\n",
       "      <td>37.106383</td>\n",
       "      <td>None</td>\n",
       "      <td>37.106383</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-13.423613</td>\n",
       "      <td>47.122449</td>\n",
       "      <td>None</td>\n",
       "      <td>47.122449</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-17.792173</td>\n",
       "      <td>58.442308</td>\n",
       "      <td>None</td>\n",
       "      <td>58.442308</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-23.063000</td>\n",
       "      <td>67.803571</td>\n",
       "      <td>None</td>\n",
       "      <td>67.803571</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-25.698027</td>\n",
       "      <td>74.644068</td>\n",
       "      <td>None</td>\n",
       "      <td>74.644068</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-28.577723</td>\n",
       "      <td>80.290323</td>\n",
       "      <td>None</td>\n",
       "      <td>80.290323</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-26.898724</td>\n",
       "      <td>85.125000</td>\n",
       "      <td>None</td>\n",
       "      <td>85.125000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-31.936125</td>\n",
       "      <td>89.153846</td>\n",
       "      <td>None</td>\n",
       "      <td>89.153846</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-41.527588</td>\n",
       "      <td>95.358209</td>\n",
       "      <td>None</td>\n",
       "      <td>95.358209</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-43.890781</td>\n",
       "      <td>103.521739</td>\n",
       "      <td>None</td>\n",
       "      <td>103.521739</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-46.390602</td>\n",
       "      <td>107.507042</td>\n",
       "      <td>None</td>\n",
       "      <td>107.507042</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-44.549587</td>\n",
       "      <td>111.555556</td>\n",
       "      <td>None</td>\n",
       "      <td>111.555556</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-48.796730</td>\n",
       "      <td>114.506849</td>\n",
       "      <td>None</td>\n",
       "      <td>114.506849</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-57.330715</td>\n",
       "      <td>119.554054</td>\n",
       "      <td>None</td>\n",
       "      <td>119.554054</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-57.502148</td>\n",
       "      <td>123.400000</td>\n",
       "      <td>None</td>\n",
       "      <td>123.400000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-57.664749</td>\n",
       "      <td>123.400000</td>\n",
       "      <td>None</td>\n",
       "      <td>123.400000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-62.856911</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-61.288902</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-71.129456</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-69.274841</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>-67.733025</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>-73.448891</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>-78.745071</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>-79.938591</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>-83.316002</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>-76.624931</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>-86.761978</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>-95.586105</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>-93.018265</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>None</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "# env='Pendulum-v0'\n",
    "from pybulletgym.envs import *\n",
    "\n",
    "env='InvertedPendulumPyBulletEnv-v0'\n",
    "agent=SAC(5,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.1,hidden_size=300,lr=0.003)\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,\n",
    "                               dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,n=1000,device=default_device())\n",
    "\n",
    "learner=SACLearner(dls,agent=agent,cbs=[ExperienceReplay(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),SACCriticTrainer],\n",
    "                   metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])\n",
    "learner.fit(30,lr=0.003,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_data.ipynb.\n",
      "Converted 05c_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/14a_actorcritic.sac.ipynb\n",
      "converting: /opt/project/fastrl/nbs/14b_actorcritic.diayn.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
