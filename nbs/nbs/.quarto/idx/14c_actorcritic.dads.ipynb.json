{"title":"DADS","markdown":{"yaml":{"description":"Dynamics-Aware Unsupervised Discovery of Skills","title":"DADS"},"headingText":"GMM","containsRefs":false,"markdown":"\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n> Warning: This is a partial-working version of DADS, HOWEVER a full implimentation will require a meta learner to be env agnostic. This is due to the planner needing to \n    know whether certain predicted states are more valuable than others so that it can select the right skill. Their implimenation uses GoalEnvs as opposed \nto regular envs which, is not very portable. I think after the refactor, I will use a DQN for DIAYN and DADS instead. \nI can have it so that if a goal env IS in fact being used, then it can ignore the meta learner and use the compute reward directly.\n\n> A neural net with a gaussian probability distribution component.\n\nGaussian Mixture Models parameterized by neural nets can be used for:\n- Giving the probability that a given `state` + `action` will result in `next_state`\n- Predict what the `next_state` will look like given a `state` + `action`\n\nHopefully from the list above, it is understandable *why* a GMM can be useful.\nSince it has a probabilistic component, you can avoid instances of (easily) overfitting. \n\nThe GMM is going to try to predict the next state. So lets make a simple input to optimize against.\nThis test will be of a hot airballoon. Lets see if we can have a reasonable next state prediction.\n\nFirst, lets see if the GMM can predict the next state accurately...\n\nversus...\n\nYeah the GMM is way off...\nSo lets first see if we can get it to optimize to predict a next state...\n\nThis is much better, the GMM is making a better prediction of what the next state is going to be...\n\n## Skill Dynamics\n> Now that we have a working GMM, we need a way to modify our input data to be useful for the DADS agent.\n\nThe `SkillDynamics` module is actually pretty simple. It is tasked with getting the mean and log probability from the `GMM` and providing a convenient way to get a predicted state. It additionaly has the capability of batch normalization, noise, and input shuffling.\n\nGoing back to our hot air balloon example, lets feed the states, actions, and resulting states into it...\n\nLets do a regular feed through and look at the output of our `SkillDynamics` instance.\n\nWe have 3 components: `dist`, `mean`, and `log_prob`.\\\n`dist` lets the user do more advanced operations with the results.\\\n`mean` will always return since at minimum `s` and `a` need to be passed in.\\\n`log_prob` can be returned if `sp` is not None.\n\nThe most immediately useful result is `log_prob` since it can be used for operations that need to know *what is the probability of this state occuring.*\n\nHowever, like we said earlier, `dist` is returned because we might want to do more advanced operations. For example `predict_state`...\n\nThis state prediction seems terrible! Obviously, we need to train `skill_dyn` so this is more accurate. You may notice the number of steps is 100 as opposed to 5000! This is because we are actually feeding a linear layer's output into the `GMM` as opposed to the `s+a` tensor directly. This is called a `latent space`, and these have friendlier values for the `GMM` to learn against. You will also see later that the results are a little more accurate also.\n\nMuch better! It is ok they are not exact, we want `SkillDynamics` to have a general idea on how current states and their actions can cause the state to change.\n\nBut here is a small wrench in the works. You will notice that the state spaces we are feeding into the model have values [-1, 1] which are nice and friendly to train our model on but also **highly unrealistic in the real world**. We will likely see values [-10,1000] possibly. Can we train on these? Let's convert our ballon problem into meters. We don't need to worry about actions because the values of actions are usually *within our control*.\n\nThese are much bigger values... Training a model on these without normalzing can be very hard. \nThe worst part is that in order to normalize something like states,\nyou would need to know *what is the absolute minimum and maximum this state will ever be*. This is a general problem in RL because when the agent is first\nstarting the states might start small, such as being newer the origin 0,0, but everntually as the agent gets better it starts to get to states like -20,100.\nOur model needs to convert these into nice [0,1] or [-1,1] values.\n\nSo first, lets see if we can even learn anything without batch norm...\n\nHm... It's diffinitely not amazing. The worst part is that this will likely be made worst with larger and differing batches. Lets see if using batch norm makes this better...\n\nGreat! It seems that batch norm makes `SkillDynamics` compatible with state space inputs of varying values \nthat will addapt as the agent explores its environment.\n\n## DADS Agent\n\nSome important notes...\n\nEarlier `SkillDynamics` had:\n```python\nSkillDynamics(s_dim=1,a_dim=1,n_components=2)\n```\nwhere `a_dim` was the action dimension. In reality this is the **skill** dimension. You could say that if `a_dim` is being fed raw actions, then `SkillDynamics` should be renamed `ActionDynamics`.\n\nHowever, you will see in `DADS`, that the `SkillDynamics` is always fed the skills as opposed to the primitive actions.\n\n### Marginalization methods\n> Ways of calculating the denominator.\n\nA good reference is [Probability concepts explained: Marginalisation](https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc). We need to calculate the intrinsic reward whose primary goal is to say:\n- Did this skill produce predictable results? If so, let's reward it for doing so!\n\nThe actual math from the [Dynamics-Aware Unsupervised Discovery of Skills](https://arxiv.org/abs/1907.01657) for intrinsic reward:\n\n<h5><center>\n$r_z(s,a,s')=\\log{\\frac{q_{\\phi}(s' \\mid s,z)}{\\sum_{i=1}^L q_{\\phi}(s' \\mid s,z_i)}}+\\log{L},~~ z_i \\sim p(z)$\n</center></h5>\n\nThe first part of getting the intrinsic reward is getting the denominator:\n$\\sum_{i=1}^L q_{\\phi}(s' \\mid s,z_i)$ which is going to be the *all other possible skills*.\n\nWe have the current skill that was used...\n\n...and so the other skills to compare it with are...\n\n... but this isn't very random is it? It just rolling them so let's add some random\nalternative skills to look at...\n\nBut you may ask, what if the skill is continuous? For example:\n\nWell then we have a few other disributions to sample instead!\n\n### Agent\n\nNow, the `DADS.__call__` function, unlike other agents, has a planning behavior. We want to test that as it learns,\nthe planners get better at estimating the future states its going to be in.\n\nFor now, we will be using dicrete skills until we refactor. The continuous implimentation is [MPPI (Williams et al., 2016)](https://www.cc.gatech.edu/~bboots3/files/InformationTheoreticMPC.pdf) and is used by DADS for continuous actions for the goal of making sure they have smooth transitions.\n\n> Warning: The orignal implimentation, the planning section gets the predicted states for all of the skill. How does it decide which state to go with? \nWell... It uses (probably) straight line distance to the goal state... This means that the planner in DADS will **only work with environments with linear dense goals**.\nBecause of this, instead we have an implimentation that simply looks ahead given a constant goal.\n\nBelow is a non-working example of the planner. This requires the env to have a goal that can be recalculated:\n```python\nclass Planner(object):\n    def __init__(self,init_s,agent:DADS,env,episide_horizon:int=1,planning_horizon:int=1,\n                 primitive_horizon:int=10):\n        store_attr()\n    \n    def __next__(self):\n        init_skills=torch.eye(self.agent.num_skills)\n        s=copy(self.init_s)\n        for _ in range(self.episide_horizon//self.primitive_horizon):\n            running_s=Tensor([s]*agent.num_skills)\n            computed_reward=0\n            for _ in range(self.planning_horizon):\n                pred_s=self.agent.skill_dny.predict_state(running_s[:,:self.num_skills],\n                                                          init_skills)\n                computed_reward+=env.compute_reward(running_s[:,:self.num_skills],\n                                                    pred_s)\n                \n            agent.z=torch.argmax(computed_reward)\n            s=agent.concat_obs_z(s,agent.z)\n            yield s\n```\nOnce we add a Meta Learner, we can replace the `computed_reward+=env.compute_reward` with something like a `DDQN` so that `DADS` can perform in-operation\nskill switching. \n\n## Full Training\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":["nbdev_filter"],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":4,"output-file":"14c_actorcritic.dads.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.141","theme":"cosmo","description":"Dynamics-Aware Unsupervised Discovery of Skills","title":"DADS"},"extensions":{"book":{"multiFile":true}}}}}