{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C\n",
    "\n",
    "> Synchronous Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinearA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearA2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "        return self.policy(fx),self.value(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def unbatch(batch, net, val_gamma,device='cpu'):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "#         print(exp.state.numpy().shape,int(exp.action),float(exp.reward),exp.last_state.numpy().shape if not bool(exp.done) else None,exp.done)\n",
    "        states.append(np.array(exp.state.cpu().detach().numpy(), copy=False))\n",
    "        actions.append(int(exp.action.cpu().detach()))\n",
    "        rewards.append(float(exp.reward.cpu().detach()))\n",
    "        if not exp.done:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(np.array(exp.last_state.cpu().detach().numpy(), copy=False))\n",
    "    states_v = torch.FloatTensor(states).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_v = torch.FloatTensor(last_states).to(device)\n",
    "        last_vals_v = net(last_states_v)[1]\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        rewards_np[not_done_idx] += val_gamma * last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "    return states_v, actions_t, ref_vals_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class A2CTrainer(Callback):\n",
    "    \n",
    "    def after_backward(self):\n",
    "#         print('clipping',self.learn.clip_grad,np.mean([o.detach().numpy().mean() for o in self.learn.model.parameters()]))\n",
    "        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)\n",
    "        \n",
    "    def after_step(self):\n",
    "        self.learn.loss+=self.learn.loss_policy_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def loss_func(pred,a,r,sp,d,episode_rewards,learn=None):\n",
    "    if type(learn.yb[0][0])!=ExperienceFirstLast:\n",
    "        bs=len(learn.xb[0])\n",
    "        yb=[]\n",
    "        for i in range(bs):\n",
    "            yb.append(ExperienceFirstLast(state=learn.xb[0][i],action=a[i],reward=r[i],last_state=sp[i],done=d[i],episode_reward=0))\n",
    "    else:\n",
    "        bs=len(learn.yb)\n",
    "        yb=learn.yb[0]\n",
    "    \n",
    "    s_t,a_t,r_est=unbatch(yb,learn.model,learn.discount**learn.reward_steps,default_device())\n",
    "    \n",
    "    learn.opt.zero_grad()\n",
    "    logits_v,value_v=learn.model(s_t)\n",
    "\n",
    "    loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)\n",
    "#     loss_value_v=F.mse_loss(value_v,r_est)\n",
    "\n",
    "    log_prob_v=F.log_softmax(logits_v,dim=1)\n",
    "    adv_v=r_est-value_v.squeeze(-1).detach()\n",
    "\n",
    "    log_prob_actions_v=adv_v*log_prob_v[range(bs),a_t]\n",
    "    loss_policy_v=-log_prob_actions_v.mean()\n",
    "\n",
    "    prob_v=F.softmax(logits_v,dim=1)\n",
    "    entropy_loss_v=learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()\n",
    "    \n",
    "    # calculate the polocy gradients only\n",
    "    loss_policy_v.backward(retain_graph=True)\n",
    "    \n",
    "\n",
    "    loss_v=entropy_loss_v+loss_value_v\n",
    "    \n",
    "    setattr(learn,'loss_policy_v',loss_policy_v)\n",
    "    return loss_v\n",
    "\n",
    "class A2CLearner(AgentLearner):\n",
    "    def __init__(self,dls,discount=0.99,entropy_beta=0.01,clip_grad=0.1,reward_steps=1,**kwargs):\n",
    "        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)\n",
    "        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))\n",
    "        self.discount=discount\n",
    "        self.entropy_beta=entropy_beta\n",
    "        self.reward_steps=reward_steps\n",
    "        self.clip_grad=clip_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that A2C without extra augmentation will lose memoiry very quickly. If it succeeds at 200+ reward, it will eventually **forget** what strategies got it there and you will see the graph for rewards drop back down. It is recommended to use some kind of early stopping for rewards. The more fun solution would be expermenting with ways to keep the agent stable possibly by keeping samples that it deemed as \"important\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model start 0.0030695\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.594112</td>\n",
       "      <td>26.006762</td>\n",
       "      <td>None</td>\n",
       "      <td>26.006762</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26.896385</td>\n",
       "      <td>27.038960</td>\n",
       "      <td>None</td>\n",
       "      <td>27.038960</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31.740593</td>\n",
       "      <td>50.093048</td>\n",
       "      <td>None</td>\n",
       "      <td>50.093048</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>49.083435</td>\n",
       "      <td>81.352500</td>\n",
       "      <td>None</td>\n",
       "      <td>81.352500</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>53.767052</td>\n",
       "      <td>113.584167</td>\n",
       "      <td>None</td>\n",
       "      <td>113.584167</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>72.958267</td>\n",
       "      <td>143.551667</td>\n",
       "      <td>None</td>\n",
       "      <td>143.551667</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>78.430984</td>\n",
       "      <td>162.593333</td>\n",
       "      <td>None</td>\n",
       "      <td>162.593333</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>84.090927</td>\n",
       "      <td>160.688333</td>\n",
       "      <td>None</td>\n",
       "      <td>160.688333</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>98.564392</td>\n",
       "      <td>182.206667</td>\n",
       "      <td>None</td>\n",
       "      <td>182.206667</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>110.922218</td>\n",
       "      <td>192.848333</td>\n",
       "      <td>None</td>\n",
       "      <td>192.848333</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "model=LinearA2C((4,),2)\n",
    "agent=ActorCriticAgent(model=model.to(default_device()),device=default_device())\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=4,dls_kwargs={'bs':128,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "#                       batch_tfms=lambda x:(x['s'],x),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*15,n=128*100,device=default_device())\n",
    "\n",
    "learner=A2CLearner(dls,agent=agent,cbs=[A2CTrainer],reward_steps=4,metrics=[AvgEpisodeRewardMetric()])\n",
    "print('model start',np.mean([o.cpu().detach().numpy().mean() for o in learner.model.parameters()]))\n",
    "learner.fit(10,lr=0.001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_data.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14_actorcritic.sac.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/14_actorcritic.sac.ipynb\n",
      "converting: /opt/project/fastrl/nbs/16_actorcritic.a2c.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    %reset -f\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
