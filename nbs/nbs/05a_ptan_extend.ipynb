{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ptan_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ptan Extension\n",
    "> Temporary extension of ptan. Mainly adding the option to remove `None`'s from iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import collections\n",
    "\n",
    "Experience = collections.namedtuple('Experience', ['state', 'action', 'reward', 'done','episode_reward','steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ptan\n",
    "import gym\n",
    "from queue import deque\n",
    "\n",
    "class ExperienceSource(ptan.experience.ExperienceSource):\n",
    "    def __init__(self, env, agent, steps_count=2, steps_delta=1, vectorized=False,seed=0):\n",
    "        \"\"\"\n",
    "        Create simple experience source\n",
    "        :param env: environment or list of environments to be used\n",
    "        :param agent: callable to convert batch of states into actions to take\n",
    "        :param steps_count: count of steps to track for every experience chain\n",
    "        :param steps_delta: how many steps to do between experience items\n",
    "        :param vectorized: support of vectorized envs from OpenAI universe\n",
    "        \"\"\"\n",
    "        assert isinstance(env, (gym.Env, list, tuple))\n",
    "        assert isinstance(agent, ptan.agent.BaseAgent)\n",
    "        assert isinstance(steps_count, int)\n",
    "        assert steps_count >= 1\n",
    "        assert isinstance(vectorized, bool)\n",
    "        if isinstance(env, (list, tuple)):\n",
    "            self.pool = env\n",
    "        else:\n",
    "            self.pool = [env]\n",
    "        self.agent = agent\n",
    "        self.steps_count = steps_count\n",
    "        self.steps_delta = steps_delta\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "        self.vectorized = vectorized\n",
    "        self.seed=seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, agent_states, histories, cur_rewards, cur_steps = [], [], [], [], []\n",
    "        env_lens = []\n",
    "        for env in self.pool:\n",
    "            obs = env.reset()\n",
    "            env.seed(self.seed)\n",
    "            # if the environment is vectorized, all it's output is lists of results.\n",
    "            # Details are here: https://github.com/openai/universe/blob/master/doc/env_semantics.rst\n",
    "            if self.vectorized:\n",
    "                obs_len = len(obs)\n",
    "                states.extend(obs)\n",
    "            else:\n",
    "                obs_len = 1\n",
    "                states.append(obs)\n",
    "            env_lens.append(obs_len)\n",
    "\n",
    "            for _ in range(obs_len):\n",
    "                histories.append(deque(maxlen=self.steps_count))\n",
    "                cur_rewards.append(0.0)\n",
    "                cur_steps.append(0)\n",
    "                agent_states.append(self.agent.initial_state())\n",
    "\n",
    "        iter_idx = 0\n",
    "        while True:\n",
    "            actions = [None] * len(states)\n",
    "            states_input = []\n",
    "            states_indices = []\n",
    "            for idx, state in enumerate(states):\n",
    "                if state is None:\n",
    "                    actions[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
    "                else:\n",
    "                    states_input.append(state)\n",
    "                    states_indices.append(idx)\n",
    "            if states_input:\n",
    "                states_actions, new_agent_states = self.agent(states_input, agent_states)\n",
    "                for idx, action in enumerate(states_actions):\n",
    "                    g_idx = states_indices[idx]\n",
    "                    actions[g_idx] = action\n",
    "                    agent_states[g_idx] = new_agent_states[idx]\n",
    "            grouped_actions = ptan.experience._group_list(actions, env_lens)\n",
    "\n",
    "            global_ofs = 0\n",
    "            for env_idx, (env, action_n) in enumerate(zip(self.pool, grouped_actions)):\n",
    "                if self.vectorized:\n",
    "                    next_state_n, r_n, is_done_n, _ = env.step(action_n)\n",
    "                else:\n",
    "                    next_state, r, is_done, _ = env.step(action_n[0])\n",
    "                    next_state_n, r_n, is_done_n = [next_state], [r], [is_done]\n",
    "\n",
    "                for ofs, (action, next_state, r, is_done) in enumerate(zip(action_n, next_state_n, r_n, is_done_n)):\n",
    "                    idx = global_ofs + ofs\n",
    "                    state = states[idx]\n",
    "                    history = histories[idx]\n",
    "\n",
    "                    cur_rewards[idx] += r\n",
    "                    cur_steps[idx] += 1\n",
    "                    if state is not None:\n",
    "                        history.append(Experience(state=state, action=action, reward=r, done=is_done,steps=cur_steps[idx],episode_reward=cur_rewards[idx]))\n",
    "                    if len(history) == self.steps_count and iter_idx % self.steps_delta == 0:\n",
    "                        yield tuple(history)\n",
    "                    states[idx] = next_state\n",
    "                    if is_done:\n",
    "                        # in case of very short episode (shorter than our steps count), send gathered history\n",
    "                        if 0 < len(history) < self.steps_count:\n",
    "                            yield tuple(history)\n",
    "                        # generate tail of history\n",
    "                        while len(history) > 1:\n",
    "                            history.popleft()\n",
    "                            yield tuple(history)\n",
    "                        self.total_rewards.append(cur_rewards[idx])\n",
    "                        self.total_steps.append(cur_steps[idx])\n",
    "                        cur_rewards[idx] = 0.0\n",
    "                        cur_steps[idx] = 0\n",
    "                        # vectorized envs are reset automatically\n",
    "                        states[idx] = env.reset() if not self.vectorized else None\n",
    "                        agent_states[idx] = self.agent.initial_state()\n",
    "                        history.clear()\n",
    "                global_ofs += len(action_n)\n",
    "            iter_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAgent(ptan.agent.BaseAgent):\n",
    "    def __call__(self,s,ss):return [0]*len(s),[0]*len(s)\n",
    "env_source=ExperienceSource([gym.make('CartPole-v1') for _ in range(2)],TestAgent())\n",
    "for i,o in enumerate(env_source):\n",
    "#     print(o)\n",
    "    if i>100:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ExperienceFirstLast = collections.namedtuple('ExperienceFirstLast', ('state', 'action', 'reward', 'last_state','done','episode_reward','steps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExperienceSourceFirstLast(ExperienceSource):\n",
    "    def __init__(self, env, agent, gamma, steps_count=1, steps_delta=1, vectorized=False,exclude_nones=False,seed=0):\n",
    "        assert isinstance(gamma, float)\n",
    "        super(ExperienceSourceFirstLast, self).__init__(env, agent, steps_count+1, steps_delta, vectorized=vectorized,seed=0)\n",
    "        self.gamma = gamma\n",
    "        self.steps = steps_count\n",
    "        self.exclude_nones=exclude_nones\n",
    "        self.individual_rewards=False\n",
    "        if exclude_nones and steps_count==1:\n",
    "            self.individual_rewards=True\n",
    "#             print('WARNING: steps_count==1 while exclude_nones is True. Setting steps_count==2 to avoid runtime errors.')\n",
    "            self.steps=2\n",
    "\n",
    "    def __iter__(self):\n",
    "        for exp in super(ExperienceSourceFirstLast, self).__iter__():\n",
    "            if self.exclude_nones:\n",
    "                if exp[-1].done and len(exp) <= self.steps:\n",
    "                    \n",
    "                    if len(exp)==1:continue\n",
    "                    last_state = exp[-1].state\n",
    "                    elems = exp\n",
    "                else:\n",
    "                    last_state = exp[-1].state\n",
    "                    elems = exp[:-1]\n",
    "            else:\n",
    "                if exp[-1].done and len(exp) <= self.steps:\n",
    "                    last_state = None\n",
    "                    elems = exp\n",
    "                else:\n",
    "                    last_state = exp[-1].state\n",
    "                    elems = exp[:-1]\n",
    "            total_reward = 0.0\n",
    "            if self.individual_rewards: total_reward=elems[-1].reward\n",
    "            else:\n",
    "                for e in reversed(elems):\n",
    "                    total_reward *= self.gamma\n",
    "                    total_reward += e.reward\n",
    "            yield ExperienceFirstLast(state=exp[0].state, action=exp[0].action,done=exp[-1].done,\n",
    "                                      reward=total_reward, last_state=last_state,steps=exp[-1].steps,\n",
    "                                      episode_reward=exp[-1].episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAgent(ptan.agent.BaseAgent):\n",
    "    def __call__(self,s,ss):return [0]*len(s),[0]*len(s)\n",
    "env_source=ExperienceSourceFirstLast([gym.make('CartPole-v1') for _ in range(2)],TestAgent(),gamma=0.99,steps_count=4,exclude_nones=True)\n",
    "for i,o in enumerate(env_source):\n",
    "#     print(o)\n",
    "    if i>100:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_data.ipynb.\n",
      "Converted 05c_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/05a_ptan_extend.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
