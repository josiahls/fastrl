{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp basic_agents\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "from dataclasses import asdict,dataclass\n",
    "from typing import Callable,Tuple,Union\n",
    "# from fastai.torch_core import *\n",
    "# from fastai.basic_data import *\n",
    "# from fastai.basic_train import *\n",
    "from fastai.basics import *\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from fastcore.foundation import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import pytest\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Action Selection\n",
    "\n",
    "> Methods of exploratively selecting actions based on a model state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ActionSelector:\n",
    "    \"Abstract class which converts scores to the actions.\"\n",
    "    def __call__(self,scores):raise NotImplementedError\n",
    "\n",
    "class ArgmaxActionSelector(ActionSelector):\n",
    "    \"Selects actions using argmax.\"\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        return np.argmax(scores,axis=1)\n",
    "\n",
    "@dataclass\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "    epsilon:float=0.05\n",
    "    selector:ActionSelector=ArgmaxActionSelector()\n",
    "\n",
    "    def __call__(self,scores):\n",
    "        assert isinstance(scores,np.ndarray)\n",
    "        bs,n_a=scores.shape\n",
    "        a=self.selector(scores)\n",
    "        mask=np.random.random(size=bs)<self.epsilon\n",
    "        rand_a=np.random.choice(n_a, sum(mask))\n",
    "        a[mask]=rand_a\n",
    "        return a\n",
    "\n",
    "class ProbabilityActionSelector(ActionSelector):\n",
    "    \"Converts probabilities of actions into action by sampling them.\"\n",
    "    def __call__(self,probs):\n",
    "        assert isinstance(probs,np.ndarray)\n",
    "        actions=[np.random.choice(len(prob),p=prob) for prob in probs]\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agents\n",
    "\n",
    "> Basic Agent classes for handling models and actions. Details, please ref `basic_train`\n",
    "\n",
    "There is an important difference between `Learner`'s, `nn.Module`'s, and `Agent`'s. \n",
    "\n",
    "`Learners`:\n",
    "- Ref `basic_train`\n",
    "\n",
    "`nn.Module`:\n",
    "- Contain only `pytorch` related code.\n",
    "- Function as the brain of any of these agents and are the objects to be optimized.\n",
    "- Are highly portable, however for runtime usage are too \"dumb\" or simple to be practical. If by themselves, extra code needs to wrap them to handle environments.\n",
    "\n",
    "`Agent` (`agent_core`):\n",
    "- Contain a `nn.Module` and a limited number of `fastrl` objects. Unlike `nn.Module`, these can maintain a state.\n",
    "- Function as the interface between the `nn.Module` and the environments. They have 2 goals:\n",
    "    - Convert states into something the `nn.Module` can interpret.\n",
    "    - Modify the `nn.Module` output (actions) for randomized exploration.\n",
    "- Designed to be highly portable only requiring `basic_agents` as a dependency. These should allow for easily saving, and using in environments where `fastrl` might not necessarily be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def default_states_preprocessor(s,dtype=np.float32):\n",
    "    \"Convert list of states into the form suitable for model. By default we assume Variable.\"\n",
    "    np_s=np.expand_dims(s,0) if len(np.array(s).shape)==1 else np.array(s, copy=False)\n",
    "    return torch.tensor(np_s.astype(dtype))\n",
    "\n",
    "def float32_preprocessor(s):\n",
    "    np_s=np.array(s, dtype=np.float32)\n",
    "    return torch.tensor(np_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ptan\n",
    "\n",
    "@dataclass\n",
    "class BaseAgent(ptan.agent.BaseAgent):\n",
    "    model:nn.Module=None # If None, learner will set\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl,include_batch_dim=False):\n",
    "        assert isinstance(sl,(list,np.ndarray))\n",
    "        assert isinstance(asl,(list,np.ndarray))\n",
    "        assert len(asl)==len(sl)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "@dataclass\n",
    "class TestAgent(BaseAgent):\n",
    "    env:object=None\n",
    "    def initial_state(self):return None\n",
    "    def __call__(self,sl,asl=None,include_batch_dim=False):\n",
    "        if type(self.env)!=list:return self.env.action_space.sample(),None\n",
    "        return self.env[0].action_space.sample(),None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(BaseAgent.initial_state,\"Should create initial empty state for the agent. It will be called for the start of the episode.\")\n",
    "add_docs(BaseAgent.__call__,textwrap.fill(\"\"\"Convert observations and state list `sl` into actions to take. Agent state list `asl` may also be used by the agent.\n",
    "         It is expected that `asl` is likely either going to be an internal state tracked by the agent, or it is simply a parameter used during subclassing.\n",
    "         The `include_batch_dim` should toggle whether to remove/include the batch dim of an action. Naturally, `gym` envs don't understand batch dimensions.\"\"\"\n",
    "        ))\n",
    "add_docs(BaseAgent,\"Abstract Agent interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAIAAAD9V4nPAAAGqklEQVR4nO3d3Y3TQBhA0QRtE9QRyqAOuya7DsrAdWwZ4QEJiZ+1lIBmgHvOo0eyvhfraiayc73f7xcAqHo3ewAAmEkIAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCmOfb12NfZU0Ddy+wBoO6tFt6WbfAk0GRHCECaEAKQJoQwx/mvg85FYRghBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCGGCY19PVm/LNmwSQAgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCGO3Y15PV27INmwS4CCEAcUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQx37erJ6W7ZhkwBfCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUII4xz7erJ6W7ZhkwDfCCEAaUIIQJoQApAmhACkCSE85vobZt0ZOCGEAKQJIQBpL7MHgJxPr8sPVz6+36dMAlzsCGGwnyv41kVgDCGEcU6Cp4UwixACkCaEAKQJIQBpQghAmhDCOCevSXiDAmYRQhjql8FTQZjoer/fZ88A/5I/+GHPz9t3r0x8WJ/PoQcZniaE8Ji/8wvXHmR4mqNRAACAKkej8BhHo/CfcTQKQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJoQApAmhACkCSEAaUIIQJp/nwAgzY4QgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0r4AUiBDI0jXK0kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image\n",
    "\n",
    "env=gym.make('CartPole-v1')\n",
    "agent=TestAgent(env=env)\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class DiscreteAgent(BaseAgent):\n",
    "    \"DiscreteAgent a simple discrete action selector.\"\n",
    "    a_selector:ActionSelector=None\n",
    "    device:str=None\n",
    "    preprocessor:Callable=default_states_preprocessor\n",
    "    apply_softmax:bool=False\n",
    "        \n",
    "    def safe_unbatch(self,o:np.array)->np.array:return o[0] if o.shape[0]==1 and len(o.shape)>1 else o\n",
    "    def split_v(self,v,asl): return v,asl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self,x,asl=None,include_batch_dim=True):\n",
    "        x=self.preprocessor(x) if self.preprocessor is not None else s\n",
    "        asl= np.zeros(x.shape) if asl is None or len(asl)==0 else asl\n",
    "        if torch.is_tensor(x): \n",
    "            x=x.to(self.device)\n",
    "        v=self.model(x)\n",
    "        if type(v)==tuple:v,asl=self.split_v(v,asl)\n",
    "        if self.apply_softmax:\n",
    "            v=F.softmax(v,dim=1)\n",
    "        q=v.data.cpu().numpy()\n",
    "        al=self.a_selector(q)\n",
    "        if not include_batch_dim:al=self.safe_unbatch(al).tolist()\n",
    "        \n",
    "#         print(al)\n",
    "#         if not isinstance(al,list): al=[al]\n",
    "        if include_batch_dim:\n",
    "            al=np.array(al)\n",
    "            asl=np.array(asl)\n",
    "            if len(al.shape)==0: al=al.reshape(1,)\n",
    "            if len(asl.shape)==0: asl=asl.reshape(1,)\n",
    "            return al,asl\n",
    "            \n",
    "        return (al[0],asl[0])\n",
    "\n",
    "@dataclass\n",
    "class DQNAgent(DiscreteAgent):\n",
    "    \"DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,ArgmaxActionSelector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(DQNAgent,__call__='DQNAgents will likely never have `asl` passed and used. This is however here for novel DQN implimentations.',\n",
    "         safe_unbatch='Will remove the batch dim from `o` if `o` represents a single item.',\n",
    "         split_v='In the event that `v` is a tuple, then there is multle ouputs from the `model`. Primarly used for A2C.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "agent=DQNAgent(model=nn.Sequential(nn.Linear(4,5),nn.ReLU(),nn.Linear(5,2)).to(default_device()),device=default_device())\n",
    "\n",
    "done,episode_count,max_episodes=True,0,10\n",
    "\n",
    "while True:\n",
    "    if done:s=env.reset()\n",
    "    s,done,r,_=env.step(agent(s,include_batch_dim=False)[0])\n",
    "    display.clear_output(wait=True)\n",
    "    im=env.render(mode='rgb_array')\n",
    "    new_im=PIL.Image.fromarray(im)\n",
    "    display.display(new_im)\n",
    "    if done and episode_count>max_episodes:break\n",
    "    if done:episode_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TargetNet:\n",
    "    \"Wrapper around model which provides copy of it instead of trained weights.\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "    def sync(self):self.target_model.load_state_dict(self.model.state_dict())\n",
    "    def alpha_sync(self,alpha):\n",
    "        \"Blend params of target net with params from the model.\"\n",
    "        assert isinstance(alpha,float)\n",
    "        assert 0.0<alpha<=1.0\n",
    "        state=self.model.state_dict()\n",
    "        tgt_state=self.target_model.state_dict()\n",
    "        for k, v in state.items():\n",
    "            tgt_state[k]=tgt_state[k]*alpha+(1-alpha)*v\n",
    "        self.target_model.load_state_dict(tgt_state)\n",
    "\n",
    "@dataclass\n",
    "class PolicyAgent(DiscreteAgent):\n",
    "    \"Policy agent gets action probabilities from the model and samples actions from it.\"\n",
    "    def __post_init__(self):\n",
    "        self.a_selector=ifnone(self.a_selector,ProbabilityActionSelector())\n",
    "        self.apply_softmax=True\n",
    "\n",
    "class ActorCriticAgent(PolicyAgent):\n",
    "    \"Policy agent which returns policy and value tensors from observations. Value are stored in agent's state \\\n",
    "     and could be reused for rollouts calculations by ExperienceSource.\"\n",
    "    def split_v(self,v,asl):\n",
    "#         v=v\n",
    "        return v[0],v[1].cpu().detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "# class ContinousActorCriticAgent(BaseAgent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(LinearA2C, self).__init__()\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx=x.float()\n",
    "        return self.policy(fx)\n",
    "model=LinearA2C((4,),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([[0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent=PolicyAgent(model=model.to(default_device()),device=default_device())\n",
    "agent([ 0.044186,-0.021265,0.033516,-0.011447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent=ActorCriticAgent(model=model.to(default_device()),device=default_device())\n",
    "agent(np.array([-0.04456399,  0.04653909,  0.01326909, -0.02099827]),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_data.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14_actorcritic.sac.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/14_actorcritic.sac.ipynb\n",
      "converting: /opt/project/fastrl/nbs/03_basic_agents.ipynb\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "from nbdev.showdoc import show_doc\n",
      "from fastrl.actorcritic.sac import *\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-1-016f8a27101a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactorcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msac\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m/opt/project/fastrl/fastrl/actorcritic/sac.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSACLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAgentLearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     72\u001b[0m     def __init__(self,dls,action_shape,critic_tau=0.1,discount=0.99,action_range:Tuple=None,temp=0.9,init_temp=0.1,\n",
      "\u001b[1;32m     73\u001b[0m                  actor_copy_freq=1,critic_copy_freq=1,**kwargs):\n",
      "\n",
      "\u001b[0;32m/opt/project/fastrl/fastrl/actorcritic/sac.py\u001b[0m in \u001b[0;36mSACLearner\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSACLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAgentLearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     72\u001b[0m     def __init__(self,dls,action_shape,critic_tau=0.1,discount=0.99,action_range:Tuple=None,temp=0.9,init_temp=0.1,\n",
      "\u001b[0;32m---> 73\u001b[0;31m                  actor_copy_freq=1,critic_copy_freq=1,**kwargs):\n",
      "\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mstore_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined\n",
      "NameError: name 'Tuple' is not defined\n",
      "\n",
      "Conversion failed on the following:\n",
      "14_actorcritic.sac.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    %reset -f\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
