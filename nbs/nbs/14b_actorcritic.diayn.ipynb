{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp actorcritic.diayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastrl.ptan_extension import *\n",
    "from fastrl.actorcritic.sac import *\n",
    "\n",
    "from torch.distributions import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import tsensor\n",
    "from tsensor.analysis import *\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAYN\n",
    "\n",
    "> Diversity Is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DIAYN` extends SAC to create *Skills* that can be used for a single or multiple environments. \n",
    "[(Eysenbach et al. 2018) [DIAYN] Diversity Is All You Need](https://arxiv.org/pdf/1802.06070.pdf) covers this in detail.\n",
    "\n",
    "    The general idea is that Skills should each be as diverse as possible and should not be   tied to a reward function specific to an environment.\n",
    "\n",
    "Their [project site](https://sites.google.com/view/diayn) shows several *incredible* examples of `DIAYN` finding *Skills* without any reward. The original implementation is in tensorflow and ca be found [here](https://github.com/haarnoja/sac/blob/master/sac/algos/diayn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Discriminator(Module):\n",
    "    \"`Module` for storing skills. Receives input (`num_inputs`+`num_actions`) -> `num_skills`.\"\n",
    "    def __init__(self, num_inputs,num_actions,num_skills,hidden_dim):\n",
    "        self.linear1 = nn.Linear(num_inputs+num_skills, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim,num_skills)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state.float()))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastExplainer(tsensor.explain):\n",
    "    def __init__(self,once=True,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr()\n",
    "        self.happened=False\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if self.happened and self.once:return self\n",
    "        # print(\"ON trace\", sys._getframe())\n",
    "        self.tracer = ExplainTensorTracer(self)\n",
    "        sys.settrace(self.tracer.listener)\n",
    "        frame = sys._getframe()\n",
    "        prev = frame.f_back # get block wrapped in \"with\"\n",
    "        prev.f_trace = self.tracer.listener\n",
    "        return self.tracer\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.happened=self.once\n",
    "        if self.happened: return None\n",
    "        # print(\"OFF trace\")\n",
    "        sys.settrace(None)\n",
    "        # At this point we have already tried to visualize the statement\n",
    "        # If there was no error, the visualization will look normal\n",
    "        # but a matrix operation error will show the erroneous operator highlighted.\n",
    "        # That was artificial execution of the code. Now the VM has executed\n",
    "        # the statement for real and has found the same exception. Make sure to\n",
    "        # augment the message with causal information.\n",
    "        if exc_type is None:\n",
    "            return\n",
    "        exc_frame, lib_entry_frame = tensor_lib_entry_frame(exc_traceback)\n",
    "        if lib_entry_frame is not None or is_interesting_exception(exc_value):\n",
    "            # print(\"exception:\", exc_value, exc_traceback)\n",
    "            # traceback.print_tb(exc_traceback, limit=5, file=sys.stdout)\n",
    "            module, name, filename, line, code = info(exc_frame)\n",
    "            # print('info', module, name, filename, line, code)\n",
    "            if code is not None:\n",
    "                # We've already displayed picture so just augment message\n",
    "                root, tokens = tsensor.parsing.parse(code)\n",
    "                if root is not None: # Could be syntax error in statement or code I can't handle\n",
    "                    offending_expr = None\n",
    "                    try:\n",
    "                        root.eval(exc_frame)\n",
    "                    except tsensor.ast.IncrEvalTrap as e:\n",
    "                        offending_expr = e.offending_expr\n",
    "                    augment_exception(exc_value, offending_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(SAC)\n",
    "class DIAYN(SAC):\n",
    "    def __init__(self,num_inputs,action_space,discriminator:Module=None,num_skills:int=20,\n",
    "                 find_best_skill_interval:int=10,scale_entropy:float=1,\n",
    "                 best_skill_n_rollouts:int=10,include_actions:bool=False,\n",
    "                 learn_p_z:bool=False,add_p_z:bool=True,hidden_size=100,lr=0.003,**kwargs):\n",
    "        store_attr()\n",
    "        self.num_inputs=num_inputs+self.num_skills\n",
    "        self.original_num_inputs=num_inputs\n",
    "        self.p_z=np.full(self.num_skills,1.0/self.num_skills)\n",
    "        self.discriminator=Discriminator(self.original_num_inputs,action_space.shape[0],\n",
    "                                         num_skills,hidden_size)\n",
    "        \n",
    "        self.discriminator_optim = Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "\n",
    "        self.log_p_z_episode=[]\n",
    "        self.z=0\n",
    "        self.reset_z()\n",
    "        \n",
    "        \n",
    "#         self.clarifier=FastClarify(hush_errors=False)\n",
    "        self.clarifier=FastExplainer(once=True)\n",
    "        \n",
    "        super().__init__(self.num_inputs,action_space,hidden_size=hidden_size,lr=lr,**kwargs)\n",
    "        \n",
    "    def sample_z(self):\n",
    "        \"\"\"Samples z from p(z), using probabilities in self._p_z.\"\"\"\n",
    "        return np.random.choice(self.num_skills,p=self.p_z)\n",
    "    \n",
    "    def reset_z(self): self.z=self.sample_z()\n",
    "    def __call__(self,s,asl):\n",
    "        aug_s=self.concat_obs_z(s,self.z)\n",
    "        return super().__call__(aug_s,asl)\n",
    "    \n",
    "    def concat_obs_z(self,obs,z):\n",
    "        \"\"\"Concatenates the observation to a one-hot encoding of Z.\"\"\"\n",
    "        assert np.isscalar(z)\n",
    "        if type(obs)==list and len(obs)==1: obs=obs[0]\n",
    "        if len(obs.shape)==2 and obs.shape[0]==1: obs=obs[0]\n",
    "            \n",
    "        z_one_hot=np.zeros(self.num_skills)\n",
    "        z_one_hot[z]=1\n",
    "        if type(obs)==Tensor: obs=obs.cpu()\n",
    "        return torch.FloatTensor(np.hstack([obs,z_one_hot])).reshape(1,-1)\n",
    "    \n",
    "    def skill_p(self,skill,next_state):\n",
    "        unnorm_skill_dist=self.discriminator(next_state).unsqueeze(0)\n",
    "        skill_p=F.softmax(unnorm_skill_dist)[:,skill]\n",
    "        return skill_p,unnorm_skill_dist\n",
    "    \n",
    "    def discriminator_learn(self,skill,out):\n",
    "        self.discriminator_optim.zero_grad()\n",
    "        loss=nn.CrossEntropyLoss()(out,torch.LongTensor([skill]))\n",
    "        loss.backward()\n",
    "        self.discriminator_optim.step()\n",
    "        \n",
    "    def intrinsic_reward(self,next_state):\n",
    "        skill_p,disc_out=self.skill_p(self.z,next_state)\n",
    "        intrinsic_reward=np.log(skill_p.cpu().detach()+1e-8)-np.log(self.p_z[self.z])\n",
    "#         print(skill_p,self.p_z,intrinsic_reward)\n",
    "        return intrinsic_reward,disc_out\n",
    "    \n",
    "    def update_parameters(self, *yb, learn):\n",
    "        # Sample a batch from memory\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch, mask_batch = learn.memory.sample(batch_size=batch_size)\n",
    "        batch=learn.sample_yb\n",
    "#         print(batch[0])\n",
    "#         pprint(batch)\n",
    "        state_batch=torch.stack([o.state.to(device=default_device()) for o in batch]).float()\n",
    "        next_state_batch=torch.stack([o.last_state.to(device=default_device()) for o in batch]).float()\n",
    "        action_batch=torch.stack([o.action.to(device=default_device()) for o in batch]).float()\n",
    "        reward_batch=torch.stack([o.reward.to(device=default_device()) for o in batch]).float()\n",
    "        mask_batch=torch.stack([o.done.to(device=default_device()) for o in batch]).float().unsqueeze(1)\n",
    "        \n",
    "        \n",
    "#         print(state_batch.shape,next_state_batch.shape,action_batch.shape,reward_batch.shape,mask_batch.shape)\n",
    "#         state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "#         next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "#         action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "#         reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "#         mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "#         with self.clarifier:\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + (1-mask_batch) * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if self.updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        self.updates+=1\n",
    "#         print(self.updates)\n",
    "#         print('complete')\n",
    "        return qf1_loss+ qf2_loss+ policy_loss+ alpha_loss+ alpha_tlogs\n",
    "    \n",
    "#     def update_parameters(self, *yb, learn):pass\n",
    "\n",
    "DIAYN.__doc__=\"\"\"\n",
    "`discriminator` is an additional `Module` to calculate z.\n",
    "`num_skills` is the number of skills/options to learn.\n",
    "`find_best_skill_interval` is how often to recompute the best skill.\n",
    "When finding the best skill, `best_skill_n_rollouts` determines how many rollouts to \n",
    "do per skill.\n",
    "`include_actions` determines whether to pass actions to the discriminator.\n",
    "`add_p_z` determines whether to include $\\log{p(z)}$ in the pseudo-reward.\n",
    "`scale_entropy` is the scaling factor for entropy.\n",
    "\n",
    "A few explainations of some of the internal fields:\n",
    "\n",
    "We now have `num_inputs` and `original_num_inputs`. `num_inputs` has the `num_skills` being\n",
    "added to it. This will then be used by the `SAC` parent in initializing the critic and actor.\n",
    "\n",
    "`original_num_inputs` will only be used by the `discriminator` now.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"DIAYN\" class=\"doc_header\"><code>class</code> <code>DIAYN</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>DIAYN</code>(**`num_inputs`**, **`action_space`**, **`discriminator`**:`Module`=*`None`*, **`num_skills`**:`int`=*`20`*, **`find_best_skill_interval`**:`int`=*`10`*, **`scale_entropy`**:`float`=*`1`*, **`best_skill_n_rollouts`**:`int`=*`10`*, **`include_actions`**:`bool`=*`False`*, **`learn_p_z`**:`bool`=*`False`*, **`add_p_z`**:`bool`=*`True`*, **`hidden_size`**=*`100`*, **`lr`**=*`0.003`*, **`gamma`**=*`0.99`*, **`tau`**=*`0.005`*, **`alpha`**=*`0.2`*, **`policy`**=*`'gaussian'`*, **`automatic_entropy_tuning`**=*`True`*, **`target_update_interval`**=*`1`*) :: [`SAC`](/fast-reinforcement-learning-2/actorcritic.sac.html#SAC)\n",
       "\n",
       "`discriminator` is an additional `Module` to calculate z.\n",
       "`num_skills` is the number of skills/options to learn.\n",
       "`find_best_skill_interval` is how often to recompute the best skill.\n",
       "When finding the best skill, `best_skill_n_rollouts` determines how many rollouts to \n",
       "do per skill.\n",
       "`include_actions` determines whether to pass actions to the discriminator.\n",
       "`add_p_z` determines whether to include $\\log{p(z)}$ in the pseudo-reward.\n",
       "`scale_entropy` is the scaling factor for entropy.\n",
       "\n",
       "A few explainations of some of the internal fields:\n",
       "\n",
       "We now have `num_inputs` and `original_num_inputs`. `num_inputs` has the `num_skills` being\n",
       "added to it. This will then be used by the [`SAC`](/fast-reinforcement-learning-2/actorcritic.sac.html#SAC) parent in initializing the critic and actor.\n",
       "\n",
       "`original_num_inputs` will only be used by the `discriminator` now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DIAYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiscriminatorTrainer(ExperienceReplay):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.log_p_z_episode=[]\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.agent.warming_up=True\n",
    "        while len(self.queue)<self.starting_els:\n",
    "            for i,o in enumerate(self.dls.train):\n",
    "                z=self.learn.agent.z\n",
    "                batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(o[0][i],z)[0],\n",
    "                                           action=o[1][i],\n",
    "                                           reward=o[2][i],\n",
    "                                           last_state=self.learn.agent.concat_obs_z(o[3][i],z)[0], \n",
    "                                           done=(o[4][i] and self.max_steps!=o[6][i]),\n",
    "                                           episode_reward=o[5][i],steps=o[6][i])\n",
    "                                    for i in range(len(o[0]))]\n",
    "#                 print(self.max_steps,max([o.steps for o in batch]))\n",
    "#                 print(batch[0])\n",
    "                for k in range(len(batch)):\n",
    "                    intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))\n",
    "                    self.learn.agent.discriminator_learn(self.agent.z,disc_out)\n",
    "                    batch[k]=ExperienceFirstLast(\n",
    "                        state=batch[k].state.to(device=default_device()),\n",
    "                        action=batch[k].action,\n",
    "                        reward=intrinsic_reward,\n",
    "                        last_state=batch[k].last_state.to(device=default_device()),\n",
    "                        done=batch[k].done,\n",
    "                        episode_reward=batch[k].episode_reward,\n",
    "                        steps=batch[k].steps\n",
    "                    )\n",
    "\n",
    "\n",
    "#                 print(batch[0])\n",
    "                for _b in batch:self.queue.append(_b)\n",
    "                if any([_b.done for _b in batch]): self.learn.agent.reset_z()\n",
    "                if len(self.queue)>self.starting_els:break\n",
    "        self.learn.agent.warming_up=False\n",
    "\n",
    "# #     def after_epoch(self):\n",
    "# #         print(len(self.queue))\n",
    "    def before_batch(self):\n",
    "#         print(len(self.queue))\n",
    "        b=list(self.learn.xb)+list(self.learn.yb)\n",
    "        z=self.learn.agent.z\n",
    "        batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(b[0][i],z)[0],\n",
    "                                   action=b[1][i],\n",
    "                                   reward=b[2][i],\n",
    "                                   last_state=self.learn.agent.concat_obs_z(b[3][i],z)[0], \n",
    "                                   done=(b[4][i] and self.max_steps!=b[6][i]),\n",
    "                                   episode_reward=b[5][i],steps=b[6][i])\n",
    "              for i in range(len(b[0]))]\n",
    "        \n",
    "        for k in range(len(batch)):\n",
    "            intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))\n",
    "            self.learn.agent.discriminator_learn(self.agent.z,disc_out)\n",
    "            batch[k]=ExperienceFirstLast(\n",
    "                state=batch[k].state.to(device=default_device()),\n",
    "                action=batch[k].action,\n",
    "                reward=intrinsic_reward,\n",
    "                last_state=batch[k].last_state.to(device=default_device()),\n",
    "                done=batch[k].done,\n",
    "                episode_reward=batch[k].episode_reward,\n",
    "                steps=batch[k].steps\n",
    "            )\n",
    "        \n",
    "#         print(self.learn.xb)\n",
    "        self.learn.xb=(torch.stack([e.state for e in batch]),)\n",
    "#         print(self.learn.yb)\n",
    "        self.learn.yb=(torch.stack([o.action for o in batch]),\n",
    "                       torch.stack([o.reward for o in batch]),\n",
    "                       torch.stack([o.last_state for o in batch]),\n",
    "                       torch.stack([o.done for o in batch]),\n",
    "                       torch.stack([o.episode_reward for o in batch]),\n",
    "                       torch.stack([o.steps for o in batch]))\n",
    "#         print(self.learn.yb)\n",
    "        \n",
    "        for _b in batch: self.queue.append(_b)\n",
    "        idxs=np.random.randint(0,len(self.queue), self.bs)\n",
    "        self.learn.sample_yb=[self.queue[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "add_docs(DiscriminatorTrainer,cls_doc=\"\"\"\n",
    "Subclasses ExperienceReplay for augmenting experience, and toggling the agent's skill thats,\n",
    "being used, and also does training of the discriminator.\"\"\",\n",
    "         before_fit=\"Similar to ExperienceReplay but augments the states and toggles the skill used.\",\n",
    "         before_batch=\"Similar situation as `before_fit`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional information about `DiscrimiatorTrainer`.\n",
    "\n",
    "As noted in [(Eysenbach et al. 2018)](https://arxiv.org/pdf/1802.06070.pdf) Algorithm 1, we need $log p(z)$. We accomplish this by getting the output from the discriminator $q_{\\phi}(z|s)$, taking the softmax which will scale $z$ to $[0,1]$ which is when is needed to prepresent probability $p$. Next, we scale the distribution by $log$ which is critical for calculating entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important notes from [(Eysenbach et al. 2018)](https://arxiv.org/pdf/1802.06070.pdf):\n",
    "- Hidden nn size is changed from 128 to 300 (pg 14)\n",
    "- Alpha is changed to 0.1 (pg 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-7.664313</td>\n",
       "      <td>28.307692</td>\n",
       "      <td>None</td>\n",
       "      <td>28.307692</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-11.469699</td>\n",
       "      <td>50.225806</td>\n",
       "      <td>None</td>\n",
       "      <td>50.225806</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-15.333174</td>\n",
       "      <td>66.342857</td>\n",
       "      <td>None</td>\n",
       "      <td>66.342857</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.612701</td>\n",
       "      <td>80.027027</td>\n",
       "      <td>None</td>\n",
       "      <td>80.027027</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-23.090410</td>\n",
       "      <td>88.725000</td>\n",
       "      <td>None</td>\n",
       "      <td>88.725000</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-23.701031</td>\n",
       "      <td>98.295455</td>\n",
       "      <td>None</td>\n",
       "      <td>98.295455</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-28.609037</td>\n",
       "      <td>103.938776</td>\n",
       "      <td>None</td>\n",
       "      <td>103.938776</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-30.065474</td>\n",
       "      <td>109.150943</td>\n",
       "      <td>None</td>\n",
       "      <td>109.150943</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-32.395199</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>None</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-36.382816</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>None</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-40.044987</td>\n",
       "      <td>123.122807</td>\n",
       "      <td>None</td>\n",
       "      <td>123.122807</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-42.630875</td>\n",
       "      <td>126.898305</td>\n",
       "      <td>None</td>\n",
       "      <td>126.898305</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-47.392937</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-49.802406</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-55.425098</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-56.940617</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-60.000553</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-62.084160</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-69.457932</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-67.988037</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>None</td>\n",
       "      <td>131.733333</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "from pybulletgym.envs import *\n",
    "\n",
    "env='InvertedPendulumPyBulletEnv-v0'\n",
    "agent=DIAYN(5,gym.make(env).action_space,gamma=0.99,tau=0.005,alpha=0.1,hidden_size=300,num_skills=5)\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=2,exclude_nones=True,\n",
    "                               dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),splitter=FuncSplitter(lambda x:False))\n",
    "dls=blk.dataloaders([env]*1,n=1000,device=default_device())\n",
    "\n",
    "learner=SACLearner(dls,agent=agent,cbs=[DiscriminatorTrainer(sz=1000000,bs=64,starting_els=1000,max_steps=gym.make(env)._max_episode_steps),\n",
    "                                        SACCriticTrainer],\n",
    "                   metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast)])\n",
    "learner.fit(20,lr=0.003,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# slow\n",
    "import gym\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "%matplotlib inline\n",
    "\n",
    "env=gym.make('InvertedPendulumPyBulletEnv-v0')\n",
    "s=env.reset()\n",
    "\n",
    "for z in range(5):\n",
    "    for i in range(0,100,20):\n",
    "        s=env.reset()\n",
    "        env.seed(i)\n",
    "        for _ in range(200):\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(PIL.Image.fromarray(env.render(mode='rgb_array')))\n",
    "\n",
    "            agent.z=z\n",
    "            a,_=agent(s,None)\n",
    "\n",
    "            s,r,d,_=env.step(a)\n",
    "            if d:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_data.ipynb.\n",
      "Converted 05c_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 14c_actorcritic.dads.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.v1.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/14b_actorcritic.diayn.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
