{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp qlearning.dist_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn.utils as nn_utils\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import *\n",
    "\n",
    "from fastrl.data import *\n",
    "from fastrl.async_data import *\n",
    "from fastrl.basic_agents import *\n",
    "from fastrl.learner import *\n",
    "from fastrl.metrics import *\n",
    "from fastrl.ptan_extension import *\n",
    "from fastrl.qlearning.dqn import *\n",
    "from fastrl.qlearning.dqn_target import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)\n",
    "\n",
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DistributionalDQN, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * N_ATOMS)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"supports\", torch.arange(Vmin, Vmax+DELTA_Z, DELTA_Z))\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "        self.loss_func=None\n",
    "\n",
    "    def set_opt(self,_):pass\n",
    "\n",
    "    def forward(self, x,only_qvals=False):\n",
    "        batch_size = x.size()[0]\n",
    "        fc_out = self.fc(x.float())\n",
    "        return fc_out.view(batch_size, -1, N_ATOMS)if not only_qvals else self.qvals(x)\n",
    "\n",
    "    def both(self, x):\n",
    "        cat_out = self(x)\n",
    "        probs = self.apply_softmax(cat_out)\n",
    "        weights = probs * self.supports\n",
    "        res = weights.sum(dim=2)\n",
    "        return cat_out, res\n",
    "\n",
    "    def qvals(self, x):\n",
    "        return self.both(x)[1]\n",
    "\n",
    "    def apply_softmax(self, t):\n",
    "        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes: This is an ugly function. Is there is a way we can simplify this? Will need to look at during the refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def distr_projection(next_distr, rewards, dones, Vmin, Vmax, n_atoms, gamma):\n",
    "    \"\"\"\n",
    "    Perform distribution projection aka Catergorical Algorithm from the\n",
    "    \"A Distributional Perspective on RL\" paper\n",
    "    \n",
    "    Note: direct from https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On\n",
    "    \"\"\"\n",
    "#     next_distr=next_distr.detach().cpu()\n",
    "    rewards=rewards.detach().cpu().numpy()\n",
    "    dones=dones.detach().cpu().numpy()\n",
    "    \n",
    "    batch_size = len(rewards)\n",
    "    proj_distr = np.zeros((batch_size, n_atoms), dtype=np.float32)\n",
    "    delta_z = (Vmax - Vmin) / (n_atoms - 1)\n",
    "    for atom in range(n_atoms):\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards + (Vmin + atom * delta_z) * gamma))\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n",
    "        ne_mask = u != l\n",
    "        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n",
    "        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n",
    "    if dones.any():\n",
    "        proj_distr[dones] = 0.0\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        eq_dones = dones.copy()\n",
    "        eq_dones[dones] = eq_mask\n",
    "        if eq_dones.any():\n",
    "            proj_distr[eq_dones, l[eq_mask]] = 1.0\n",
    "        ne_mask = u != l\n",
    "        ne_dones = dones.copy()\n",
    "        ne_dones[dones] = ne_mask\n",
    "        if ne_dones.any():\n",
    "            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n",
    "            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n",
    "    return proj_distr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def loss_fn(a,b): return (-a*b).sum(dim=1).mean()\n",
    "\n",
    "def calc_dist_target_batch(learn,trainer,s,a,sp,r,d):\n",
    "#     states_v = torch.tensor(states).to(device)\n",
    "#     actions_v = torch.tensor(actions).to(device)\n",
    "#     next_states_v = torch.tensor(next_states).to(device)\n",
    "\n",
    "    \n",
    "    next_distr_v, next_qvals_v = learn.target_model.both(sp)\n",
    "    next_actions = next_qvals_v.max(1)[1].data.cpu().numpy()\n",
    "    next_distr = learn.target_model.apply_softmax(next_distr_v).data.cpu().numpy()\n",
    "\n",
    "    next_best_distr = next_distr[range(s.shape[0]), next_actions]\n",
    "    # project our distribution using Bellman update\n",
    "    with torch.no_grad():\n",
    "        proj_distr = distr_projection(next_best_distr, r, d, Vmin, Vmax, N_ATOMS, learn.discount)\n",
    "\n",
    "    # calculate net output\n",
    "    distr_v = learn.model(s,).to(device=default_device())\n",
    "    state_action_values = distr_v[range(s.shape[0]), a.data]\n",
    "    state_log_sm_v = F.log_softmax(state_action_values, dim=1).to(device=default_device())\n",
    "    proj_distr_v = torch.tensor(proj_distr).to(device=default_device())\n",
    "\n",
    "#     loss_v = -state_log_sm_v * proj_distr_v\n",
    "#     print(-state_log_sm_v * proj_distr_v)\n",
    "#     print(state_log_sm_v.shape,proj_distr_v.shape)\n",
    "    return state_log_sm_v,proj_distr_v\n",
    "    \n",
    "#     state_action_values=learn.model(s.float()).gather(1, a.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#     next_state_values=trainer.get_next_state_values(sp)\n",
    "#     next_state_values[d] = 0.0\n",
    "\n",
    "#     expected_state_action_values=next_state_values.detach()*(learn.discount**learn.n_steps)+r\n",
    "#     return expected_state_action_values,state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DistDiscreteAgent(BaseAgent):\n",
    "    \"DiscreteAgent a simple discrete action selector.\"\n",
    "    a_selector:ActionSelector=None\n",
    "    device:str=None\n",
    "    preprocessor:Callable=default_states_preprocessor\n",
    "    apply_softmax:bool=False\n",
    "\n",
    "    def safe_unbatch(self,o:np.array)->np.array:return o[0] if o.shape[0]==1 and len(o.shape)>1 else o\n",
    "    def split_v(self,v,asl): return v,asl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self,x,asl=None,include_batch_dim=True):\n",
    "        x=self.preprocessor(x) if self.preprocessor is not None else s\n",
    "        asl= np.zeros(x.shape) if asl is None or len(asl)==0 else asl\n",
    "        if torch.is_tensor(x):\n",
    "            x=x.to(self.device)\n",
    "        v=self.model(x,only_qvals=True)\n",
    "        if type(v)==tuple:v,asl=self.split_v(v,asl)\n",
    "        if self.apply_softmax:\n",
    "            v=F.softmax(v,dim=1)\n",
    "        q=v.data.cpu().numpy()\n",
    "        al=self.a_selector(q)\n",
    "        if not include_batch_dim:al=self.safe_unbatch(al).tolist()\n",
    "\n",
    "#         print(al)\n",
    "#         if not isinstance(al,list): al=[al]\n",
    "        if include_batch_dim:\n",
    "            al=np.array(al)\n",
    "            asl=np.array(asl)\n",
    "            if len(al.shape)==0: al=al.reshape(1,)\n",
    "            if len(asl.shape)==0: asl=asl.reshape(1,)\n",
    "            return al,asl\n",
    "\n",
    "        return (al[0],asl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_avg_episode_r</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_avg_episode_r</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.458412</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>None</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.044310</td>\n",
       "      <td>25.164384</td>\n",
       "      <td>None</td>\n",
       "      <td>25.164384</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.571930</td>\n",
       "      <td>30.340659</td>\n",
       "      <td>None</td>\n",
       "      <td>30.340659</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.739827</td>\n",
       "      <td>36.060000</td>\n",
       "      <td>None</td>\n",
       "      <td>36.060000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.252485</td>\n",
       "      <td>41.340000</td>\n",
       "      <td>None</td>\n",
       "      <td>41.340000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.912612</td>\n",
       "      <td>47.680000</td>\n",
       "      <td>None</td>\n",
       "      <td>47.680000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.714993</td>\n",
       "      <td>53.640000</td>\n",
       "      <td>None</td>\n",
       "      <td>53.640000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.612818</td>\n",
       "      <td>59.390000</td>\n",
       "      <td>None</td>\n",
       "      <td>59.390000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.524101</td>\n",
       "      <td>66.490000</td>\n",
       "      <td>None</td>\n",
       "      <td>66.490000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.454261</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>None</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.412645</td>\n",
       "      <td>76.120000</td>\n",
       "      <td>None</td>\n",
       "      <td>76.120000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.363117</td>\n",
       "      <td>81.680000</td>\n",
       "      <td>None</td>\n",
       "      <td>81.680000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.336479</td>\n",
       "      <td>85.130000</td>\n",
       "      <td>None</td>\n",
       "      <td>85.130000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.304332</td>\n",
       "      <td>93.450000</td>\n",
       "      <td>None</td>\n",
       "      <td>93.450000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.278873</td>\n",
       "      <td>100.240000</td>\n",
       "      <td>None</td>\n",
       "      <td>100.240000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.272402</td>\n",
       "      <td>106.840000</td>\n",
       "      <td>None</td>\n",
       "      <td>106.840000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.246317</td>\n",
       "      <td>111.910000</td>\n",
       "      <td>None</td>\n",
       "      <td>111.910000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.233462</td>\n",
       "      <td>118.930000</td>\n",
       "      <td>None</td>\n",
       "      <td>118.930000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.225858</td>\n",
       "      <td>123.260000</td>\n",
       "      <td>None</td>\n",
       "      <td>123.260000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.229128</td>\n",
       "      <td>128.660000</td>\n",
       "      <td>None</td>\n",
       "      <td>128.660000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.204113</td>\n",
       "      <td>132.930000</td>\n",
       "      <td>None</td>\n",
       "      <td>132.930000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.177285</td>\n",
       "      <td>137.590000</td>\n",
       "      <td>None</td>\n",
       "      <td>137.590000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.193802</td>\n",
       "      <td>142.270000</td>\n",
       "      <td>None</td>\n",
       "      <td>142.270000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.197363</td>\n",
       "      <td>146.810000</td>\n",
       "      <td>None</td>\n",
       "      <td>146.810000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.183566</td>\n",
       "      <td>151.400000</td>\n",
       "      <td>None</td>\n",
       "      <td>151.400000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.185290</td>\n",
       "      <td>157.680000</td>\n",
       "      <td>None</td>\n",
       "      <td>157.680000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.168233</td>\n",
       "      <td>163.290000</td>\n",
       "      <td>None</td>\n",
       "      <td>163.290000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.173684</td>\n",
       "      <td>167.670000</td>\n",
       "      <td>None</td>\n",
       "      <td>167.670000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.161375</td>\n",
       "      <td>171.920000</td>\n",
       "      <td>None</td>\n",
       "      <td>171.920000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.173723</td>\n",
       "      <td>176.520000</td>\n",
       "      <td>None</td>\n",
       "      <td>176.520000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.165821</td>\n",
       "      <td>181.110000</td>\n",
       "      <td>None</td>\n",
       "      <td>181.110000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.179164</td>\n",
       "      <td>183.990000</td>\n",
       "      <td>None</td>\n",
       "      <td>183.990000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.146669</td>\n",
       "      <td>189.360000</td>\n",
       "      <td>None</td>\n",
       "      <td>189.360000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.148607</td>\n",
       "      <td>193.290000</td>\n",
       "      <td>None</td>\n",
       "      <td>193.290000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.151430</td>\n",
       "      <td>197.810000</td>\n",
       "      <td>None</td>\n",
       "      <td>197.810000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.160303</td>\n",
       "      <td>202.630000</td>\n",
       "      <td>None</td>\n",
       "      <td>202.630000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.168462</td>\n",
       "      <td>207.440000</td>\n",
       "      <td>None</td>\n",
       "      <td>207.440000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.181730</td>\n",
       "      <td>212.040000</td>\n",
       "      <td>None</td>\n",
       "      <td>212.040000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.160863</td>\n",
       "      <td>216.840000</td>\n",
       "      <td>None</td>\n",
       "      <td>216.840000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.143814</td>\n",
       "      <td>224.510000</td>\n",
       "      <td>None</td>\n",
       "      <td>224.510000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.132378</td>\n",
       "      <td>228.530000</td>\n",
       "      <td>None</td>\n",
       "      <td>228.530000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.138066</td>\n",
       "      <td>232.910000</td>\n",
       "      <td>None</td>\n",
       "      <td>232.910000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.142707</td>\n",
       "      <td>237.730000</td>\n",
       "      <td>None</td>\n",
       "      <td>237.730000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.139202</td>\n",
       "      <td>242.630000</td>\n",
       "      <td>None</td>\n",
       "      <td>242.630000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.139002</td>\n",
       "      <td>245.040000</td>\n",
       "      <td>None</td>\n",
       "      <td>245.040000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.147746</td>\n",
       "      <td>248.210000</td>\n",
       "      <td>None</td>\n",
       "      <td>248.210000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.131007</td>\n",
       "      <td>253.140000</td>\n",
       "      <td>None</td>\n",
       "      <td>253.140000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "env='CartPole-v1'\n",
    "model=DistributionalDQN((4,),2)\n",
    "agent=DistDiscreteAgent(model=model.to(default_device()),device=default_device(),\n",
    "                    a_selector=EpsilonGreedyActionSelector())\n",
    "\n",
    "block=FirstLastExperienceBlock(agent=agent,seed=0,n_steps=1,dls_kwargs={'bs':1,'num_workers':0,'verbose':False,'indexed':True,'shuffle_train':False})\n",
    "blk=IterableDataBlock(blocks=(block),\n",
    "                      splitter=FuncSplitter(lambda x:False),\n",
    "                     )\n",
    "dls=blk.dataloaders([env]*1,n=1*1000,device=default_device())\n",
    "\n",
    "learner=TargetDQNLearner(dls,agent=agent,n_steps=3,loss_func=loss_fn,cbs=[EpsilonTracker,\n",
    "                                        ExperienceReplay(sz=100000,bs=32,starting_els=32,max_steps=gym.make(env)._max_episode_steps),\n",
    "                                        TargetDQNTrainer(target_fn=calc_dist_target_batch)],metrics=[AvgEpisodeRewardMetric(experience_cls=ExperienceFirstLast,always_extend=True)])\n",
    "learner.fit(47,lr=0.0001,wd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_data.ipynb.\n",
      "Converted 05c_async_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14a_actorcritic.sac.ipynb.\n",
      "Converted 14b_actorcritic.diayn.ipynb.\n",
      "Converted 14c_actorcritic.dads.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted 19_policy_gradient.trpo.ipynb.\n",
      "Converted 20a_qlearning.dqn.ipynb.\n",
      "Converted 20b_qlearning.dqn_n_step.ipynb.\n",
      "Converted 20c_qlearning.dqn_target.ipynb.\n",
      "Converted 20d_qlearning.dqn_double.ipynb.\n",
      "Converted 20e_qlearning.dqn_noisy.ipynb.\n",
      "Converted 20f_qlearning.dqn_dueling.ipynb.\n",
      "Converted 20g_qlearning.dddqn.ipynb.\n",
      "Converted 20h_qlearning.dist_dqn.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n",
      "converting: /opt/project/fastrl/nbs/20h_qlearning.dist_dqn.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
