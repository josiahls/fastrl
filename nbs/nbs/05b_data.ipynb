{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.basics import *\n",
    "from dataclasses import field,asdict,fields\n",
    "from typing import List,Any,Dict,Callable\n",
    "from collections import deque\n",
    "import gym\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fastrl.ptan_extension import *\n",
    "\n",
    "if IN_NOTEBOOK:\n",
    "    from IPython import display\n",
    "    import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from itertools import product\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.imports import *\n",
    "from nbdev.export2html import *\n",
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Blocks\n",
    "\n",
    "> Iterable datasets for returning environment outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `TfmdSourceDL` to trigger some cleanup before doing an iteration. \n",
    "\n",
    "TODO: (Josiah): Is there a way to override the `before_iter` in the DataBlock instead? The main issue is that we need to be able to reference `self` which isn't possible when passing methods through the `DataLoader` params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def is_single_nested_tuple(b):return isinstance(b,tuple) and len(b)==1 and isinstance(b[0],tuple)\n",
    "    \n",
    "class TfmdSourceDL(TfmdDL):\n",
    "    def __init__(self,*args,**kwargs): \n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.fitting=False\n",
    "    \n",
    "    def before_iter(self):\n",
    "        super().before_iter()\n",
    "        if not self.fitting: self.dataset.reset_src()\n",
    "        \n",
    "    def create_item(self,b):\n",
    "        b=super().create_item(b)\n",
    "        return b[0] if is_single_nested_tuple(b) else b\n",
    "    \n",
    "    def after_iter(self):\n",
    "        super().after_iter()\n",
    "        if not self.fitting: self.dataset.close_src()\n",
    "    \n",
    "    def after_cancel_fit(self):\n",
    "        self.dataset.close_src()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TfmdSource` has an adjustable `__len__`. Unlike the `TfmdLists`, `TfmdSource` iters on a single item until the item raises a `SourceExhausted` exception. This means that the soruces `items` are being tracked by a separate index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates()\n",
    "class TfmdSource(TfmdLists):\n",
    "    \"A `Pipeline` of `tfms` applied to a collection of sources called `items`. Only swtches between them if they get exhausted.\"\n",
    "    def __init__(self,items,tfms,n:int=None,cycle_srcs=True,verbose=False,**kwargs):\n",
    "        self.n=n;self.cycle_srcs=cycle_srcs;self.source_idx=0;self.verbose=verbose;self.res_buffer=deque([]);self.extra_len=0\n",
    "        super().__init__(items,tfms,**kwargs)\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}: Cycling sources: {self.cycle_srcs}\\n{self.items}\\ntfms - {self.tfms.fs}\"\n",
    "    def close_src(self):\n",
    "        [t.close(self) for t in self.tfms if hasattr(t,'close')]\n",
    "        self.res_buffer.clear()\n",
    "        \n",
    "    def reset_src(self): \n",
    "        [t.reset(self) for t in self.tfms if hasattr(t,'reset')]\n",
    "        pv(f'clearing buffer: {self.res_buffer}',self.verbose)\n",
    "        self.res_buffer.clear()\n",
    "        \n",
    "    def setup(self,train_setup=True):super().setup(train_setup);self.reset_src()\n",
    "     \n",
    "    def __len__(self):\n",
    "#         return ifnone(self.n,super().__len__()) TODO (Josiah): self.n is not settable in DataBlock, and since TfmdLists gets reinit, this will not persist\n",
    "        if len(self.items)!=0 and isinstance(self.items[0],gym.Env) and self.cycle_srcs:\n",
    "            self.reset_src()\n",
    "            return self.items[0].spec.max_episode_steps+self.extra_len # TODO(Josiah): This is the only OpenAI dependent code. How do we have htis set in setup?\n",
    "        if self.n is not None: return self.n\n",
    "        if len(self.items)!=0 and hasattr(self.items[0],'n'):\n",
    "            return self.items[0].n # TODO(Josiah): Possible solution to make this more generic?\n",
    "        return super().__len__()\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if len(self.res_buffer)!=0:return self.res_buffer.popleft()\n",
    "        res=super().__getitem__(self.source_idx)\n",
    "        self.res_buffer.extend([tuple(o) for o in res])\n",
    "        return self.res_buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class IterableDataBlock(DataBlock):\n",
    "    tls_type=TfmdSource\n",
    "    def datasets(self, source, verbose=False):\n",
    "        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "        splits = (self.splitter or RandomSplitter())(items)\n",
    "        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "        tls=L([self.tls_type(items, t,verbose=verbose) for t in L(ifnone(self._combine_type_tfms(),[None]))])\n",
    "        return Datasets(items,tls=tls,splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExperienceBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SeedZeroWrapper(gym.Wrapper):\n",
    "    def reset(self,*args,**kwargs):\n",
    "        self.seed(0)\n",
    "        return super().reset(*args,**kwargs)\n",
    "\n",
    "class MakeTfm(Transform):\n",
    "    def setup(self,items:TfmdSource,train_setup=False):\n",
    "        for i in range(len(items.items)):items.items[i]=SeedZeroWrapper(gym.make(items.items[i]))\n",
    "        return super().setup(items,train_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "import ptan\n",
    "\n",
    "def env_display(env:gym.Env):\n",
    "    img=env.render('rgb_array')\n",
    "    try:display.clear_output(wait=True)\n",
    "    except AttributeError:pass\n",
    "    new_im=PIL.Image.fromarray(img)\n",
    "    display.display(new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TestAgent(ptan.agent.BaseAgent):\n",
    "    def __call__(self,s,ss):return [0]*len(s),[0]*len(s)\n",
    "\n",
    "                         \n",
    "def envlen(o:gym.Env):return o.spec.max_episode_steps\n",
    "\n",
    "@dataclass\n",
    "class ResetAndStepTfm(Transform):\n",
    "    def __init__(self,seed:int=None,agent:object=None,n_steps:int=1,steps_delta:int=1,a:Any=None,histories:Dict[str,deque]=None,\n",
    "                 s:dict=None,steps:dict=None,maxsteps:int=None,display:bool=False,hist2dict:bool=True):\n",
    "        self.seed=seed;self.agent=agent;self.n_steps=n_steps;self.steps_delta=steps_delta;self.a=a;self.histories=histories;self.hist2dict=hist2dict\n",
    "        self.maxsteps=maxsteps;self.display=display\n",
    "        self.s=ifnone(s,{})\n",
    "        self.steps=ifnone(steps,{})\n",
    "        self._exhausted=False\n",
    "        self.exp_src=None\n",
    "        self.exp_src_iter=None\n",
    "        # store_attr('n,cycle_srcs', self) TODO (Josiah): Does not seem to work?\n",
    "            \n",
    "    def setup(self,items:TfmdSource,train_setup=False):\n",
    "#         self.reset(items)\n",
    "        self.exp_src=ExperienceSource(items.items, ifnone(self.agent,TestAgent()), steps_count=self.n_steps,steps_delta=self.steps_delta)\n",
    "        self.exp_src_iter=iter(self.exp_src)\n",
    "        return super().setup(items,train_setup)\n",
    "    \n",
    "    def reset(self,items):\n",
    "        if len(items.items)==0:return\n",
    "        if items.extra_len==0:\n",
    "            items.extra_len=items.items[0].spec.max_episode_steps*(self.n_steps-1) # Extra steps to unwrap done\n",
    "        self.exp_src=ExperienceSource(items.items,ifnone(self.agent,TestAgent()), steps_count=self.n_steps,steps_delta=self.steps_delta)\n",
    "        self.exp_src_iter=iter(self.exp_src)\n",
    "        \n",
    "    def encodes(self,o:gym.Env):\n",
    "        exps=next(self.exp_src_iter)\n",
    "        return exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(ResetAndStepTfm)\n",
    "def ExperienceBlock(dls_kwargs=None,**kwargs):\n",
    "    return TransformBlock(type_tfms=[MakeTfm(),ResetAndStepTfm(**kwargs)],dl_type=TfmdSourceDL,dls_kwargs=dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_step_param,steps_delta_param in product(range(1,8),range(1,8)):\n",
    "    blk=IterableDataBlock(blocks=(ExperienceBlock(n_steps=n_step_param,steps_delta=steps_delta_param,a=0,seed=0)),\n",
    "                                  splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "    envs=[gym.make('CartPole-v1')]\n",
    "    envs=[SeedZeroWrapper(e) for e in envs]\n",
    "    exp_src=ptan.experience.ExperienceSource(envs, TestAgent(), steps_count=n_step_param,steps_delta=steps_delta_param)\n",
    "\n",
    "    dls=blk.dataloaders(['CartPole-v1'],bs=n_step_param,num_workers=0,verbose=False,\n",
    "                          indexed=True,shuffle_train=False,n=40)\n",
    "\n",
    "    fastrl_exp=[]\n",
    "    ptan_exps=[]\n",
    "    counter=0\n",
    "\n",
    "    for i,(x,e) in enumerate(zip(dls[0],exp_src)):\n",
    "\n",
    "        for i in range(x[1].shape[0]):\n",
    "            fastrl_exp.append(Experience(*tuple(el[i].cpu().detach().numpy() for el in x)))\n",
    "\n",
    "        for ptan_e in e:\n",
    "            fastrl_e=fastrl_exp[counter]\n",
    "            test_eq(fastrl_e.state,ptan_e.state)\n",
    "            test_eq(fastrl_e.done,ptan_e.done)\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FirstLastExperienceBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(ResetAndStepTfm)\n",
    "class FirstLastTfm(ResetAndStepTfm):\n",
    "    def __init__(self,discount=0.99,exclude_nones=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.discount=discount\n",
    "        self.exclude_nones=exclude_nones\n",
    "        self.exp_src_iter=None\n",
    "        self.exp_src=None\n",
    "    \n",
    "    def setup(self,items:TfmdSource,train_setup=False):\n",
    "        self.exp_src=iter(ExperienceSourceFirstLast(items.items,ifnone(self.agent,TestAgent()), steps_count=self.n_steps,steps_delta=self.steps_delta,\n",
    "                                                                    gamma=self.discount,exclude_nones=self.exclude_nones))\n",
    "        \n",
    "        self.exp_src_iter=iter(self.exp_src)\n",
    "        return super().setup(items,train_setup)\n",
    "    \n",
    "    def reset(self,items):\n",
    "#         print('reset')\n",
    "        if len(items.items)==0:return\n",
    "        if items.extra_len==0:\n",
    "            items.extra_len=items.items[0].spec.max_episode_steps*(self.n_steps-1) # Extra steps to unwrap done\n",
    "        self.exp_src=ExperienceSourceFirstLast(items.items,ifnone(self.agent,TestAgent()), steps_count=self.n_steps,steps_delta=self.steps_delta,\n",
    "                                                                    gamma=self.discount,exclude_nones=self.exclude_nones)\n",
    "        self.exp_src_iter=iter(self.exp_src)\n",
    "    \n",
    "    def encodes(self,o:gym.Env):\n",
    "        exps=next(self.exp_src_iter)\n",
    "        if exps.last_state is None or (exps.done and self.exclude_nones): \n",
    "            r=self.exp_src.pop_total_rewards()\n",
    "            if len(r)==0: r=0\n",
    "            else:         r=r[0]\n",
    "#             print(r)\n",
    "            if self.exclude_nones:\n",
    "                exps=ExperienceFirstLast(state=exps.state,reward=exps.reward,action=exps.action,last_state=exps.last_state,\n",
    "                                         done=True,episode_reward=r,steps=exps.steps)\n",
    "            else:\n",
    "                exps=ExperienceFirstLast(state=exps.state,reward=exps.reward,action=exps.action,last_state=exps.state,done=True,\n",
    "                                         episode_reward=r,steps=exps.steps)\n",
    "        else:exps=ExperienceFirstLast(state=exps.state,reward=exps.reward,action=exps.action,last_state=exps.last_state,\n",
    "                                      done=False,episode_reward=0,steps=exps.steps)\n",
    "        return [exps]\n",
    "\n",
    "@delegates(FirstLastTfm)\n",
    "def FirstLastExperienceBlock(dls_kwargs=None,**kwargs):\n",
    "    return TransformBlock(type_tfms=[MakeTfm(),FirstLastTfm(**kwargs)],dl_type=TfmdSourceDL,dls_kwargs=dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# slow\n",
    "for n_step_param,steps_delta_param in product(range(1,8),range(1,8)):\n",
    "    blk=IterableDataBlock(blocks=(FirstLastExperienceBlock(n_steps=n_step_param,steps_delta=steps_delta_param,a=0,seed=0)),\n",
    "                                  splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "    n_envs=15\n",
    "    envs=[gym.make('CartPole-v1') for _ in range(n_envs)]\n",
    "    envs=[SeedZeroWrapper(e) for e in envs]\n",
    "    exp_src=ptan.experience.ExperienceSourceFirstLast(envs, TestAgent(), gamma=0.99,steps_count=n_step_param,steps_delta=steps_delta_param)\n",
    "\n",
    "    dls=blk.dataloaders(['CartPole-v1']*n_envs,bs=n_step_param,num_workers=0,verbose=False,\n",
    "                          indexed=True,shuffle_train=False,n=40)\n",
    "\n",
    "    fastrl_exp=[]\n",
    "    ptan_exps=[]\n",
    "    counter=0\n",
    "\n",
    "    for i,(x,e) in enumerate(zip(dls[0],exp_src)):\n",
    "        for i in range(x[1].shape[0]):\n",
    "#             print(x)\n",
    "            fastrl_exp.append(ExperienceFirstLast(*tuple(el[i].cpu().detach().numpy() if not x[-3][i] or j!=3 else None for j,el in enumerate(x))))\n",
    "        for ptan_e in [e]:\n",
    "            fastrl_e=fastrl_exp[counter]\n",
    "            test_eq(fastrl_e.state,ptan_e.state)\n",
    "            test_eq(fastrl_e.last_state,ptan_e.last_state)\n",
    "            test_eq(fastrl_e.reward,ptan_e.reward)\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastrl.async_data\n",
    "from fastrl.basic_agents import *\n",
    "\n",
    "for n_step_param,steps_delta_param in product(range(1,8),range(1,8)):\n",
    "    model=fastrl.async_data._LinearA2C((4,),2)\n",
    "    model.to(default_device())\n",
    "    agent=partialler(ActorCriticAgent,model=model,device=default_device())\n",
    "    blk=IterableDataBlock(blocks=(FirstLastExperienceBlock(n_steps=n_step_param,steps_delta=steps_delta_param,a=0,seed=0,agent=agent())),\n",
    "                                  splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "    n_envs=15\n",
    "    dls=blk.dataloaders(['CartPole-v1']*n_envs,bs=n_step_param,num_workers=0,verbose=False,\n",
    "                          indexed=True,shuffle_train=False,n=40,device=default_device())\n",
    "    counter=0\n",
    "    for i,x in enumerate(dls[0]):\n",
    "#         print(x)\n",
    "        counter=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastrl.async_data\n",
    "from fastrl.basic_agents import *\n",
    "\n",
    "for n_step_param,steps_delta_param in ((3,1),):#product(range(1,8),range(1,8)):\n",
    "    model=fastrl.async_data._LinearA2C((4,),2)\n",
    "    model.to(default_device())\n",
    "    agent=partialler(ActorCriticAgent,model=model,device=default_device())\n",
    "    blk=IterableDataBlock(blocks=(FirstLastExperienceBlock(n_steps=n_step_param,steps_delta=steps_delta_param,exclude_nones=True,a=0,seed=0,agent=agent())),\n",
    "                                  splitter=FuncSplitter(lambda x:False))\n",
    "\n",
    "    n_envs=1\n",
    "    dls=blk.dataloaders(['CartPole-v1']*n_envs,bs=n_step_param,num_workers=0,verbose=False,\n",
    "                          indexed=True,shuffle_train=False,n=40,device=default_device())\n",
    "    counter=0\n",
    "    for i,x in enumerate(dls[0]):\n",
    "#         print(x)\n",
    "        counter=i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_wrappers.ipynb.\n",
      "Converted 03_basic_agents.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05a_ptan_extend.ipynb.\n",
      "Converted 05b_async_data.ipynb.\n",
      "Converted 05c_data.ipynb.\n",
      "Converted 13_metrics.ipynb.\n",
      "Converted 14_actorcritic.sac.ipynb.\n",
      "Converted 15_actorcritic.a3c_data.ipynb.\n",
      "Converted 16_actorcritic.a2c.ipynb.\n",
      "Converted 17_actorcritc.dads.ipynb.\n",
      "Converted 18_policy_gradient.ppo.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted notes.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /opt/project/fastrl/nbs/18_policy_gradient.ppo.ipynb\n",
      "converting: /opt/project/fastrl/nbs/13_metrics.ipynb\n",
      "converting: /opt/project/fastrl/nbs/03_basic_agents.ipynb\n",
      "converting: /opt/project/fastrl/nbs/14_actorcritic.sac.ipynb\n",
      "converting: /opt/project/fastrl/nbs/05c_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "from nbdev.export2html import *\n",
    "notebook2script()\n",
    "notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
