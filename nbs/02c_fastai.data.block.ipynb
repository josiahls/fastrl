{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbverbose.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "from torchdata.datapipes.iter import *\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules\n",
    "from fastrl.fastai.loop import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# DataBlock\n",
    "> High level API to quickly get your data in a DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7860b20f-8f9b-439b-a9fe-7c7b3398f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "letters = list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a159c49-08a9-45bd-a50b-99240aba2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    def __init__(self, \n",
    "                 type_tfms:Transform=None, # Executed when the DataPipe is \n",
    "                 # initialized / wif is run. Intended as a 1 time transform.\n",
    "                 item_tfms:Transform=None, # Executed on individual elements.\n",
    "                 batch_tfms:Transform=None, # Executed over a batch.\n",
    "                 dl_type:MinimumDataLoader=None, # Its recommended not to set this, \n",
    "                 # all custom behaviors should be done via callbacks. \n",
    "                 dls_kwargs:dict=None\n",
    "                ):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  = ToTensor + L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.dl_type,self.dls_kwargs = dl_type,ifnone(dls_kwargs,{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9376e8d-b8b9-47f1-b5d5-ea401cde9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CategoryBlock(vocab=None, sort=True, add_na=False):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395ad291-72d1-4344-9774-42f276872cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _merge_grouper(o):\n",
    "    if isinstance(o, LambdaType): return id(o)\n",
    "    elif isinstance(o, type): return o\n",
    "    elif (isfunction(o) or ismethod(o)): return o.__qualname__\n",
    "    return o.__class__\n",
    "\n",
    "def _merge_tfms(*tfms):\n",
    "    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n",
    "    g = groupby(concat(*tfms), _merge_grouper)\n",
    "    return L(v[-1] for k,v in g.items()).map(instantiate)\n",
    "\n",
    "def _zip(x): return L(x).zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8b9323ce-f587-4d01-803a-490037907236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBlock():\n",
    "    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n",
    "    _msg = \"\"\"If you wanted to compose several transforms in your getter don't \n",
    "    forget to wrap them in a `Pipeline`.\"\"\"\n",
    "    def __init__(self, \n",
    "                 blocks=TransformBlock, \n",
    "                 dl_type=MinimumDataLoader, \n",
    "                 get_items=None,\n",
    "                 type_tfms=None, \n",
    "                 item_tfms=None, \n",
    "                 batch_tfms=None,\n",
    "                 bs=1,\n",
    "                 splitter:Optional[Union[IterDataPipe,Callable]]=None, # If a callable, it is \n",
    "                 # assumed to split the datapipe into 2. If you want more than 2, \n",
    "                 # create a custom IterDataPipe with `__len__` for the number of splits.\n",
    "                 shuffle:bool=False\n",
    "                ):\n",
    "        blocks = L(self.blocks if blocks is None else blocks)\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        self.default_type_tfms  = _merge_tfms(*blocks.attrgot('type_tfms',  L()))\n",
    "        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        for b in blocks:\n",
    "            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.get_items = get_items\n",
    "        self.splitter = splitter\n",
    "        self.shuffle = shuffle\n",
    "        self.bs = bs\n",
    "        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "        self.new(item_tfms, batch_tfms, type_tfms)\n",
    "\n",
    "    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n",
    "        lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n",
    "\n",
    "    def new(self, item_tfms=None, batch_tfms=None, type_tfms=None):\n",
    "        self.type_tfms  = _merge_tfms(self.default_type_tfms,  type_tfms)\n",
    "        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "        return self\n",
    "    \n",
    "    def datapipes(self, \n",
    "                  source:Union[L,Callable] # Absolute initial items for create the `IterDataPipe`s from.\n",
    "                  # These should be picklable/probably uninitialized. \n",
    "                 )->List[IterDataPipe]:\n",
    "        items = source() if source is callable else source\n",
    "        items = ifnone(Pipeline(self.get_items),noop)(items)\n",
    "        \n",
    "        dps = IterableWrapper(items)\n",
    "        \n",
    "        if callable(self.splitter):     dps = dps.demux(2,self.splitter)\n",
    "        elif self.splitter is not None: dps = self.splitter(dps)\n",
    "        \n",
    "        # Regardless of the splitter or not, we will assume it to be a list to\n",
    "        # standardize the following code.\n",
    "        dps = L(dps)\n",
    "        if self.shuffle: \n",
    "            for i in range(len(dps)): dps[i] = dps[i].shuffle()\n",
    "            \n",
    "                    \n",
    "        \n",
    "        dps = dps.map(Self.map(Pipeline(self.type_tfms)))\n",
    "        \n",
    "        dps = dps.map(Cacher)\n",
    " \n",
    "        dps = dps.map(Self.map(Pipeline(self.item_tfms)))\n",
    "   \n",
    "        for i in range(len(dps)): dps[i] = dps[i].batch(self.bs)\n",
    "        dps = dps.map(Self.map(Pipeline(self.batch_tfms)))\n",
    "  \n",
    "        return dps\n",
    "        \n",
    "        \n",
    "    def dataloaders(self, source, verbose=False, **kwargs)->List[DataLoader2]:\n",
    "        dsets = self.datasets(source, verbose=verbose)\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "454964fa-9623-46f6-8668-f0c06e8c4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "class Cacher(IterDataPipe[T_co]):\n",
    "    def __init__(self, source_datapipe, **kwargs) -> None:\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __iter__(self) -> Iterator[T_co]:\n",
    "        cached_entries=[]\n",
    "        use_cache=False\n",
    "        while True:\n",
    "            if use_cache:\n",
    "                yield from cached_entries\n",
    "            else:\n",
    "                try:\n",
    "                    for v in self.source_datapipe:\n",
    "                        cached_entries.append(v)\n",
    "                        yield v\n",
    "                except StopIteration:\n",
    "                    use_cache=True\n",
    "                    cached_entries=cycle(cached_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4b6d7f9f-091c-4ae6-97d2-faa79c78e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, so not exported\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fd368b03-ce74-47d0-a031-62bfb6d74c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/fastrl_user/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41085d30-6533-417d-a11e-744c86247c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GrandparentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` to indexes 0 (train) and 1 (valid).\"\n",
    "    def _inner(o,negate=False):\n",
    "        return o.parent.parent.name==(train_name if negate else valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d8e9436-b612-465b-8844-e3cd9935f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7959d26a-5687-48cf-b3f1-42849a5b6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = DataBlock((ImageBlock(cls=PILImageBW),), \n",
    "                  get_items=get_image_files, splitter=GrandparentSplitter(),\n",
    "                  shuffle=True\n",
    "                   # get_y=parent_label\n",
    "                 )\n",
    "dsets = mnist.datapipes(untar_data(URLs.MNIST_TINY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "637b9caa-ece1-46cd-97c4-5de393267a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [<bound method PILBase.create of <class 'fastai.vision.core.PILImageBW'>>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.type_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "89727d37-5cb3-4245-8f26-9e358d5dbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.graph import traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "08e29073-ad1a-4e08-b5a5-735aa12be05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import Shuffler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e579bddf-1446-41f0-aac1-c209681b1de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<torch.utils.data.datapipes.iter.callable.MapperIterDataPipe at 0x7fba2b084f10>: {<torch.utils.data.datapipes.iter.grouping.BatcherIterDataPipe at 0x7fba2a467090>: {<torch.utils.data.datapipes.iter.callable.MapperIterDataPipe at 0x7fba2b084110>: {<__main__.Cacher at 0x7fba2b17dc10>: {<torch.utils.data.datapipes.iter.callable.MapperIterDataPipe at 0x7fba2b0843d0>: {<torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe at 0x7fba2a98c410>: {<torch.utils.data.datapipes.iter.combining._ChildDataPipe at 0x7fba2a98cf50>: {<torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe at 0x7fba2a98ca50>: {}}}}}}}}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(dsets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "884baf86-72fe-4700-85ae-45da1ec022b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorImageBW([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  83,\n",
       "           253, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6, 199,\n",
       "           254, 224,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  41, 190, 254,\n",
       "           254, 167,   8,   0,   0,   5,  37, 130, 211, 194,  14,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9, 132, 254, 254,\n",
       "           254, 254, 201, 188, 188, 196, 254, 254, 254, 251,  63,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37, 223, 242, 203,\n",
       "           180, 254, 254, 254, 254, 254, 254, 254, 237,  91,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 240, 255, 134,   0,\n",
       "            34,  78,  78,  78, 139, 254, 254, 249,  59,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,  61, 164, 254, 254,  86,   0,\n",
       "             0,   0,   0, 112, 209, 254, 222,  83,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,  24, 133, 254, 248, 162,  11,   0,\n",
       "             0,  13,  63, 215, 254, 221, 160,   9,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,  97, 254, 253, 181,   0,   0,   0,\n",
       "             0,  12, 213, 254, 251, 159,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,  77, 198,  94,   0,   0,   0,   0,\n",
       "             2, 176, 254, 254,  62,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            89, 254, 254, 138,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54,\n",
       "           235, 254, 221,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 197,\n",
       "           254, 236,  34,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9, 179, 254,\n",
       "           250,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 106, 254, 254,\n",
       "           185,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 127, 247, 254, 198,\n",
       "             5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,  10, 103, 248, 254, 222,  99,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,  73, 227, 254, 252, 129,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 245, 254, 249, 140,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 159, 254, 101,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n",
       "        dtype=torch.uint8)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=iter(dsets[1])\n",
    "next(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "03298920-39cb-4b13-b82e-a26cd5fe7118",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/3729723785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test_eq(mnist.n_inp, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastcore/fastcore/basics.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_attr_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "\n",
    "# test_eq(mnist.n_inp, 2)\n",
    "test_eq(len(dsets.train[0]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbverbose.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()\n",
    "    notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
