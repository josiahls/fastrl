{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp envs.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Callable, Any, Union, Iterable, Optional\n",
    "# Third party libs\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "# from fastrl.torch_core import *\n",
    "from fastcore.all import add_docs\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2.graph import find_dps,DataPipeGraph,DataPipe,traverse_dps\n",
    "from torchdata.dataloader2 import MultiProcessingReadingService,DataLoader2\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torchdata.datapipes.map import MapDataPipe\n",
    "# Local modules\n",
    "from fastrl.core import StepTypes,SimpleStep\n",
    "from fastrl.pipes.core import find_dps\n",
    "from fastrl.pipes.iter.nskip import NSkipper\n",
    "from fastrl.pipes.iter.nstep import NStepper,NStepFlattener\n",
    "from fastrl.pipes.iter.firstlast import FirstLastMerger\n",
    "import fastrl.pipes.iter.cacheholder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Envs Gym\n",
    "> Fastrl API for working with OpenAI Gyms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7d0a427-5a0a-4c5c-90cd-1e7b1eb7bcbf",
   "metadata": {},
   "source": [
    "### Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d10e8a-7d1f-4909-be41-5c88b0fc4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GymStepper(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "        source_datapipe:Union[Iterable,dp.iter.IterDataPipe], # Calling `next()` should produce a `gym.Env`\n",
    "        agent=None, # Optional `Agent` that accepts a `SimpleStep` to produce a list of actions.\n",
    "        seed:int=None, # Optional seed to set the env to and also random action sames if `agent==None`\n",
    "        synchronized_reset:bool=False, # Some `gym.Envs` require reset to be terminated on *all* envs before proceeding to step.\n",
    "        include_images:bool=False, # Render images from the environment\n",
    "        terminate_on_truncation:bool=True\n",
    "    ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.agent = agent\n",
    "        self._agent_iter = None\n",
    "        self.seed = seed\n",
    "        self.include_images = include_images\n",
    "        self.synchronized_reset = synchronized_reset\n",
    "        self.terminate_on_truncation = terminate_on_truncation\n",
    "        self._env_ids = {}\n",
    "        \n",
    "    def env_reset(self,\n",
    "      env:gym.Env, # The env to rest along with its numeric object id\n",
    "      env_id:int # Resets env in `self._env_ids[env_id]`\n",
    "    ) -> StepTypes.types:\n",
    "        # self.agent.reset()\n",
    "        state, info = env.reset(seed=self.seed)\n",
    "        env.action_space.seed(seed=self.seed)\n",
    "        episode_n = self._env_ids[env_id].episode_n+1 if env_id in self._env_ids else torch.tensor(1)\n",
    "\n",
    "        step = (self.no_agent_create_step if self.agent is None else self.agent.create_step)(\n",
    "            state=torch.tensor(state),\n",
    "            next_state=torch.tensor(state),\n",
    "            terminated=torch.tensor(False),\n",
    "            truncated=torch.tensor(False),\n",
    "            reward=torch.tensor(0),\n",
    "            total_reward=torch.tensor(0.),\n",
    "            env_id=torch.tensor(env_id),\n",
    "            proc_id=torch.tensor(os.getpid()),\n",
    "            step_n=torch.tensor(0),\n",
    "            episode_n=episode_n,\n",
    "            # image=env.render(mode='rgb_array') if self.include_images else torch.FloatTensor([0])\n",
    "            image=torch.tensor(env.render()) if self.include_images else torch.FloatTensor([0]),\n",
    "            raw_action=torch.FloatTensor([0])\n",
    "        )\n",
    "        self._env_ids[env_id] = step\n",
    "        return step\n",
    "    \n",
    "    def no_agent_create_step(self,**kwargs): return SimpleStep(**kwargs,batch_size=[])\n",
    "\n",
    "    def __iter__(self) -> SimpleStep:\n",
    "        for env in self.source_datapipe:\n",
    "            assert issubclass(env.__class__,gym.Env),f'Expected subclass of gym.Env, but got {env.__class__}'    \n",
    "            env_id = id(env)\n",
    "            \n",
    "            if env_id not in self._env_ids or self._env_ids[env_id].terminated:\n",
    "                if self.synchronized_reset:\n",
    "                    if env_id in self._env_ids \\\n",
    "                    and not self._env_ids[env_id].terminated \\\n",
    "                    and self._resetting_all:\n",
    "                        # If this env has already been reset, and we are currently in the \n",
    "                        # self._resetting_all phase, then skip this so we can reset all remaining envs\n",
    "                        continue\n",
    "                    elif env_id not in self._env_ids \\\n",
    "                    or all([self._env_ids[s].terminated for s in self._env_ids])\\\n",
    "                    or self._resetting_all:\n",
    "                        # If the id is not in the _env_ids, we can assume this is a fresh start.\n",
    "                        # OR \n",
    "                        # If all the envs are terminated, then we can start doing a reset operation.\n",
    "                        # OR\n",
    "                        # If we are currently resetting all the envs anyways\n",
    "                        # This means we want to reset ALL the envs before doing any steps.\n",
    "                        self.env_reset(env,env_id)\n",
    "                        # Move to the next env, eventually we will reset all the envs in sync.\n",
    "                        # then we will be able to start calling `step` for each of them.\n",
    "                        # _resetting_all is True when there are envs still \"terminated\".\n",
    "                        self._resetting_all = any([self._env_ids[s].terminated for s in self._env_ids])\n",
    "                        continue \n",
    "                    elif self._env_ids[env_id].terminated:\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise ValueError('This else should never happen.')\n",
    "                else:\n",
    "                    step = self.env_reset(env,env_id)\n",
    "            else:\n",
    "                step = self._env_ids[env_id]\n",
    "\n",
    "            action = None\n",
    "            raw_action = None\n",
    "            self._agent_iter = iter((self.agent([step]) if self.agent is not None else [env.action_space.sample()]))\n",
    "            while True:\n",
    "                try:\n",
    "                    action = next(self._agent_iter)\n",
    "                    if isinstance(action,tuple):\n",
    "                        action, raw_action = action\n",
    "                    next_state,reward,terminated,truncated,_ = env.step(\n",
    "                        self.agent.augment_actions(action) if self.agent is not None else action\n",
    "                    )\n",
    "\n",
    "                    if self.terminate_on_truncation and truncated: terminated = True\n",
    "\n",
    "                    step = (self.no_agent_create_step if self.agent is None else self.agent.create_step)(\n",
    "                        state=step.next_state.clone().detach(),\n",
    "                        next_state=torch.tensor(next_state),\n",
    "                        action=torch.tensor(action).float(),\n",
    "                        terminated=torch.tensor(terminated),\n",
    "                        truncated=torch.tensor(truncated),\n",
    "                        reward=torch.tensor(reward),\n",
    "                        total_reward=step.total_reward+reward,\n",
    "                        env_id=torch.tensor(env_id),\n",
    "                        proc_id=torch.tensor(os.getpid()),\n",
    "                        step_n=step.step_n+1,\n",
    "                        episode_n=step.episode_n,\n",
    "                        # image=env.render(mode='rgb_array') if self.include_images else torch.FloatTensor([0])\n",
    "                        image=torch.tensor(env.render()) if self.include_images else torch.FloatTensor([0]),\n",
    "                        raw_action=raw_action if raw_action is not None else torch.FloatTensor([0])\n",
    "                    )\n",
    "                    self._env_ids[env_id] = step\n",
    "                    yield step\n",
    "                    if terminated: break\n",
    "                except StopIteration:\n",
    "                    self._agent_iter = None\n",
    "                    break\n",
    "                finally:\n",
    "                    if self._agent_iter is not None:\n",
    "                        while True:\n",
    "                            try: next(self._agent_iter)\n",
    "                            except StopIteration:break\n",
    "            if action is None: \n",
    "                raise Exception('The agent produced no actions. This should never occur.')\n",
    "                \n",
    "add_docs(\n",
    "GymStepper,\n",
    "\"\"\"Accepts a `source_datapipe` or iterable whose `next()` produces a single `gym.Env`.\n",
    "    Tracks multiple envs using `id(env)`.\"\"\",\n",
    "env_reset=\"Resets a env given the env_id.\",\n",
    "no_agent_create_step=\"If there is no agent for creating the step output, then `GymStepper` will create its own\",\n",
    "reset=\"Resets the env's back to original str types to avoid pickling issues.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138af400-0508-48db-86a6-52aa5caf6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Used here to avoid UserWarnings related to gym complaining about bounding box / action space format.\n",
    "# There must be a bug in the CartPole-v1 env that is causing this to show. Also couldnt figure out the \n",
    "# regex, so instead we filter on the lineno, which is line 98.\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning,lineno=98)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2299ed48-9725-418e-b1cd-28c2545a9998",
   "metadata": {},
   "source": [
    "## Iteration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9076f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.agents.core import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfa51c-a3d7-4335-a059-cd75c37d4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: False\n",
    "class ConstantRunner(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,constant=1,array_nestings=0): \n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.agent_base = find_dps(traverse_dps(self.source_datapipe),AgentBase)\n",
    "        self.constant = constant\n",
    "        self.array_nestings = array_nestings\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for o in self.source_datapipe: \n",
    "            try: \n",
    "                if self.array_nestings==0: yield self.constant\n",
    "                else:\n",
    "                    yield [self.constant]*self.array_nestings\n",
    "            except Exception:\n",
    "                print('Failed on ',o)\n",
    "                raise\n",
    "\n",
    "agent = AgentBase(None,[])\n",
    "agent = ConstantRunner(agent)\n",
    "# Tests whether the agent is correctly being exhuasted / reset.\n",
    "split_1,split_2 = agent.fork(2)\n",
    "agent = split_1.zip(split_2)\n",
    "agent = AgentHead(agent)\n",
    "\n",
    "pipe = dp.iter.IterableWrapper(['CartPole-v1']*3)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "# pipe = TypeTransformer(pipe,[GymTypeTransform])\n",
    "# pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = pipe.pickleable_in_memory_cache()\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,agent=agent,seed=0)\n",
    "\n",
    "# pd.DataFrame([step for step,_ in zip(*(pipe,range(10)))])[['state','next_state','action','terminated']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c9a9e",
   "metadata": {},
   "source": [
    "There show be no resetting buffer warnings. If the pipe is ended early, it should reset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3648f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: False\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    # If you always want to trigger the warning, regardless of filter configurations:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    for step in pipe:\n",
    "        break\n",
    "    for step in pipe:\n",
    "        break\n",
    "    \n",
    "    # If any warnings are triggered, fail the test\n",
    "    assert len(w) == 0,f'There should be no warnings, but got: {[o.message for o in w]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c347866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87be925-e36c-4812-81e6-a01d0edc226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper(['CartPole-v1']*3)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "# pipe = TypeTransformer(pipe,[GymTypeTransform])\n",
    "# pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = pipe.pickleable_in_memory_cache()\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,seed=0,include_images=True)\n",
    "\n",
    "pd.DataFrame([step for step,_ in zip(*(pipe,range(10)))])[['state','next_state','action','terminated','env_id','step_n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2 import DataLoader2,MultiProcessingReadingService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4af44-2fbf-4f6a-a8cf-e8ff0b0d8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(pipe,worker_info): \n",
    "    torch.manual_seed(0)\n",
    "    return pipe\n",
    "\n",
    "dl = DataLoader2(pipe,reading_service=MultiProcessingReadingService(\n",
    "        num_workers = 0,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    ")\n",
    "\n",
    "pd.DataFrame([step for step,_ in zip(*(dl,range(10)))])[['state','next_state','action','terminated','env_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ad7a7-bf50-4f57-9369-9752642e8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.map.Mapper(['CartPole-v1']*3)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle()\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "\n",
    "pd.DataFrame([step for step,_ in zip(*(dl,range(10)))])[['state','next_state','action','terminated']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56d7aada-cf08-4a63-9a33-df998d9f4122",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba2c6b52-3384-4fb1-a8f9-f23ad6ba8b70",
   "metadata": {},
   "source": [
    "We create 3 envs and put a max iteration count at 180. Each env will run for 18 steps before ending, which means\n",
    "we expect there to be 10 total episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bfdc1-863a-41b5-a604-12851dce4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['CartPole-v1']*3\n",
    "n_episodes = 3\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "# We want to cycle through the envs enough times that their epsiode sum to 9, 3 episodes each\n",
    "pipe = pipe.cycle(count=(18*len(envs))) \n",
    "pipe = GymStepper(pipe,seed=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13b2d31-0b99-4bc7-9348-0562e71d3326",
   "metadata": {},
   "source": [
    "All the of the environments should reach max 18 steps given a seed of 0...\\\n",
    "The total number of iterations should be `( 18 * n_envs) * n_episodes_per_env = 162`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3746a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.core import test_len\n",
    "from fastcore.all import test_eq,test_ne\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa4ff2-37b3-47b0-9d0e-962561f04c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(pipe)\n",
    "gsteps = dict(groupby(steps,lambda o:int(o.step_n)))\n",
    "test_len(gsteps.keys(),18)\n",
    "pd.DataFrame([step for step in steps])[['state','terminated','env_id','episode_n','step_n']][::10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c022610-ce31-4431-8fe8-b983b480c8d4",
   "metadata": {},
   "source": [
    "All of the step groups should be the same length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc48432-dc8a-43b9-aafa-1402920a2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "gsteps = {k:list(v) for k,v in groupby(steps,lambda o:int(o.step_n))}\n",
    "for name,group in gsteps.items():\n",
    "    if group_sz is None: group_sz = len(group)\n",
    "    else:                assert len(group)==group_sz,f' Got lengths {len(group)} and {group_sz} for {name}.\\n\\n{group}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbda18b2-7875-40c1-9a1d-5f10a2f6b62e",
   "metadata": {},
   "source": [
    "Each step group's state and next_states should match across envs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1022cc-9e03-4d89-ab61-83918486e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "for name,group in gsteps.items():\n",
    "    e1 = group[0]\n",
    "    for other in group[1:]: test_eq(e1.state,other.state)\n",
    "    for other in group[1:]: test_eq(e1.next_state,other.next_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91a6e989-d957-44ec-bfd3-b2052221a2d1",
   "metadata": {},
   "source": [
    "Each step group value should not show up/be duplicated in any other step groups..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47f8ba-6d27-4261-9b2f-48783df2a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "for name,group in gsteps.items():\n",
    "    e1 = group[0]\n",
    "    for other_name,other_group in gsteps.items():\n",
    "        if other_name==name: continue\n",
    "        for other in other_group[1:]: test_ne(e1.state,other.state)\n",
    "        for other in other_group[1:]: test_ne(e1.next_state,other.next_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d57f579-7e72-4be5-a104-c5d8c1f9933f",
   "metadata": {},
   "source": [
    "Given 3 envs, single steps, epsiodes of 18 steps in len, 3 episodes each, run for 162 iterations, we should\n",
    "expect there to be 9 dones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.all import L,Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a296b3-c9bf-4ba3-8b3d-71fe7ac8b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sum([o.terminated for o in steps]),torch.tensor([9]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8231127f-e50d-4432-893e-e7d914335487",
   "metadata": {},
   "source": [
    "The max episode numbers for each env should sum to 9 where for each env, it should reach and finish 3 episodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed659afa-2443-410c-8fb1-4eeae320af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsteps = {k:list(v) for k,v in groupby(steps,lambda o:int(o.env_id))}\n",
    "test_len(gsteps.keys(),3)\n",
    "env1,env2,env3 = L(gsteps.values()).map(L).map(Self.map(Self.episode_n()).map(int))\n",
    "test_eq(max(env1)+max(env2)+max(env3),9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ad4328-f201-4489-a8db-cc791c5f41d7",
   "metadata": {},
   "source": [
    "### Test the `synchronized_reset` param...\n",
    "> In this case, we will have iterate through the 3 envs without producing a step on warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d5939-8434-4487-95e5-f9b6f815f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['CartPole-v1']*3\n",
    "n_episodes = 3\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "# We want to cycle through the envs enough times that their epsiode sum to 9, 3 episodes each\n",
    "# We add an additional +3 cycles since `synchronized_reset` cycles through the envs additional times\n",
    "# to make sure they are all reset prior to stepping\n",
    "pipe = pipe.cycle(count=(18*len(envs))+3) \n",
    "pipe = GymStepper(pipe,seed=0,synchronized_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0695c-ee99-46c9-b55f-3a13898803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(pipe)\n",
    "gsteps = {k:list(v) for k,v in groupby(steps,lambda o:int(o.step_n))}\n",
    "test_len(gsteps.keys(),18)\n",
    "pd.DataFrame([step for step in steps])[['state','terminated','env_id','episode_n','step_n']][::10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ed9fe0a-842d-44e7-98b3-fa58591556ab",
   "metadata": {},
   "source": [
    "All of the step groups should be the same length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b4ef0-d2e9-426c-b27a-fba0e2063d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "for name,group in gsteps.items():\n",
    "    if group_sz is None: group_sz = len(group)\n",
    "    else:                assert len(group)==group_sz,f' Got lengths {len(group)} and {group_sz} for {name}.\\n\\n{group}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8f2fde6-c905-4686-8062-34d25c962191",
   "metadata": {},
   "source": [
    "Each step group's state and next_states should match across envs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f101c4-122e-4b16-9960-be3bc8425f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "for name,group in gsteps.items():\n",
    "    e1 = group[0]\n",
    "    for other in group[1:]: test_eq(e1.state,other.state)\n",
    "    for other in group[1:]: test_eq(e1.next_state,other.next_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60111c68-0dab-44f2-b903-ff8c69e49504",
   "metadata": {},
   "source": [
    "Each step group value should not show up/be duplicated in any other step groups..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c248e-90ed-47d7-9d7f-ebbb910e573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sz = None\n",
    "for name,group in gsteps.items():\n",
    "    e1 = group[0]\n",
    "    for other_name,other_group in gsteps.items():\n",
    "        if other_name==name: continue\n",
    "        for other in other_group[1:]: test_ne(e1.state,other.state)\n",
    "        for other in other_group[1:]: test_ne(e1.next_state,other.next_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "272c4186-6dbb-4ebf-9eee-23f7963dc4b5",
   "metadata": {},
   "source": [
    "Given 3 envs, single steps, epsiodes of 18 steps in len, 3 episodes each, run for 162 iterations, we should\n",
    "expect there to be 9 dones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4040f9-30d1-4758-b3e6-2482521eec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sum([o.terminated for o in steps]),torch.tensor([9]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b695305-fa8a-4e45-a027-cac8af3b3910",
   "metadata": {},
   "source": [
    "The max episode numbers for each env should sum to 9 where for each env, it should reach and finish 3 episodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef84010-469d-45be-b485-201a062d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsteps = groupby(steps,lambda o:int(o.env_id))\n",
    "gsteps = {k:list(v) for k,v in groupby(steps,lambda o:int(o.env_id))}\n",
    "test_len(gsteps.keys(),3)\n",
    "env1,env2,env3 = L(gsteps.values()).map(L).map(Self.map(Self.episode_n()).map(int))\n",
    "test_eq(max(env1)+max(env2)+max(env3),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aab9f5-716b-42ea-8d8f-84af11a1529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle(count=(18*len(envs))) \n",
    "# Turn off the seed so that some envs end before others...\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "steps = list(pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f643b49-f7da-48a3-bbb4-4c993dcca8c0",
   "metadata": {},
   "source": [
    "Since the seed is turned off the only properties we are to expect are:\n",
    "    \n",
    "    - If an env finishes, no steps from that env should be seen until all 9 of the other envs finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee911ea-5366-464c-b850-ce6306b0caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronized_reset_checker(steps):\n",
    "    env_id_done_tracker = {}\n",
    "    did_syncs_happen = False\n",
    "    for d,env_id,idx in [(bool(o.terminated),int(o.env_id),i) for i,o in enumerate(steps)]:\n",
    "\n",
    "        if d: \n",
    "            env_id_done_tracker[env_id] = idx\n",
    "            continue\n",
    "\n",
    "        if env_id in env_id_done_tracker:\n",
    "            if len(env_id_done_tracker)!=len(envs):\n",
    "                raise Exception(f'env_id {env_id} was iterated through when it should not have been! idx: {idx}')\n",
    "        if len(env_id_done_tracker)==len(envs):\n",
    "            did_syncs_happen = True\n",
    "            env_id_done_tracker = {}\n",
    "\n",
    "    if not did_syncs_happen: \n",
    "        raise Exception('There should have at least been 1 time where all the envs had to reset, which did not happen.')\n",
    "synchronized_reset_checker(steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1baf90-84c5-4eea-b76f-d515d7ee9c35",
   "metadata": {},
   "source": [
    "For sanity, we should expect that without `synchronized_reset` envs will be reset and stepped through before other \n",
    "envs are reset, `synchronized_reset_checker` should fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149717a6-e87b-4b2d-9ac8-5e65205a1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle(count=(18*len(envs))) \n",
    "# Turn off the seed so that some envs end before others...\n",
    "pipe = GymStepper(pipe)\n",
    "steps = list(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93519e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import ExceptionExpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faee17-a7c5-438b-9450-bf10d1777866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ExceptionExpected(regex='was iterated through when it should not have been'):\n",
    "    synchronized_reset_checker(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79b56a-af03-414c-a7b1-26115498bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def GymDataPipe(\n",
    "    source,\n",
    "    agent:DataPipe=None, # An AgentHead\n",
    "    seed:Optional[int]=None, # The seed for the gym to use\n",
    "    # Used by `NStepper`, outputs tuples / chunks of assiciated steps\n",
    "    nsteps:int=1, \n",
    "    # Used by `NSkipper` to skip a certain number of steps (agent still gets called for each)\n",
    "    nskips:int=1,\n",
    "    # Whether when nsteps>1 to merge it into a single `StepType`\n",
    "    firstlast:bool=False,\n",
    "    # The batch size, which is different from `nsteps` in that firstlast will be \n",
    "    # run prior to batching, and a batch of steps might come from multiple envs,\n",
    "    # where nstep is associated with a single env\n",
    "    bs:int=1,\n",
    "    # The prefered default is for the pipeline to be infinate, and the learner\n",
    "    # decides how much to iter. If this is not None, then the pipeline will run for \n",
    "    # that number of `n`\n",
    "    n:Optional[int]=None,\n",
    "    # Whether to reset all the envs at the same time as opposed to reseting them \n",
    "    # the moment an episode ends. \n",
    "    synchronized_reset:bool=False,\n",
    "    # Should be used only for validation / logging, will grab a render of the gym\n",
    "    # and assign to the `StepType` image field. This data should not be used for training.\n",
    "    # If it images are needed for training, then you should wrap the env instead. \n",
    "    include_images:bool=False,\n",
    "    # If an environment truncates, terminate it.\n",
    "    terminate_on_truncation:bool=True\n",
    ") -> Callable:\n",
    "    \"Basic `gymnasium` `DataPipeGraph` with first-last, nstep, and nskip capability\"\n",
    "    pipe = dp.iter.IterableWrapper(source)\n",
    "    if include_images:\n",
    "        pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "    else:\n",
    "        pipe = pipe.map(gym.make)\n",
    "    # pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "    pipe = pipe.pickleable_in_memory_cache()\n",
    "    pipe = pipe.cycle() # Cycle through the envs inf\n",
    "    pipe = GymStepper(pipe,agent=agent,seed=seed,\n",
    "                        include_images=include_images,\n",
    "                        terminate_on_truncation=terminate_on_truncation,\n",
    "                        synchronized_reset=synchronized_reset)\n",
    "    if nskips!=1: pipe = NSkipper(pipe,n=nskips)\n",
    "    if nsteps!=1:\n",
    "        pipe = NStepper(pipe,n=nsteps)\n",
    "        if firstlast:\n",
    "            pipe = FirstLastMerger(pipe)\n",
    "        else:\n",
    "            pipe = NStepFlattener(pipe) # We dont want to flatten if using FirstLastMerger\n",
    "    if n is not None: pipe = pipe.header(limit=n)\n",
    "    pipe  = pipe.batch(batch_size=bs)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ded40-bef6-41a7-840d-32bf7af03d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "envs = ['CartPole-v1']*3\n",
    "pipes = GymDataPipe(envs,None,seed=0,nsteps=2,nskips=2,firstlast=True,bs=1,n=100)\n",
    "pd.DataFrame([o[0] for o in pipes])[['state','action','terminated','reward','step_n']][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c111e-411f-4830-ab67-85ca71922615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "envs = ['CartPole-v1']*3\n",
    "pipes = GymDataPipe(envs,None,nsteps=1,nskips=1,firstlast=True,bs=1,n=100)\n",
    "\n",
    "\n",
    "pd.DataFrame([o[0] for o in pipes])[['state','action','terminated','reward']][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058b830-8570-40a7-a2cc-8da8d06d6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['CartPole-v1']*3\n",
    "pipes = GymDataPipe(envs,None,nsteps=2,nskips=1,firstlast=True,bs=1,n=100)\n",
    "\n",
    "pd.DataFrame([o[0] for o in pipes])[['state','action','terminated','reward']][:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb5e2b4-36b3-4362-9e07-7325edcf5e46",
   "metadata": {},
   "source": [
    "## Multi Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.dataloader2 import DataLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b187248-5fb9-4ebe-a441-4abd62f9d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../external_run_scripts/spawn_multiproc.py\n",
    "import torch\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2 import DataLoader2,MultiProcessingReadingService\n",
    "       \n",
    "class PointlessLoop(dp.iter.IterDataPipe):\n",
    "    def __init__(self,datapipe=None):\n",
    "        self.datapipe = datapipe\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield torch.LongTensor(4).detach().clone()\n",
    "            \n",
    "\n",
    "if __name__=='__main__':\n",
    "    from torch.multiprocessing import Pool, Process, set_start_method\n",
    "    try:\n",
    "         set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    pipe = PointlessLoop()\n",
    "    pipe = pipe.header(limit=10)\n",
    "    dls = [DataLoader2(pipe,\n",
    "            reading_service=MultiProcessingReadingService(\n",
    "                num_workers = 2\n",
    "            ))]\n",
    "    # Setup the Learner\n",
    "    print('type: ',type(dls[0]))\n",
    "    for o in dls[0]:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4172b2-404c-4034-8d58-f8cc2b7acfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python ../external_run_scripts/spawn_multiproc.py\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83517423-7c47-446a-8742-394b7a0354bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
