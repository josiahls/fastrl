{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.dqn.categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from copy import deepcopy\n",
    "from typing import Optional,Callable,Tuple\n",
    "# Third party libs\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2.graph import traverse_dps,DataPipe\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from fastcore.all import store_attr,ifnone\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "# Local modulesf\n",
    "from fastrl.torch_core import default_device,to_detach,evaluating\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.agents.core import StepFieldSelector,SimpleModelRunner,NumpyConverter\n",
    "from fastrl.agents.discrete import EpsilonCollector,PyPrimativeConverter,ArgMaxer,EpsilonSelector\n",
    "from fastrl.memory.experience_replay import ExperienceReplay\n",
    "from fastrl.loggers.core import BatchCollector,EpochCollector\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe\n",
    "from fastrl.agents.core import AgentHead,AgentBase\n",
    "from fastrl.agents.dqn.basic import (\n",
    "    LossCollector,\n",
    "    RollingTerminatedRewardCollector,\n",
    "    EpisodeCollector,\n",
    "    StepBatcher,\n",
    "    TargetCalc,\n",
    "    LossCalc,\n",
    "    ModelLearnCalc,\n",
    "    DQN,\n",
    "    DQNAgent\n",
    ")\n",
    "from fastrl.agents.dqn.target import TargetModelUpdater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Categorical DQN\n",
    "> An implimentation of a DQN that uses distributions to represent Q from the paper [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d8044-4a06-48e1-aa09-c8bd43f8b615",
   "metadata": {},
   "source": [
    "The Categorical DQN can be summarized as:\n",
    "    \n",
    "    Instead of action outputs being single Q values, they are instead distributions of `N` size.\n",
    "    \n",
    "We start off with the idea of atoms and supports. A support acts as a mask over the output action\n",
    "distributions. This is illistrated by the equations and the corresponding functions.\n",
    "\n",
    "We start with the equation...\n",
    "\n",
    "$$\n",
    "{\\large\n",
    "Z_{\\theta}(z,a) = z_i \\quad w.p. \\: p_i(x,a):= \\frac{ e^{\\theta_i(x,a)}} {\\sum_j{e^{\\theta_j(x,a)}}} \n",
    "}\n",
    "$$\n",
    "\n",
    "... which shows that the end of our neural net model needs to be squished to be a proper probability.\n",
    "It also defines $z_i$ which is a support we will define very soon.\n",
    "Below is the implimentation of the right side equation for $p_i(x,a)$\n",
    "\n",
    "An important note is that $\\frac{ e^{\\theta_i(x,a)}} {\\sum_j{e^{\\theta_j(x,a)}}} $ is just:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb11dbe-fad6-4715-a7e6-d4fe48903376",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dcbbb-66c6-439c-af95-4c19104c147d",
   "metadata": {},
   "source": [
    "We pretend that the output of the neural net is of shape `(batch_sz,n_actions,n_atoms)`. In this instance,\n",
    "there is only one action. This implies that $Z_{\\theta}$ is just $z_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee746f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2669a-1ebc-42c7-b8ee-90086e9194df",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=nn.Softmax(dim=1)(torch.randn(1,51,1))[0] # Action 0\n",
    "plt.plot(out.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d11e62-bbb4-4d4b-8b8c-62f84df030f2",
   "metadata": {},
   "source": [
    "The next function describes how propabilities are calculated from the neural net output. The equation describes a $z_i$ which\n",
    "is explained by:\n",
    "$$\n",
    "\\{z_i = V_{min} + i\\Delta z : 0 \\leq i < N \\}, \\: \\Delta z := \\frac{V_{max} - V_{min}}{N - 1}\n",
    "$$\n",
    "\n",
    "Where $V_{max}$, $V_{min}$, and $N$ are constants that we define. Note that $N$ is the number of atoms.\n",
    "So what does a $z_i$ look like? We will define this in code below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de331a86-565d-450d-8a4c-c361165aede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_support(v_min=-10,v_max=10,n_atoms=51)->Tuple[torch.Tensor,float]:\n",
    "    \"Creates the support and returns the z_delta that was used.\"\n",
    "    z_delta=(v_max-v_min)/(n_atoms-1)\n",
    "    return (torch.Tensor([i*z_delta for i in range(n_atoms)])+v_min,z_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb09134-ac1c-400b-9f57-ccb376210eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_dist,z_delta=create_support()\n",
    "print('z_delta: ',z_delta)\n",
    "plt.plot(support_dist.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc3734-5323-4604-9521-621578a69cd6",
   "metadata": {},
   "source": [
    "This is a single $z_i$ in $Z_{\\theta}$. The number of $z_i$s is equal to the number of actions that the DQN is operating with.\n",
    "\n",
    "> Note: Josiah: Is this always the case? Could there be only $z_0$ and multiple actions?\n",
    "\n",
    "Ok! Hopefully this wasn't too bad to go through. We basically normalized the neural net output to be nicer to deal with, \n",
    "and created/initialized a (bunch) of increasing arrays that we are calling discrete distributions i.e. output from `create_support`.\n",
    "\n",
    "Now for the fun part! We have this giant ass update equation:\n",
    "\n",
    "$$\n",
    "{\\large\n",
    "(\\Phi\\hat{\\mathcal{T}}Z_{\\theta}(x,a))_i = \\sum_{j=0}^{N-1} \\left[ 1 - \\frac{ | \\mathcal{T}z_j |_{V_{min}}^{V_{max}} - z_i }{ \\Delta z } \\right]_0^1 p_j(x^{\\prime},\\pi(x^{\\prime}))\n",
    "}\n",
    "$$\n",
    "Good god... and we also have\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{T}}z_j := r + \\gamma z_j\n",
    "$$\n",
    "\n",
    "where, to quote the paper:\n",
    "\n",
    "<center>\n",
    "\"for each atom $z_j$, [and] then distribute its probability $ p_j(x^{\\prime},\\pi(x^{\\prime})) $ to the immediate neighbors of $ \\hat{\\mathcal{T}}z_j $\"\n",
    "</center><br>\n",
    "\n",
    "I highly recommend reading pg6 in the paper for a fuller explaination. I was originally wondering what the difference was between $\\pi$ and simple $\\theta$, which the main difference is that $\\pi$ is a greedy action selection i.e. we run argmax to get the action.\n",
    "\n",
    "This was a lot! Luckily they have a re-formalation in algorithmic form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829cc01e-f4a9-4a23-8b8c-7054cb5fceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_update(v_min,v_max,n_atoms,support,delta_z,model,reward,gamma,action,next_state):\n",
    "    t_q=(support*Softmax(model(next_state).gather(action))).sum()\n",
    "    a_star=torch.argmax(t_q)\n",
    "    \n",
    "    m=torch.zeros((N,)) # m_i = 0 where i in 1,...,N-1\n",
    "    \n",
    "    for j in range(n_atoms):\n",
    "        # Compute the projection of $ \\hat{\\mathcal{T}}z_j $ onto support $ z_j $\n",
    "        target_z=torch.clamp(reward+gamma*support[:,j],v_min,v_max)\n",
    "        b_j=(target_z-v_min)/delta_z # b_j in [0,N-1]\n",
    "        l=torch.floor(b_j)\n",
    "        u=torch.ceil(b_j)\n",
    "        # Distribute probability of $ \\hat{\\mathcal{T}}z_j $\n",
    "        m[:,l]=m[:,l]+a_star*(u-b)\n",
    "        m[:,u]=m[:,u]+a_star*(b-l)\n",
    "    return # Some cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73c496-b76d-4cea-875b-5f7227ff38b9",
   "metadata": {},
   "source": [
    "There is a small problem with the above equation. This was a (fairly) literal convertion from `Algorithm 1` in the paper to Python.\n",
    "There are some problems here:<br>\n",
    "- The current setup doesnt handle batches\n",
    "- Some of the variables are a little vague\n",
    "- Does not handle terminal states\n",
    "\n",
    "Lets rename these! We will instead have:<br>\n",
    "$$\n",
    "m\\_i    \\rightarrow projection\\\\\n",
    "a\\_star \\rightarrow next\\_action\\\\\n",
    "b\\_j    \\rightarrow support\\_value\\\\\n",
    "l      \\rightarrow support\\_left\\\\\n",
    "u      \\rightarrow support\\_right\\\\\n",
    "$$\n",
    "\n",
    "So lets revise the problem and pretend that we have a 2 action model, batch size of 8, where the last element has a reward of 0, and where\n",
    "left actions are -1, while right actions are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcfaa3-ec9a-48b0-8d12-3ee02557297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4db35-36f1-45c0-982f-b3ed352ab9c7",
   "metadata": {},
   "source": [
    "So for a single action we would have a distribution like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364c85c-a667-4925-aac2-446176b8b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Normal(0,1).sample((51,)).numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb81c7c-3ca1-420c-b29d-e354c0d1d120",
   "metadata": {},
   "source": [
    "So since our model has 2 actions that it can pick, we create some distributions for them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28b1d9-2e96-41cf-b96a-7d3c742e158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_left=torch.vstack([Normal(0.5,1).sample((1,51)),Normal(0.5,0.1).sample((1,51))]).unsqueeze(0)\n",
    "dist_right=torch.vstack([Normal(0.5,0.1).sample((1,51)),Normal(0.5,1).sample((1,51))]).unsqueeze(0)\n",
    "(dist_left.shape,dist_right.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acaefe-0064-4736-bf28-ca5ab20e6c92",
   "metadata": {},
   "source": [
    "...where the $[1, 2, 51]$ is $[batch, action, n\\_atoms]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6677ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6785b-8deb-4339-814f-48ac5cae0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out=torch.vstack([copy([dist_left,dist_right][i%2==0]) for i in range(1,9)]).to(device=default_device())\n",
    "(model_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90334790-beae-41f8-9f52-457bcba85493",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_model_out=model_out.sum(dim=2);summed_model_out=nn.Softmax(dim=1)(summed_model_out).to(device=default_device())\n",
    "(summed_model_out.shape,summed_model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c6d46-acb9-4096-8132-856c82b54dfc",
   "metadata": {},
   "source": [
    "So when we sum/normalize the distrubtions per batch, per action, we get an output that looks like your typical dqn output...\n",
    "\n",
    "We can also treat this like a regular DQN and do an argmax to get actions like usual..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b8ae0-fb02-4224-8ef8-23126f214bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=torch.argmax(summed_model_out,dim=1).reshape(-1,1).to(device=default_device());actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4378b-385f-422f-a7fb-e60ec38a2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=actions;rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86b45a-bc6c-4fb4-a7b6-4bdf52e17884",
   "metadata": {},
   "outputs": [],
   "source": [
    "dones=torch.Tensor().new_zeros((8,1)).bool().to(device=default_device());dones[-1][0]=1;dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349e28b-39e4-417e-a3d7-13885f82f776",
   "metadata": {},
   "source": [
    "So lets decompose the `categorical_update` above into something easier to read. First we will note the author's original algorithm:\n",
    "\n",
    "<img src=\"../../images/10e_agents.dqn.categorical_algorithm1.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec07e56-5fcc-4035-9774-035866a226cd",
   "metadata": {},
   "source": [
    "We can break this into 3 different functions:<br> \n",
    "    - getting the Q<br>\n",
    "    - calculating the update<br>\n",
    "    - calculating the loss\n",
    "    \n",
    "We will start with the $Q(x_{t+1},a):=\\sum_iz_ip_i(x_{t_1},a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce7888-a342-4707-99ac-c78758369db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CategoricalDQN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_sz:int,\n",
    "                 action_sz:int,\n",
    "                 n_atoms:int=51,\n",
    "                 hidden=512,\n",
    "                 v_min=-10,\n",
    "                 v_max=10,\n",
    "                 head_layer=nn.Linear,\n",
    "                 activation_fn=nn.ReLU\n",
    "                ):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_sz,hidden),\n",
    "            activation_fn(),\n",
    "            head_layer(hidden,action_sz*n_atoms),\n",
    "        )\n",
    "        self.supports,self.z_delta = create_support(v_min=v_min,v_max=v_max,n_atoms=n_atoms)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs) \n",
    "        self.supports = self.supports.to(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def forward(self,x): return self.layers(x).view(x.shape[0],self.action_sz,self.n_atoms)\n",
    "    def policy(self,x):  return (self.supports*self.p(x)).mean(dim=2)\n",
    "    def q(self,x):       return (self.supports*self.p(x)).sum(dim=2)\n",
    "    def p(self,x):       return self.softmax(self(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4dbd72-0ed2-49a3-8fec-fdde6ace68a0",
   "metadata": {},
   "source": [
    "The `CategoricalDQN.q` function gets us 90% of the way to the equation above. However, \n",
    "you will notice that that equation is for a specific action. We will handle this in the actual update function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e634c3-0d14-4386-8914-3bd09478c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn=CategoricalDQN(4,2).to(device=default_device())\n",
    "dqn(torch.randn(8,4).to(device=default_device())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4bdf3-ed63-4f00-b671-a8f15c2a4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.q(torch.randn(8,4).to(device=default_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e45fe-42e4-4537-85b1-8aa3d5d8e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.policy(torch.randn(8,4).to(device=default_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5129b-c076-4956-8544-27bb2a95cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def distribute(projection,left,right,support_value,p_a,atom,done):\n",
    "    \"Does: m_l <- m_l + p_j(x_{t+1},a*)(u - b_j) operation for non-final states.\"\n",
    "    diffs=torch.hstack((support_value-left,right-support_value))\n",
    "    # If they are the same location, then just split the value in half, and add twice\n",
    "    diffs[(left==right).reshape(-1,),:]=0.5 \n",
    "    mask=~done.reshape(-1,)\n",
    "    \n",
    "    left_v=projection[mask].gather(1,left[mask])+(p_a[:,atom][mask]*diffs[:,0][mask]).reshape(-1,1)\n",
    "    right_v=projection[mask].gather(1,right[mask])+(p_a[:,atom][mask]*diffs[:,1][mask]).reshape(-1,1)\n",
    "    \n",
    "    projection[mask]=projection[mask].scatter(dim=1,index=left[mask],src=left_v)\n",
    "    projection[mask]=projection[mask].scatter(dim=1,index=right[mask],src=right_v)\n",
    "\n",
    "def final_distribute(projection,left,right,support_value,p_a,atom,done):\n",
    "    \"Does: m_l <- m_l + p_j(x_{t+1},a*)(u - b_j) operation for final states.\"\n",
    "    diffs=torch.hstack((support_value-left,right-support_value))\n",
    "    # If they are the location, then just split the value in half, and add twice\n",
    "    diffs[(left==right).reshape(-1,),:]=0.5 \n",
    "    mask=done.reshape(-1,)\n",
    "    \n",
    "    left_v=diffs[:,0].reshape(-1,1)\n",
    "    right_v=projection[mask].gather(1,right)+diffs[:,1].reshape(-1,1)\n",
    "\n",
    "    projection[mask]=0.0\n",
    "    projection[mask]=projection[mask].scatter(dim=1,index=left,src=left_v)\n",
    "    projection[mask]=projection[mask].scatter(dim=1,index=right,src=right_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87567db0-65da-4bc5-8358-af7306c69a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def categorical_update(support,delta_z,q,p,actions,rewards,dones,v_min=-10,\n",
    "                       v_max=10,n_atoms=51,gamma=0.99,passes=None,nsteps=1,debug=False):\n",
    "    if debug:\n",
    "        print(f'support: {support.shape}, delta_z: {delta_z}, q: {q.shape}\\n',\n",
    "              f'\\tp: {p.shape}, actions: {actions.shape}, rewards: {rewards.shape}\\n',\n",
    "              f'\\tdones: {dones.shape}')\n",
    "    \n",
    "    bs=q.shape[0]\n",
    "    passes=ifnone(passes,n_atoms)\n",
    "    # Do this outside of the loop so we only have to do it once\n",
    "    # Represents: p_j(x_{t+1},a*)\n",
    "    p_a=p[torch.arange(bs),actions.reshape(-1,)]#.reshape(-1,1)\n",
    "    # get a*\n",
    "    next_actions=torch.argmax(q,dim=1)\n",
    "    # m_i = 0 for i in [0,N-1]\n",
    "    projection=torch.zeros((bs,n_atoms)).to(device=support.device)\n",
    "    # j in [0, N - 1]\n",
    "    for atom in range(0,passes):\n",
    "        # Tz_j <- [r_t + gamma * z_j]_v_min^v_max\n",
    "        target_z=rewards+(gamma**nsteps)*support[atom]\n",
    "        target_z[dones.reshape(-1)]=rewards[dones.reshape(-1)].float()\n",
    "        target_z=torch.clamp(target_z,v_min,v_max)\n",
    "        # b_j <- (Tz_j - Vmin)/delta_z\n",
    "        support_value=(target_z-v_min)/delta_z\n",
    "        # l <- floor(b_j), u <- ceil(b_j)\n",
    "        left,right=support_value.floor().long(),support_value.ceil().long()\n",
    "        # m_l <- m_l + p_j(x_{t+1},a*)(u - b_j)\n",
    "        distribute(projection,left,right,support_value,p_a,atom,dones)\n",
    "\n",
    "    if dones.sum()>=1:\n",
    "        target_z=rewards[dones.reshape(-1)]\n",
    "        target_z=torch.clamp(target_z,v_min,v_max)\n",
    "        support_value=(target_z-v_min)/delta_z\n",
    "        left,right=support_value.floor().long(),support_value.ceil().long()\n",
    "        final_distribute(projection,left,right,support_value,p_a,atom,dones)\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc2573-9cf8-407f-931d-230785f1329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def show_q_distribution(cat_dist,title='Update Distributions'):\n",
    "    \"`cat_dist` being shape: (bs,n_atoms)\"\n",
    "    from IPython.display import HTML\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "    fig = go.Figure(data=[go.Surface(z=to_detach(cat_dist).numpy())])\n",
    "    fig.update_layout(title=title,autosize=False,\n",
    "                      width=500, height=500,#template='plotly_dark',\n",
    "                      margin=dict(l=65, r=50, b=80, t=90))\n",
    "    # return HTML(fig.to_html())\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce5e88-da37-41d4-a8d0-a5205a9e2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=categorical_update(dqn.supports,dqn.z_delta,summed_model_out,\n",
    "                          nn.Softmax(dim=2)(model_out),actions,rewards,dones,passes=None)\n",
    "show_q_distribution(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a233df-edc0-4551-b450-8bc9c3e3c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=dqn.q(torch.randn(8,4).to(device=default_device()))\n",
    "p=dqn.p(torch.randn(8,4).to(device=default_device()))\n",
    "\n",
    "output=categorical_update(dqn.supports,dqn.z_delta,q,p,actions,rewards,dones)\n",
    "show_q_distribution(output,title='Real Model Update Distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff7f35-68f3-4191-af16-9746c5d2e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def PartialCrossEntropy(p,q): return (-p*q).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c3433-2c59-4275-b9cb-b88358cc0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CategoricalTargetQCalc(dp.iter.IterDataPipe):\n",
    "    debug = False\n",
    "    \n",
    "    def __init__(self,source_datapipe,discount=0.99,nsteps=1,device=None,double_dqn_strategy=False):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.device = device\n",
    "        self.double_dqn_strategy = double_dqn_strategy\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        if 'device' in kwargs: self.device = kwargs.get('device',None)\n",
    "        return self\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:            \n",
    "            if self.device is not None: batch = batch.device(self.device)\n",
    "            actions = batch.action.long()\n",
    "            self.learner.done_mask = batch.terminated.reshape(-1,)\n",
    "            with torch.no_grad():\n",
    "                target_actions = actions\n",
    "                if self.double_dqn_strategy:\n",
    "                    target_actions = self.learner.model.policy(batch.next_state).argmax(dim=1).reshape(-1,)\n",
    "                    target_actions = target_actions.long()\n",
    "                \n",
    "                distribution_m = categorical_update(self.learner.target_model.supports,\n",
    "                                          self.learner.target_model.z_delta,\n",
    "                                          self.learner.target_model.q(batch.next_state),\n",
    "                                          self.learner.target_model.p(batch.next_state),\n",
    "                                          target_actions,batch.reward,\n",
    "                                          self.learner.done_mask,nsteps=self.nsteps,\n",
    "                                          debug=self.debug)\n",
    "            self.learner.target_qs = distribution_m # (distribution_m,)\n",
    "            v = self.learner.model(batch.state)\n",
    "            self.learner.pred_raw = v[np.arange(v.shape[0]),actions.reshape(-1,),:]\n",
    "            self.learner.pred = F.log_softmax(self.learner.pred_raw,dim=1)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040a01b-0dd0-4281-b1e5-13ac4362ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiModelRunner(dp.iter.IterDataPipe):\n",
    "    \"If a model contains multiple models, then we support selecting a sub model.\"\n",
    "    def __init__(self,\n",
    "                 source_datapipe,\n",
    "                 device:Optional[str]=None,\n",
    "                 model_name:str='policy'\n",
    "                ): \n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.agent_base = find_dp(traverse_dps(self.source_datapipe),AgentBase)\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.model = getattr(self.agent_base.model,self.model_name)\n",
    "        for x in self.source_datapipe:\n",
    "            if self.device is not None: \n",
    "                x = x.to(torch.device(self.device))\n",
    "            if len(x.shape)==1: x = x.unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with evaluating(self.agent_base.model):\n",
    "                    res = self.model(x)\n",
    "            yield res\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        if 'device' in kwargs: self.device = kwargs.get('device',None)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "DataPipeAugmentationFn = Callable[[DataPipe],Optional[DataPipe]]\n",
    "\n",
    "def CategoricalDQNAgent(\n",
    "    model,\n",
    "    min_epsilon=0.02,\n",
    "    max_epsilon=1,\n",
    "    max_steps=1000,\n",
    "    device='cpu',\n",
    "    do_logging:bool=False\n",
    ")->AgentHead:\n",
    "    agent_base = AgentBase(model)\n",
    "    agent_base = StepFieldSelector(agent_base,field='next_state')\n",
    "    agent_base = MultiModelRunner(agent_base).to(device=device)\n",
    "    agent,raw_agent = agent_base.fork(2)\n",
    "    agent = agent.map(torch.clone)\n",
    "    agent = ArgMaxer(agent)\n",
    "    agent = EpsilonSelector(agent,min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps,device=device)\n",
    "    if do_logging: \n",
    "        agent = EpsilonCollector(agent).catch_records()\n",
    "    agent = ArgMaxer(agent,only_idx=True)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = PyPrimativeConverter(agent)\n",
    "    agent = agent.zip(raw_agent)\n",
    "    agent = AgentHead(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed03064-9284-47da-8294-f97c3590eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.envs.gym import GymDataPipe\n",
    "from fastrl.loggers.core import ProgressBarLogger\n",
    "from fastrl.dataloading.core import dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e74f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DQNCategoricalLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    logger_bases:Optional[Callable]=None,\n",
    "    loss_func=PartialCrossEntropy,\n",
    "    opt=optim.AdamW,\n",
    "    lr=0.005,\n",
    "    bs=128,\n",
    "    max_sz=10000,\n",
    "    nsteps=1,\n",
    "    device=None,\n",
    "    batches=None,\n",
    "    target_sync=300,\n",
    "    double_dqn_strategy=True\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(model,dls=dls[0])\n",
    "    learner = BatchCollector(learner,batches=batches)\n",
    "    learner = EpochCollector(learner)\n",
    "    if logger_bases: \n",
    "        learner = logger_bases(learner)\n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner)\n",
    "    learner = learner.catch_records()\n",
    "    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz)\n",
    "    learner = StepBatcher(learner,device=device)\n",
    "    learner = CategoricalTargetQCalc(learner,nsteps=nsteps,double_dqn_strategy=double_dqn_strategy).to(device=device)\n",
    "    # learner = TargetCalc(learner,nsteps=nsteps)\n",
    "    learner = LossCalc(learner,loss_func=loss_func)\n",
    "    learner = ModelLearnCalc(learner,opt=opt(model.parameters(),lr=lr))\n",
    "    learner = TargetModelUpdater(learner,target_sync=target_sync)\n",
    "    if logger_bases: \n",
    "        learner = LossCollector(learner).catch_records()\n",
    "\n",
    "    if len(dls)==2:\n",
    "        val_learner = LearnerBase(model,dls[1])\n",
    "        val_learner = BatchCollector(val_learner,batches=batches)\n",
    "        val_learner = EpochCollector(val_learner).catch_records(drop=True)\n",
    "        val_learner = VSCodeDataPipe(val_learner)\n",
    "        return LearnerHead((learner,val_learner),model)\n",
    "    else:\n",
    "        return LearnerHead(learner,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89f614-19bf-4d27-8551-0e205a80224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "# Setup Loggers\n",
    "def logger_bases(pipe):\n",
    "    pipe = pipe.dump_records()\n",
    "    pipe = ProgressBarLogger(pipe)\n",
    "    return pipe\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = CategoricalDQN(4,2)\n",
    "# Setup the Agent\n",
    "agent = CategoricalDQNAgent(model,do_logging=True,min_epsilon=0.02,max_epsilon=1,max_steps=5000)\n",
    "# Setup the DataBlock\n",
    "train_pipe = GymDataPipe(\n",
    "    ['CartPole-v1']*1,\n",
    "    agent=agent,\n",
    "    nsteps=2,\n",
    "    nskips=2,\n",
    "    firstlast=True,\n",
    "    bs=1\n",
    ")\n",
    "dls = dataloaders(train_pipe)\n",
    "# Setup the Learner\n",
    "learner = DQNCategoricalLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    logger_bases=logger_bases,\n",
    "    bs=128,\n",
    "    max_sz=100_000,\n",
    "    nsteps=2,\n",
    "    lr=0.001,\n",
    "    batches=1000,\n",
    "    target_sync=300\n",
    ")\n",
    "learner.fit(7)\n",
    "\n",
    "\n",
    "#|eval: false\n",
    "# # Setup Loggers\n",
    "# logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "#                  batch_on_pipe=BatchCollector)\n",
    "\n",
    "# # Setup up the core NN\n",
    "# torch.manual_seed(0)\n",
    "# model = CategoricalDQN(4,2).to(device='cuda')\n",
    "# # Setup the Agent\n",
    "# agent = DQNAgent(model,[logger_base],max_steps=4000,device='cuda',\n",
    "#                 dp_augmentation_fns=[\n",
    "#                     MultiModelRunner.replace_dp(device='cuda')\n",
    "# ])\n",
    "# # Setup the DataBlock\n",
    "# block = DataBlock(\n",
    "#     GymTransformBlock(agent=agent,nsteps=2,nskips=2,firstlast=True), # We basically merge 2 steps into 1 and skip. \n",
    "#     (GymTransformBlock(agent=agent,nsteps=2,nskips=2,firstlast=True,n=100,include_images=True),VSCodeTransformBlock())\n",
    "# )\n",
    "# # pipes = L(block.datapipes(['CartPole-v1']*1,n=10))\n",
    "# dls = L(block.dataloaders(['CartPole-v1']*1))\n",
    "# # Setup the Learner\n",
    "# learner = DQNLearner(model,dls,logger_bases=[logger_base],bs=128,\n",
    "#                      batches=1000,\n",
    "#                      loss_func = PartialCrossEntropy,\n",
    "#                      device='cuda',\n",
    "#                      max_sz=100_000,\n",
    "#                      lr=0.001,\n",
    "#                      dp_augmentation_fns=[\n",
    "#                          TargetModelUpdater.insert_dp(),\n",
    "#                          CategoricalTargetQCalc.replace_remove_dp(device='cuda',nsteps=2,double_dqn_strategy=True)\n",
    "#                      ])\n",
    "# learner.fit(1)\n",
    "# learner.fit(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af609734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f974d-076c-4acf-b293-728dd6c0e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# from IPython.display import HTML\n",
    "# import plotly.express as px\n",
    "# from torchdata.dataloader2.graph import find_dps,traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0398a3-0d81-45c4-ba79-a06272ec45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def show_q(cat_dist,title='Update Distributions'):\n",
    "    \"`cat_dist` being shape: (bs,n_atoms)\"\n",
    "    from IPython.display import HTML\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    distributions=to_detach(cat_dist).numpy()\n",
    "    actions=np.argmax(distributions,axis=1).reshape(-1,)\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        specs=[[{\"type\": \"surface\"},{\"type\": \"xy\"}]])\n",
    "    fig.add_trace(go.Surface(z=distributions, showscale=False),row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(actions)),y=actions),row=1, col=2)\n",
    "    fig.update_layout(title=title,autosize=False,\n",
    "                      width=1000, height=500,#template='plotly_dark',\n",
    "                      margin=dict(l=65, r=50, b=80, t=90))\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f49e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
