{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.dqn.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from collections import deque\n",
    "from typing import Callable,Optional,List\n",
    "# Third party libs\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2.graph import traverse_dps,DataPipe\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "# Local modules\n",
    "from fastrl.agents.core import AgentHead,AgentBase\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.memory.experience_replay import ExperienceReplay\n",
    "from fastrl.agents.core import StepFieldSelector,SimpleModelRunner,NumpyConverter\n",
    "from fastrl.agents.discrete import EpsilonCollector,PyPrimativeConverter,ArgMaxer,EpsilonSelector\n",
    "from fastrl.loggers.core import (\n",
    "    Record,BatchCollector,EpochCollector,RollingTerminatedRewardCollector,EpisodeCollector,ProgressBarLogger\n",
    ")\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead,StepBatcher\n",
    "from fastrl.torch_core import Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# DQN Basic\n",
    "> Core DQN modules, pipes, and tooling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f18f79cf-4763-451d-92a2-817b532ba9d8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241ea68-611d-4cdf-abdb-4e2a58af23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DQN(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_sz:int,  # The input dim of the state\n",
    "            action_sz:int, # The output dim of the actions\n",
    "            hidden=512,    # Number of neurons connected between the 2 input/output layers\n",
    "            head_layer:Module=nn.Linear, # DQN extensions such as Dueling DQNs have custom heads\n",
    "            activition_fn:Module=nn.ReLU # The activiation fn used by `DQN`\n",
    "        ):\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(state_sz,hidden),\n",
    "            activition_fn(),\n",
    "            head_layer(hidden,action_sz),\n",
    "        )\n",
    "    def forward(self,x): return self.layers(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4cfe338-de27-4c61-a18d-d74ea33fde37",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480529a0-a23f-425a-a165-d4915c96c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "DataPipeAugmentationFn = Callable[[DataPipe],Optional[DataPipe]]\n",
    "\n",
    "def DQNAgent(\n",
    "    model,\n",
    "    min_epsilon=0.02,\n",
    "    max_epsilon=1,\n",
    "    max_steps=1000,\n",
    "    device='cpu',\n",
    "    do_logging:bool=False\n",
    ")->AgentHead:\n",
    "    agent_base = AgentBase(model)\n",
    "    agent_base = StepFieldSelector(agent_base,field='next_state')\n",
    "    agent_base = SimpleModelRunner(agent_base).to(device=device)\n",
    "    agent,raw_agent = agent_base.fork(2)\n",
    "    agent = agent.map(torch.clone)\n",
    "    agent = ArgMaxer(agent)\n",
    "    agent = EpsilonSelector(agent,min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps,device=device)\n",
    "    if do_logging: \n",
    "        agent = EpsilonCollector(agent).catch_records()\n",
    "    agent = ArgMaxer(agent,only_idx=True)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = PyPrimativeConverter(agent)\n",
    "    agent = agent.zip(raw_agent)\n",
    "    agent = AgentHead(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f871a-64bb-42ac-868c-d37b0282c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "\n",
    "agent = DQNAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ed064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import test_eq\n",
    "from fastrl.core import SimpleStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c6eac-7851-4539-aa91-295479d05cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([1,2,3,4]).float()\n",
    "step = SimpleStep(next_state=input_tensor)\n",
    "\n",
    "for i in range(10):\n",
    "    for action in agent([step]):\n",
    "        print(action)\n",
    "    \n",
    "test_eq(input_tensor,torch.tensor([1., 2., 3., 4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47810afd-4430-4ba6-a840-3d8e5878aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.envs.gym import GymDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61711f0-7a1d-4b0e-bd81-46ff7547179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "\n",
    "agent = DQNAgent(model,do_logging=True)\n",
    "\n",
    "pipe = GymDataPipe(['CartPole-v1']*1,agent=agent,n=10)\n",
    "pipe = BatchCollector(pipe,batches=5)\n",
    "pipe = EpochCollector(pipe,epochs=10).dump_records()\n",
    "# Setup Logger\n",
    "pipe = ProgressBarLogger(pipe)\n",
    "\n",
    "list(pipe);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c98be0-6288-443a-b4ab-9390fbe3081c",
   "metadata": {},
   "source": [
    "## Training DataPipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578d4b1-fa02-49b4-a02a-6bb881a45d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.done_mask = batch.terminated.reshape(-1,)\n",
    "            self.learner.next_q = self.learner.model(batch.next_state)\n",
    "            self.learner.next_q = self.learner.next_q.max(dim=1).values.reshape(-1,1)\n",
    "            self.learner.next_q[self.learner.done_mask] = 0 \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea139c-52c0-4005-9101-a45c25c2d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TargetCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,discount=0.99,nsteps=1):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.learner = None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.targets = batch.reward+self.learner.next_q*(self.discount**self.nsteps)\n",
    "            self.learner.pred = self.learner.model(batch.state)\n",
    "            self.learner.target_qs = self.learner.pred.clone().float()\n",
    "            self.learner.target_qs.scatter_(1,batch.action.long(),self.learner.targets.float())\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1f7c0-8ae0-4bf9-b193-6f5ce0300606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,loss_func):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.loss_grad = self.loss_func(self.learner.pred, self.learner.target_qs)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca2d01-a3c4-4f9e-8fef-186c9e1f1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ModelLearnCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe, opt):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.loss_grad.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "            self.learner.loss = self.learner.loss_grad.clone()\n",
    "            yield self.learner.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa721c-22fc-49e6-8bc4-fc01b20e6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossCollector(dp.iter.IterDataPipe):\n",
    "    title:str='loss'\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = None\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse_dps(self),LearnerBase)\n",
    "        yield Record(self.title,None)\n",
    "        for i,steps in enumerate(self.source_datapipe):\n",
    "            yield Record('loss',self.learner.loss.cpu().detach().numpy())\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50c9dd-89fc-4a89-a6f0-ccd6126135d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DQNLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    do_logging:bool=True,\n",
    "    loss_func=nn.MSELoss(),\n",
    "    opt=optim.AdamW,\n",
    "    lr=0.005,\n",
    "    bs=128,\n",
    "    max_sz=10000,\n",
    "    nsteps=1,\n",
    "    device=None,\n",
    "    batches=None\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(model,dls[0])\n",
    "    learner = BatchCollector(learner,batches=batches)\n",
    "    learner = EpochCollector(learner)\n",
    "    if do_logging: \n",
    "        learner = learner.dump_records()\n",
    "        learner = ProgressBarLogger(learner)\n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner)\n",
    "    learner = learner.catch_records(drop=not do_logging)\n",
    "\n",
    "    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz,freeze_memory=True)\n",
    "    learner = StepBatcher(learner,device=device)\n",
    "    learner = QCalc(learner)\n",
    "    learner = TargetCalc(learner,nsteps=nsteps)\n",
    "    learner = LossCalc(learner,loss_func=loss_func)\n",
    "    learner = ModelLearnCalc(learner,opt=opt(model.parameters(),lr=lr))\n",
    "    if do_logging: \n",
    "        learner = LossCollector(learner).catch_records()\n",
    "\n",
    "    if len(dls)==2:\n",
    "        val_learner = LearnerBase(model,dls[1]).visualize_vscode()\n",
    "        val_learner = BatchCollector(val_learner,batches=batches)\n",
    "        val_learner = EpochCollector(val_learner).dump_records()\n",
    "        learner = LearnerHead((learner,val_learner))\n",
    "    else:\n",
    "        learner = LearnerHead(learner)\n",
    "    return learner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b8f9ed8-fb05-40a1-ac0d-d4cafee8fa07",
   "metadata": {},
   "source": [
    "Try training with basic defaults..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df20853-a286-4695-b3a4-8cf44f5bf70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2).cuda()\n",
    "# Setup the Agent\n",
    "agent = DQNAgent(model,do_logging=True,max_steps=4000,device='cuda')\n",
    "# Setup the DataBlock\n",
    "params = dict(source=['CartPole-v1']*1,agent=agent,nsteps=1,nskips=1,firstlast=False,bs=1)\n",
    "dls = dataloaders((\n",
    "    GymDataPipe(**params), GymDataPipe(**params,include_images=True)\n",
    "))\n",
    "\n",
    "# Setup the Learner\n",
    "learner = DQNLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    batches=1000,\n",
    "    bs=128,\n",
    "    max_sz=1000,\n",
    "    device='cuda'\n",
    ")\n",
    "# learner.fit(1)\n",
    "learner.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5e0ed73-7bbf-415b-9ee7-9a95de31d638",
   "metadata": {},
   "source": [
    "If we try a regular DQN with nsteps/nskips it doesnt really converge after 130. We cannot expect stability at all, and nsteps (correctly) tries to reduce to number of duplicated states so that the agent can sample more unique state transitions. The problem with this is the base dqn is not stable, so giving it lots of \"new\" unique state transitions do not help. In otherwords, its going to forget the old stuff very quickly, and having duplicate states helps \"remind it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d3626-1702-4f22-ae15-bb93a75bec68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
