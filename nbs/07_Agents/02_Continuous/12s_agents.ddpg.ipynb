{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "import torchdata.datapipes as dp\n",
    "from torch import nn\n",
    "import torch\n",
    "from  torchdata.dataloader2.graph import DataPipe\n",
    "# Local modules\n",
    "from fastrl.torch_core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.data.dataloader2 import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.learner.core import *\n",
    "from fastrl.loggers.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e740d6",
   "metadata": {},
   "source": [
    "# DDPG \n",
    "> Deep Deterministic Policy Gradiant for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b12d",
   "metadata": {},
   "source": [
    "[(Lillicrap et al., 2016) [DDPG] Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) based on the \n",
    "DPG algorithm in [(Silver et al., 2014) [DPG] Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf).\n",
    "\n",
    "DDPG uses an actor-critic architecture and has a similar training / learning paradym to DQNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f9a6f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Critic(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_sz:int,  # The input dim of the state\n",
    "            action_sz:int, # The input dim of the actions\n",
    "            hidden=512,    # Number of neurons connected between the 2 input/output layers\n",
    "            head_layer:Module=nn.Linear, # Output layer\n",
    "            activition_fn:Module=nn.ReLU # The activation function\n",
    "        ):\n",
    "        # TODO: Add batch normalization\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_sz+action_sz,hidden),\n",
    "            activition_fn(),\n",
    "            head_layer(hidden,1),\n",
    "        )\n",
    "    def forward(\n",
    "            self,\n",
    "            x:torch.Tensor # A single tensor of shape [Batch,`state_sz`+`action_sz`]\n",
    "            # A single tensor of shape [B,1] representing the cumulative value estimate of state+action combinations  \n",
    "        ) -> torch.Tensor: \n",
    "        return self.layers(x)\n",
    "\n",
    "add_docs(\n",
    "Critic,\n",
    "\"Takes a single tensor of size [B,`state_sz`+`action_sz`] -> [B,1] outputs a 1d tensor repersenting the value\",\n",
    "forward=\"\"\"Takes in a single tensor of a state tensor and action tensor and output\n",
    " the culative value estimates of that state,action combination\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875954b",
   "metadata": {},
   "source": [
    "The `Critic` is used by `DDPG` to estimate the state-action pairs and is updated using the \n",
    "the Bellman-Equation similarly to DQN/Q-Learning and is represeted by $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "critic = Critic(4,2)\n",
    "\n",
    "state = torch.randn(1,4)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_eq(\n",
    "        str(critic(torch.cat((state,action),dim=1))),\n",
    "        str(tensor([[0.0040]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f696cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Actor(Module):\n",
    "    def __init__(self,\n",
    "                 state_sz:int,  # The input dim of the state\n",
    "                 action_sz:int, # The output dim of the actions\n",
    "                 hidden=512,    # Number of neurons connected between the 2 input/output layers\n",
    "                 head_layer:Module=nn.Linear, # Output layer\n",
    "                 activition_fn:Module=nn.ReLU # The activiation function\n",
    "                ):\n",
    "        # TODO: Add batch normalization\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_sz,hidden),\n",
    "            activition_fn(),\n",
    "            head_layer(hidden,action_sz),\n",
    "        )\n",
    "    def forward(self,x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdb09",
   "metadata": {},
   "source": [
    "The `Actor` is used by `DDPG` to predict actions based on state inputs and is represeted by $\\mu(s|\\theta^\\mu)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "actor = Actor(4,2)\n",
    "\n",
    "state = torch.randn(1,4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_eq(\n",
    "        str(actor(state)),\n",
    "        str(tensor([[ 0.1440, -0.3434]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4256df9",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DDPGAgent(\n",
    "    model,\n",
    "    logger_bases=None,\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None\n",
    ")->AgentHead:\n",
    "    agent_base = AgentBase(model,logger_bases=ifnone(logger_bases,[CacheLoggerBase()]))\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = InputInjester(agent)\n",
    "    agent = SimpleModelRunner(agent)\n",
    "\n",
    "    agent = AgentHead(agent)\n",
    "    \n",
    "    agent = apply_dp_augmentation_fns(agent,dp_augmentation_fns)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
