{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #|export\n",
    "# # Python native modules\n",
    "# import os\n",
    "# from typing import *\n",
    "# from typing_extensions import Literal\n",
    "# from copy import deepcopy\n",
    "# # Third party libs\n",
    "# from fastcore.all import *\n",
    "# import torchdata.datapipes as dp\n",
    "# from  torchdata.dataloader2.graph import DataPipe,traverse\n",
    "# from torch import nn\n",
    "# from torch.optim import AdamW,Adam\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# # Local modules\n",
    "# from fastrl.core import *\n",
    "from fastrl.torch_core import Module\n",
    "# from fastrl.pipes.core import *\n",
    "# from fastrl.data.block import *\n",
    "# from fastrl.data.dataloader2 import *\n",
    "# from fastrl.agents.core import *\n",
    "# from fastrl.memory.experience_replay import ExperienceReplay\n",
    "# from fastrl.learner.core import *\n",
    "# from fastrl.loggers.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e740d6",
   "metadata": {},
   "source": [
    "# DDPG \n",
    "> Deep Deterministic Policy Gradiant for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177b12d",
   "metadata": {},
   "source": [
    "[(Lillicrap et al., 2016) [DDPG] Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) based on the \n",
    "DPG algorithm in [(Silver et al., 2014) [DPG] Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf).\n",
    "\n",
    "DDPG uses an actor-critic architecture and has a similar training / learning paradym to DQNs.\n",
    "\n",
    "Below is `(Lillicrap et al., 2016) Algorithm 1` that summarizes DDPG. \n",
    "\n",
    "<img src=\"../../images/(Lillicrap et al., 2016) DDPG Algorithm 1.png\" width=\"847\" height=\"594\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f9a6f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_xavier_uniform_weights(m:Module,bias=0.01):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.xavier_uniform_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_uniform_weights(m:Module,bound):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.uniform_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dff3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_kaiming_normal_weights(m:Module,bias=0.01):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.kaiming_normal_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c7e14",
   "metadata": {},
   "source": [
    "`Lillicrap et al., 2016` pg 11 notes: \n",
    "\"The other layers were initialized from uniform distributions $[ \\frac{-1}{\\sqrt{f}},\\frac{1}{\\sqrt{f}}]$ where f is the fan-in of the layer.\"\n",
    "\n",
    "`init_kaiming_normal_weights` is the most similar to this strategy. Other implimentations\n",
    "of DDPGs have also used `init_xavier_uniform_weights`\n",
    "\n",
    "> Note: There does not appear to be a major difference between performance of using either.\n",
    "\n",
    "The same page notes:\n",
    "\"final layer weights and biases of both the actor and critic\n",
    "were initialized from a uniform distribution $[−3 * 10^{−3}, 3 * 10^{−3}]$ and $[3 * 10^{−4}, 3 * 10^{−4}]$ for the\n",
    "low dimensional and pixel cases respectively.\", so the default value for `final_layer_init_fn` uses `init_uniform_weights`\n",
    "with a bound of `1e-4` for low dim, and if pixels, needs to be changed to `1e-5`.\n",
    "\n",
    "The same page notes:\n",
    "\"The low-dimensional networks had 2 hidden layers with 400 and 300 units respectively ... When learning from pixels we used 3 convolutional layers\n",
    "(no pooling) with 32 filters at each layer. This was followed by two fully connected layers with\n",
    "200 units\"\n",
    "\n",
    "We default to expect low-dimensions, and for images we will augment this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aae3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ddpg_conv2d_block(\n",
    "        # A tuple of state sizes generally representing an image of format: \n",
    "        # [channel,width,height]\n",
    "        state_sz:Tuple[int,int,int],\n",
    "        # Number of filters to use for each conv layer\n",
    "        filters=32,\n",
    "        # Activation function between each layer.\n",
    "        activation_fn=nn.ReLU,\n",
    "        # We assume the channels dim should be size 3 max. If it is more\n",
    "        # we assume the width/height are in the location of channel and need to\n",
    "        # be transposed.\n",
    "        ignore_warning:bool=False\n",
    "    ) -> Tuple[nn.Sequential,int]: # (Convolutional block,n_features_out)\n",
    "    \"Creates a 3 layer conv block from `state_sz` along with expected n_feature output shape.\"\n",
    "    channels = state_sz[0]\n",
    "    if channels>3 and not ignore_warning:\n",
    "        warn(f'Channels is {channels}>3 in state_sz {state_sz}')\n",
    "    layers = nn.Sequential(\n",
    "        nn.BatchNorm2d(channels),\n",
    "        nn.Conv2d(channels,channels,filters),\n",
    "        activation_fn(),\n",
    "        nn.Conv2d(channels,channels,filters),\n",
    "        activation_fn(),\n",
    "        nn.Conv2d(channels,channels,filters),   \n",
    "        nn.Flatten()\n",
    "    )\n",
    "    m_layers = deepcopy(layers).to(device='meta')\n",
    "    out_sz = m_layers(torch.ones((1,*state_sz),device='meta')).shape[-1]\n",
    "    return layers.to(device='cpu'),out_sz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_conv2d_block((3,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Critic(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_sz:int,  # The input dim of the state / flattened conv output\n",
    "            action_sz:int, # The input dim of the actions\n",
    "            hidden1:int=400,    # Number of neurons connected between the 2 input/output layers\n",
    "            hidden2:int=300,    # Number of neurons connected between the 2 input/output layers\n",
    "            head_layer:Module=nn.Linear, # Output layer\n",
    "            activation_fn:Module=nn.ReLU, # The activation function\n",
    "            weight_init_fn:Callable=init_kaiming_normal_weights, # The weight initialization strategy\n",
    "            # Final layer initialization strategy\n",
    "            final_layer_init_fn:Callable=partial(init_uniform_weights,bound=1e-4),\n",
    "            # For pixel inputs, we can plug in a `nn.Sequential` block from `ddpg_conv2d_block`.\n",
    "            # This means that actions will be feed into the second linear layer instead of the \n",
    "            # first.\n",
    "            conv_block:Optional[nn.Sequential]=None,\n",
    "            # Whether to do batch norm. \n",
    "            batch_norm:bool=False\n",
    "        ):\n",
    "        self.action_sz = action_sz\n",
    "        self.state_sz = state_sz\n",
    "        self.conv_block = conv_block\n",
    "        if conv_block is None:\n",
    "            if batch_norm:\n",
    "                ln_bn = nn.Sequential(\n",
    "                    nn.BatchNorm1d(state_sz+action_sz),\n",
    "                    nn.Linear(state_sz+action_sz,hidden1)\n",
    "                )\n",
    "            else:\n",
    "                ln_bn = nn.Linear(state_sz+action_sz,hidden1)\n",
    "            self.layers = nn.Sequential(\n",
    "                ln_bn,\n",
    "                activation_fn(),\n",
    "                nn.Linear(hidden1,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,1),\n",
    "            )\n",
    "        else:\n",
    "            self.conv_block = nn.Sequential(\n",
    "                self.conv_block,\n",
    "                nn.Linear(state_sz,hidden1),\n",
    "                activation_fn(),\n",
    "            )\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(hidden1+action_sz,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,1),\n",
    "            )\n",
    "        self.layers.apply(weight_init_fn)\n",
    "        if final_layer_init_fn is not None:\n",
    "            final_layer_init_fn(self.layers[-1])\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            s:torch.Tensor, # A single tensor of shape [Batch,`state_sz`]\n",
    "            a:torch.Tensor # A single tensor of shape [Batch,`action_sz`]\n",
    "            # A single tensor of shape [B,1] representing the cumulative value estimate of state+action combinations  \n",
    "        ) -> torch.Tensor: \n",
    "            if self.conv_block:\n",
    "                s = self.conv_block(s)\n",
    "            return self.layers(torch.hstack((s,a)))\n",
    "\n",
    "add_docs(\n",
    "Critic,\n",
    "\"Takes a 2 tensors of size [B,`state_sz`], [B,`action_sz`] -> [B,1] outputs a 1d tensor representing the Q value\",\n",
    "forward=\"\"\"Takes in a 2 tensors of a state tensor and action tensor and output\n",
    " the Q value estimates of that state,action combination\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875954b",
   "metadata": {},
   "source": [
    "The `Critic` is used by `DDPG` to estimate the Q value of state-action pairs and is updated using the \n",
    "the Bellman-Equation similarly to DQN/Q-Learning and is represeted by $Q(s,a)$\n",
    "\n",
    "Check that low dim input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "critic = Critic(4,2)\n",
    "\n",
    "state = torch.randn(1,4)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad(),evaluating(critic):\n",
    "    test_eq(\n",
    "        str(critic(state,action)),\n",
    "        str(tensor([[0.0083]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018856d",
   "metadata": {},
   "source": [
    "Check that image input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "image_shape = (3,100,100)\n",
    "\n",
    "conv_block,feature_out = ddpg_conv2d_block(image_shape)\n",
    "critic = Critic(feature_out,2,conv_block=conv_block)\n",
    "\n",
    "state = torch.randn(1,*image_shape)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad(),evaluating(critic):\n",
    "    test_eq(\n",
    "        str(critic(state,action)),\n",
    "        str(tensor([[0.0102]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f696cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Actor(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_sz:int,  # The input dim of the state\n",
    "            action_sz:int=0, # The output dim of the actions\n",
    "            hidden1:int=400,    # Number of neurons connected between the 2 input/output layers\n",
    "            hidden2:int=300,    # Number of neurons connected between the 2 input/output layers\n",
    "            head_layer:Module=nn.Linear, # Output layer\n",
    "            activation_fn:Module=nn.ReLU, # The activiation function\n",
    "            weight_init_fn:Callable=init_kaiming_normal_weights, # The weight initialization strategy\n",
    "            # Final layer initialization strategy\n",
    "            final_layer_init_fn:Callable=partial(init_uniform_weights,bound=1e-4),\n",
    "            # For pixel inputs, we can plug in a `nn.Sequential` block from `ddpg_conv2d_block`.\n",
    "            conv_block:Optional[nn.Sequential]=None,\n",
    "            # Whether to do batch norm. \n",
    "            batch_norm:bool=False\n",
    "        ):\n",
    "        self.action_sz = action_sz\n",
    "        self.state_sz = state_sz\n",
    "        self.conv_block = conv_block\n",
    "        if conv_block is None:\n",
    "            if batch_norm:\n",
    "                ln_bn = nn.Sequential(\n",
    "                    nn.BatchNorm1d(state_sz),\n",
    "                    nn.Linear(state_sz,hidden1)\n",
    "                )\n",
    "            else:\n",
    "                ln_bn = nn.Linear(state_sz,hidden1)\n",
    "            self.layers = nn.Sequential(\n",
    "                ln_bn,\n",
    "                activation_fn(),\n",
    "                nn.Linear(hidden1,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,action_sz),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                self.conv_block,\n",
    "                nn.Linear(state_sz,hidden1),\n",
    "                activation_fn(),\n",
    "                nn.Linear(hidden1,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,action_sz),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        self.layers.apply(init_kaiming_normal_weights)\n",
    "        if final_layer_init_fn is not None:\n",
    "            final_layer_init_fn(self.layers[-2])\n",
    "\n",
    "    def forward(self,x): return self.layers(x)\n",
    "\n",
    "add_docs(\n",
    "Actor,\n",
    "\"Takes a single tensor of size [B,`state_sz`] -> [B,`action_sz`] and outputs a tensor of actions.\",\n",
    "forward=\"\"\"Takes in a state tensor and output\n",
    " the actions value mappings\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdb09",
   "metadata": {},
   "source": [
    "The `Actor` is used by `DDPG` to predict actions based on state inputs and is represeted by $\\mu(s|\\theta^\\mu)$\n",
    "\n",
    "Check that low dim input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "actor = Actor(4,2)\n",
    "\n",
    "state = torch.randn(1,4)\n",
    "\n",
    "with torch.no_grad(),evaluating(actor):\n",
    "    test_eq(\n",
    "        str(actor(state)),\n",
    "        str(tensor([[0.0101, 0.0083]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018856d",
   "metadata": {},
   "source": [
    "Check that image input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7434012",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "image_shape = (3,100,100)\n",
    "\n",
    "conv_block,feature_out = ddpg_conv2d_block(image_shape)\n",
    "actor = Actor(feature_out,2,conv_block=conv_block)\n",
    "\n",
    "state = torch.randn(1,*image_shape)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad(),evaluating(actor):\n",
    "    test_eq(\n",
    "        str(actor(state)),\n",
    "        str(tensor([[0.0100, 0.0100]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pipe_to_device(pipe,device,debug=False):\n",
    "    \"Attempt to move an entire `pipe` and its pipeline to `device`\"\n",
    "    pipes = find_dps(traverse(pipe),dp.iter.IterDataPipe,include_subclasses=True)\n",
    "    for pipe in pipes:\n",
    "        if hasattr(pipe,'to'): \n",
    "            if debug: print(f'Moving {pipe} to {device}')\n",
    "            pipe.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddc041",
   "metadata": {},
   "source": [
    "## Ornstein-Uhlenbeck Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2871621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OrnsteinUhlenbeck(dp.iter.IterDataPipe):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\tsource_datapipe:DataPipe, # a datapipe whose next(source_datapipe) -> `Tensor` \n",
    "\t\t\taction_sz:int, # The action dimension\n",
    "\t\t\tmu:float=0., # Used in preturbing continuous actions\n",
    "\t\t\ttheta:float=0.15, # Used in preturbing continuous actions\n",
    "\t\t\tsigma:float=0.2, # Used in preturbing continuous actions\n",
    "            min_epsilon:float=0.2, # The minimum epsilon to drop to\n",
    "            # The max/starting epsilon if `epsilon` is None and used for calculating epislon decrease speed.\n",
    "            max_epsilon:float=1, \n",
    "            # Determines how fast the episilon should drop to `min_epsilon`. This should be the number\n",
    "            # of steps that the agent was run through.\n",
    "            max_steps:int=100,\n",
    "            # The starting epsilon which determines how much exploration to do.\n",
    "\t\t\t# epislon close to 1 does maximal exploration, while close to 0\n",
    "\t\t\t# does very little.\n",
    "            epsilon:float=None,\n",
    "            # Based on the `base_agent.model.training`, by default no decrement or step tracking will\n",
    "            # occur during validation steps.\n",
    "            decrement_on_val:bool=False,\n",
    "            # Based on the `base_agent.model.training`, by default random actions will not be attempted\n",
    "            explore_on_val:bool=False,\n",
    "            # Also return the original action prior to exploratory noise\n",
    "            ret_original:bool=False,\n",
    "\t\t):\n",
    "\t\tself.source_datapipe = source_datapipe\n",
    "\t\tself.min_epsilon = min_epsilon\n",
    "\t\tself.max_epsilon = max_epsilon\n",
    "\t\tself.max_steps = max_steps\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.decrement_on_val = decrement_on_val\n",
    "\t\tself.explore_on_val = explore_on_val\n",
    "\t\tself.ret_original = ret_original\n",
    "\t\tself.agent_base = None\n",
    "\t\tself.step = 0\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.theta = theta\n",
    "\t\tself.mu = mu\n",
    "\t\tself.device = None\n",
    "\t\tself.normal_dist = torch.distributions.Normal(0,1)\n",
    "\t\tself.x = torch.full((action_sz,),1).float()\n",
    "\t\tif not (self.decrement_on_val and self.explore_on_val):\n",
    "\t\t\tself.agent_base = find_dp(traverse(self.source_datapipe),AgentBase)\n",
    "\n",
    "\tdef to(self,*args,**kwargs):\n",
    "\t\tself.device = kwargs.get('device',self.device)\n",
    "\t\tself.x = self.x.to(*args,**kwargs)\n",
    "\t\tself.normal_dist = torch.distributions.Normal(\n",
    "\t\t\ttensor(0).float().to(*args,**kwargs),\n",
    "\t\t\ttensor(1).float().to(*args,**kwargs)\n",
    "\t\t)\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tfor action in self.source_datapipe:\n",
    "\t\t\t# TODO: Support tuples of actions also\n",
    "\t\t\tif not issubclass(action.__class__,torch.Tensor):\n",
    "\t\t\t\traise Exception(f'Expected Tensor, got {type(action)}\\n{action}')\n",
    "\t\t\tif action.dtype not in (torch.float32,torch.float64):\n",
    "\t\t\t\traise ValueError(f'Expected Tensor of dtype float32,float64, got: {action.dtype} from {self.source_datapipe}')\n",
    "\n",
    "\t\t\taction.to(device=self.device)\n",
    "\t\t\tif self.decrement_on_val or self.agent_base.model.training:\n",
    "\t\t\t\tself.step+=1\n",
    "\t\t\t\tself.epsilon = max(self.min_epsilon,self.max_epsilon-self.step/self.max_steps)\n",
    "\n",
    "\t\t\t# Add a batch dim if missing\n",
    "\t\t\tif len(action.shape)==1: action.unsqueeze_(0)\n",
    "\n",
    "\t\t\tif self.explore_on_val or self.agent_base.model.training:\n",
    "\t\t\t\tdist = self.normal_dist.sample((len(self.x),))\n",
    "\t\t\t\tself.x += self.theta*(self.mu-self.x)+self.sigma*dist\n",
    "\n",
    "\t\t\t\tif self.ret_original: yield (self.epsilon*self.x+action,action)\n",
    "\t\t\t\telse:                 yield self.epsilon*self.x+action\n",
    "\t\t\telse:\n",
    "\t\t\t\tyield action\n",
    "\n",
    "add_docs(\n",
    "OrnsteinUhlenbeck,\n",
    "\"\"\"Used for exploration in continuous action domains via temporaly correlated noise.\n",
    "\n",
    "[1] From https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "[2] Cumulatively based on [Uhlenbeck et al., 1930](http://www.entsphere.com/pub/pdf/1930%20Uhlenbeck,%20on%20the%20theory%20of%20the%20Brownian%20motion.pdf)\"\"\",\n",
    "to=torch.Tensor.to.__doc__\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a891ba",
   "metadata": {},
   "source": [
    "The `OrnsteinUhlenbeck` for DDPG has notation:\n",
    "\n",
    "$\\mu'(s_t)=\\mu(s_t|\\theta_{t}^{\\mu}) + N$\n",
    "\n",
    "> Note: (Lillicrap et al., 2016) pg 4 says \"generate temporally correlated exploration for exploration efficiency in physical control problems with inertia\". This might be important to consider when training on environments that don't require inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ExplorationComparisonCollector(dp.iter.IterDataPipe):\n",
    "    title:str='exploration-compare'`\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield Record(self.title,None)\n",
    "        for action in self.source_datapipe:\n",
    "            if not_record(action):\n",
    "                    yield Record(\n",
    "                        self.title,\n",
    "                        tuple((action.detach().cpu(),original_action.detach().cpu()))\n",
    "                    )\n",
    "            yield element\n",
    "    \n",
    "    def show(self,title='Explored Actions vs Original Actions'):\n",
    "        import plotly.express as px\n",
    "        import plotly.io as pio\n",
    "        pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "        action,original_action = zip(*[o.value for o in self.buffer])\n",
    "        difference = torch.sub(torch.vstack(action),torch.vstack(original_action)).abs()\n",
    "\n",
    "        fig = px.scatter(\n",
    "            pd.DataFrame(difference.numpy()),\n",
    "            title=title,\n",
    "            labels={\n",
    "                \"index\": \"N Steps\",\n",
    "                \"value\": \"Difference between Original/Explored Action\"\n",
    "            },\n",
    "        )\n",
    "        return fig.show()\n",
    "\n",
    "add_docs(\n",
    "ExplorationComparisonLogger,\n",
    "\"\"\"Allows for quickly doing a \"what if\" on exploration methods by comparing\n",
    "the actions selected via exploration with the ones chosen by the model.\n",
    "\"\"\",\n",
    "show=\"\"\"Shows the absolute difference between explored actions and original actions.\n",
    "We would expect as the number of steps increase, the difference between the \n",
    "explored and original actions would get smaller. In other words, if there is no\n",
    "exploration, then the explored actions and the original actions should be almost\n",
    "identical.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea31dc7",
   "metadata": {},
   "source": [
    "Below we demonstrate that the exploration works. As the number of steps increase,\n",
    "epsilon will decrease to zero, and so the actions slowly become more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "actions = dp.iter.IterableWrapper(\n",
    "    # Batch of 4 actions with dimensions 2\n",
    "    torch.randn(4,2).to(device=default_device())\n",
    ")\n",
    "\n",
    "actions = OrnsteinUhlenbeck(\n",
    "    actions,\n",
    "    min_epsilon=0,\n",
    "    max_steps=200,\n",
    "    action_sz=2,\n",
    "    decrement_on_val=True,\n",
    "    explore_on_val=True,\n",
    "    ret_original=True\n",
    ")\n",
    "actions.to(device=default_device())\n",
    "actions = actions.cycle(count=50)\n",
    "actions = ExplorationComparisonLogger(actions)\n",
    "list(actions)\n",
    "actions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4256df9",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActionUnbatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # An IterDataPipe whose __next__ produces an action of shape:\n",
    "            # [D]\n",
    "            # or\n",
    "            # [1,D]\n",
    "            # Where D is the number of dimensions of the action.\n",
    "            source_datapipe:DataPipe\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for action in self.source_datapipe:\n",
    "            if len(action.shape)==2: action.squeeze_(0)\n",
    "            yield action\n",
    "\n",
    "add_docs(\n",
    "ActionUnbatcher,\n",
    "\"\"\"Removes the batch dim from an action.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActionClip(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Produces action as tensors\n",
    "        source_datapipe:DataPipe,\n",
    "        # Minimum clip value\n",
    "        clip_min:float=-1,\n",
    "        # Maximum clip value\n",
    "        clip_max:float=1\n",
    "    ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "\n",
    "    def __iter__(self):\n",
    "        for action in self.source_datapipe:\n",
    "            action.clip_(self.clip_min,self.clip_max)\n",
    "            yield action\n",
    "\n",
    "add_docs(\n",
    "ActionClip,\n",
    "\"\"\"Restricts actions from `source_datapipe` between `clip_min` and `clip_max`\n",
    "\n",
    "Interally calls `torch.clip`\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DDPGAgent(\n",
    "    model:Actor, # The actor to use for mapping states to actions\n",
    "    # LoggerBases push logs to. If None, logs will be collected and output\n",
    "    # by the dataloader.\n",
    "    logger_bases:Optional[LoggerBase]=None, \n",
    "    min_epsilon:float=0.2, # The minimum epsilon to drop to\n",
    "    # The max/starting epsilon if `epsilon` is None and used for calculating epislon decrease speed.\n",
    "    max_epsilon:float=1, \n",
    "    # Determines how fast the episilon should drop to `min_epsilon`. This should be the number\n",
    "    # of steps that the agent was run through.\n",
    "    max_steps:int=100,\n",
    "    do_logging:bool=False\n",
    ")->AgentHead:\n",
    "    \"Produces continuous action outputs.\"\n",
    "    agent_base = AgentBase(model)\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = SimpleModelRunner(agent)\n",
    "    agent = OrnsteinUhlenbeck(\n",
    "        agent,\n",
    "        action_sz=model.action_sz,\n",
    "        min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps\n",
    "    )\n",
    "    if do_logging:\n",
    "        agent = ExplorationComparisonCollector(agent).catch_records()\n",
    "    agent = ActionClip(agent)\n",
    "    agent = ActionUnbatcher(agent)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = AgentHead(agent)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26226100",
   "metadata": {},
   "source": [
    "Check that given a `step`, we can get actions from the `DDPGAgent`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55777fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "actor = Actor(4,2)\n",
    "\n",
    "agent = DDPGAgent(actor)\n",
    "\n",
    "input_tensor = tensor([1,2,3,4]).float()\n",
    "step = SimpleStep(state=input_tensor)\n",
    "\n",
    "for _ in range(10):\n",
    "    for action in agent([step]):\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff674fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.envs.gym import gymnasium as gymTransformBlock\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeTransformBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "torch.manual_seed(0)\n",
    "\n",
    "actor = Actor(3,1)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = DDPGAgent(actor,max_steps=10000)\n",
    "\n",
    "pipe = GymTransformBlock(agent=agent,n=100,seed=None,include_images=True)(['Pendulum-v1'])\n",
    "pipe = VSCodeTransformBlock()(pipe)\n",
    "\n",
    "pipe_to_device(pipe,default_device(),debug=True)\n",
    "\n",
    "L(pipe);\n",
    "pipe.show(step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1546842",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5731f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BasicOptStepper(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "        # The parent datapipe that should produce a dict of format `{'loss':tensor(...)}`\n",
    "        # all other types will be passed through.\n",
    "        source_datapipe:DataPipe, \n",
    "        # The model to attach\n",
    "        model:nn.Module,\n",
    "        # The learning rate\n",
    "        lr:float,\n",
    "        # The optimizer to use\n",
    "        opt:torch.optim.Optimizer=AdamW,\n",
    "        # If an input is loss, catch it and prevent it from proceeding to the\n",
    "        # rest of the pipeline.\n",
    "        filter:bool=False,\n",
    "        # Whether to `zero_grad()` before doing backward/step\n",
    "        do_zero_grad:bool=True,\n",
    "        # kwargs to be passed to the `opt`\n",
    "        **opt_kwargs\n",
    "    ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.opt_kwargs = opt_kwargs\n",
    "        self.do_zero_grad = do_zero_grad\n",
    "        self.filter = filter\n",
    "        self._opt = self.opt(self.model.parameters(),lr=self.lr,**self.opt_kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.source_datapipe:\n",
    "            if isinstance(x,dict) and 'loss' in x:\n",
    "                if self.do_zero_grad: self._opt.zero_grad()\n",
    "                x['loss'].backward()\n",
    "                self._opt.step()\n",
    "                if self.filter: continue \n",
    "            yield x\n",
    "\n",
    "add_docs(\n",
    "BasicOptStepper,\n",
    "\"\"\"Optimizes `model` using `opt`. `source_datapipe` must produce a dictionary of format: `{\"loss\":...}`,\n",
    "otherwise all non-dicts will be passed through.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossCollector(LogCollector):\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe, likely the one to collect metrics from\n",
    "            header:str='loss', # Name of the record. Change if using multiple instances.\n",
    "            # If an input is loss, catch it and prevent it from proceeding to the\n",
    "            # rest of the pipeline.\n",
    "            filter:bool=False,\n",
    "            # By default, LossCollector will search the pipeline for logger bases\n",
    "            # and attach them here. However we can directly attach them here if\n",
    "            # we need. This must be a list of lists/queues.\n",
    "            main_buffers:Optional[List[List]]=None \n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = main_buffers\n",
    "        self.header = header\n",
    "        self.filter = filter\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for x in self.source_datapipe:\n",
    "            if isinstance(x,dict) and 'loss' in x:\n",
    "                for q in self.main_buffers: \n",
    "                    q.append(Record(self.header,x['loss'].cpu().detach().numpy()))\n",
    "                if self.filter: continue\n",
    "            yield x\n",
    "\n",
    "    def show(self,title='Loss over N-Steps'):\n",
    "        import plotly.express as px\n",
    "        import plotly.io as pio\n",
    "        pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "        losses = {i:[o.value for o in ls] for i,ls in enumerate(self.main_buffers)}\n",
    "\n",
    "        fig = px.line(\n",
    "            pd.DataFrame(losses),\n",
    "            title=title,\n",
    "            labels={\n",
    "                \"index\": \"N Steps\",\n",
    "                \"value\": \"Loss\"\n",
    "            },\n",
    "        )\n",
    "        return fig.show()\n",
    "\n",
    "add_docs(\n",
    "LossCollector,\n",
    "\"\"\"Itercepts dictionary results generated from `source_datapipe` that are in the \n",
    "format: `{'loss':tensor(...)}`. All other elements will be ignored and passed through.\n",
    "\n",
    "If `filter=true`, then intercepted dictionaries will filtered out by this pipe, and will\n",
    "not be propagated to the rest of the pipeline. \n",
    "\"\"\",\n",
    "show=\"\"\"Shows the loss over n-steps/n-batchs depending on how the loss values are loaded \n",
    "into the `main_buffers`. If there is no `LoggerBase`s, then \n",
    "`LossCollector(...,main_buffers=[[]],...)` must be passed so that \n",
    "losses can be cached for showing.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SoftTargetUpdater(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Expected to produces batch elements, however could be anything\n",
    "            # since `SoftTargetUpdater` only tracks the number of iters.\n",
    "            source_datapipe:DataPipe,\n",
    "            # The model to be soft copied from\n",
    "            model:nn.Module,\n",
    "            # How often to soft copy the model as:\n",
    "            # `n_batch%target_sync==0`\n",
    "            target_sync:int=1,\n",
    "            # A percent of the model to copy to the target.\n",
    "            tau:float=0.001\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.model = model\n",
    "        self.target_model = deepcopy(model)\n",
    "        self.target_sync = target_sync\n",
    "        self.tau = tau\n",
    "        self.n_batch = 0\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.model.to(**kwargs)\n",
    "        self.target_model.to(**kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            yield batch\n",
    "            if self.n_batch%self.target_sync==0:\n",
    "                for tp, fp in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                    tp.data.copy_(self.tau * fp.data + (1.0 - self.tau) * tp.data)\n",
    "            self.n_batch+=1\n",
    "\n",
    "add_docs(\n",
    "SoftTargetUpdater,\n",
    "\"\"\"Soft-Copies `model` to a `target_model` (internal) every `target_sync` batches.\"\"\",\n",
    "to=\"Executes `to` on `target_model` and `model`\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92be82",
   "metadata": {},
   "source": [
    "We use `SoftTargetUpdater` to update the target `Critic` and `Actor`. This is \n",
    "characterized by the notation:\n",
    "\n",
    "$$\n",
    "\\theta^{Q'} \\leftarrow \\tau \\theta^Q + (1 - \\tau)\\theta^{Q'}\n",
    "$$\n",
    "$$\n",
    "\\theta^{\\mu'} \\leftarrow \\tau \\theta^\\mu + (1 - \\tau)\\theta^{\\mu'}\n",
    "$$\n",
    "\n",
    "For both the `Critic`(Q) and `Actor`($\\mu$) are slowly copied to their targets based on the value $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aff1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_target_model(\n",
    "        # If `model` is not none, then we assume it to be the target model\n",
    "        # and simply return it, otherwise we search for a `target_model`\n",
    "        model:Optional[nn.Module],\n",
    "        # The pipe to start search along\n",
    "        pipe:DataPipe,\n",
    "        # The class of the model we are looking for\n",
    "        model_cls:nn.Module,\n",
    "        # A tuple of datapipes that have a field called `target_model`.\n",
    "        # `get_target_model` will look for these in `pipe`\n",
    "        target_updater_cls:Tuple[DataPipe]=(SoftTargetUpdater,),\n",
    "        # Verbose output\n",
    "        debug:bool=False\n",
    "    ):\n",
    "        \"Basic utility for getting the 'target' version of `model_cls` in `pipe`\"\n",
    "        if model is not None: return model\n",
    "        target_updaters = []\n",
    "        for target_updater in target_updater_cls:\n",
    "            target_updaters.extend(find_dps(traverse(pipe),target_updater))\n",
    "        if debug: print(target_updaters)\n",
    "        target_updaters = [o for o in target_updaters if isinstance(o.target_model,model_cls)]\n",
    "        if debug: print(f'After filtering on {model_cls}: {target_updaters}')\n",
    "        if len(target_updaters)==0: raise RuntimeError('target_updaters is empty')\n",
    "        elif len(target_updaters)>1: \n",
    "            warn(f'Found multiple target updaters with {model_cls}, {target_updaters}')\n",
    "        return target_updaters[0].target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CriticLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            critic:Critic, # The critic to optimize\n",
    "            # The optional target actor, where if None, will look for a \n",
    "            # target model along `source_datapipe`.\n",
    "            t_actor:Optional[Actor]=None, \n",
    "            # The optional target critic, where if None, will look for a \n",
    "            # target model along `source_datapipe`.\n",
    "            t_critic:Optional[Critic]=None,\n",
    "            # The loss function to use\n",
    "            loss:nn.Module=nn.MSELoss,\n",
    "            # The discount factor of `q`. Typically does not need to be changed,\n",
    "            # and determines the importants of earlier state qs verses later state qs\n",
    "            discount:float=0.99,\n",
    "            # If the environment has `nsteps>1`, it is recommended to change this\n",
    "            # param to reflect that so the reward estimates are more accurate.\n",
    "            nsteps:int=1\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.critic = critic\n",
    "        self.t_critic = get_target_model(t_critic,source_datapipe,Critic,debug=self.debug)\n",
    "        self.t_actor = get_target_model(t_actor,source_datapipe,Actor,debug=self.debug)\n",
    "        self.loss = loss()\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.critic.to(**kwargs)\n",
    "        self.t_critic.to(**kwargs)\n",
    "        self.t_actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            batch.to(self.device)\n",
    "            done_mask = batch.terminated\n",
    "            with torch.no_grad():\n",
    "                t_actions = self.t_actor(batch.next_state)\n",
    "                q = self.t_critic(batch.next_state,t_actions)\n",
    "\n",
    "            self.critic.zero_grad()\n",
    "            targets = batch.reward+q*(self.discount**self.nsteps)*(~done_mask)\n",
    "            pred = self.critic(batch.state,batch.action)\n",
    "            yield {'loss':self.loss(pred,targets)}\n",
    "            yield batch\n",
    "\n",
    "add_docs(\n",
    "CriticLossProcessor,\n",
    "r\"\"\"Produces a critic loss based on `critic`,`t_actor`,`t_critic` and batch `StepTypes`\n",
    "from `source_datapipe` where the targets and predictions are fed into `loss`.\n",
    "\n",
    "This datapipe produces either Dict[Literal['loss'],torch.Tensor] or `SimpleStep`.\n",
    "\n",
    "From (Lillicrap et al., 2016), we expect to get N transitions from $R$ where $R$ is\n",
    "`source_datapipe`.\n",
    "\n",
    "$N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$ where $(s_i, a_i, r_i, s_{i+1})$\n",
    "are `StepType`\n",
    "\n",
    "The targets are similar to DQN since we are estimating the $Q$ value:\n",
    "\n",
    "$y_i = r_i + \\gamma Q' (s_{i+1}, \\mu'(s_{i+1} | \\theta^{\\mu'})|\\theta^{Q'})$\n",
    "\n",
    "Where $y_i$ is the `targets`, $\\gamma$ is the `discount**nsteps`, $Q'$ is the \n",
    "`t_critic`, $\\mu'$ is the `t_actor`.\n",
    "\n",
    "$\\mu'(s_{i+1} | \\theta^{\\mu'})$ is the `t_actors` predicted actions of `s_{i+1}`\n",
    "\n",
    "Update critic by minimizing the loss: $L = \\frac{1}{N}\\sum_i{y_i - Q(s_i,a_i|\\theta^Q))^2}$\n",
    "\n",
    "Where $Q(s_i,a_i|\\theta^Q)$ is `critic(batch.state,batch.action)` and anything\n",
    "with $\\frac{1}{N}\\sum_i{(...)}^2$ is just `nn.MSELoss`\n",
    "\n",
    "\"\"\",\n",
    "to=\"Executes the `to` for `critic`,`t_actor`,`t_critic` and will grab the `device` from `kwargs` if it exists.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b765a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "pipe = GymTransformBlock(agent=None,n=1000,bs=64,seed=0)(['Pendulum-v1'])\n",
    "pipe = StepBatcher(pipe)\n",
    "\n",
    "actor = Actor(3,1)\n",
    "critic = Critic(3,1)\n",
    "\n",
    "pipe = SoftTargetUpdater(pipe,critic)\n",
    "pipe = CriticLossProcessor(pipe,critic,actor)\n",
    "\n",
    "pipe_loss = LossCollector(pipe,main_buffers=[[]])\n",
    "pipe = BasicOptStepper(pipe_loss,critic,1e-3)\n",
    "list(pipe)\n",
    "pipe_loss.show(title='Critic Loss over N-Steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActorLossProcessor(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            # The parent datapipe that should yield step types\n",
    "            source_datapipe:DataPipe, \n",
    "            # The critic model to use.\n",
    "            critic:Critic,\n",
    "            # The actor to optimize.\n",
    "            actor:Actor,\n",
    "            # Critic grad might get very large, and so we can optionally\n",
    "            # restrict it to a max/min value.\n",
    "            clip_critic_grad:Optional[int]=None\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.critic = critic\n",
    "        self.actor = actor\n",
    "        self.clip_critic_grad = clip_critic_grad\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.critic.to(**kwargs)\n",
    "        self.actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            batch.to(self.device)\n",
    "            self.actor.zero_grad()\n",
    "            q = self.critic(batch.state,self.actor(batch.state))\n",
    "            if self.clip_critic_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(),self.clip_critic_grad)\n",
    "\n",
    "            loss = (-q).mean()\n",
    "            yield {'loss':loss}\n",
    "            yield batch\n",
    "\n",
    "add_docs(\n",
    "ActorLossProcessor,\n",
    "r\"\"\"Produces a critic loss based on `critic`,`actor` and batch `StepTypes`\n",
    "from `source_datapipe` where the targets and predictions are fed into `loss`.\n",
    "\n",
    "(Lillicrap et al., 2016) notes: \"The actor is updated by following the applying the chain rule\n",
    "to the expected return from the start distribution J with respect to the actor parameters\"\n",
    "\n",
    "The loss is defined as the \"policy gradient\" below:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta^{\\mu}} J \\approx \\frac{1}{N} \\sum_i{\\nabla_aQ(s,a|\\theta^Q)|_{s={s_i},a={\\mu(s_i)}}\\nabla_{\\theta^{\\mu}\\mu(s|\\theta^Q)|_{s_i}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\frac{1}{N} \\sum_i$ is the mean.\n",
    "\n",
    "$\\nabla_{\\theta^{\\mu}\\mu(s|\\theta^Q)|_{s_i}}$ is the `actor` output.\n",
    "\n",
    "$\\nabla_aQ(s,a|\\theta^Q)|_{s={s_i},a={\\mu(s_i)}}$ is the `critic` output, using actions from the `actor`.\n",
    "\n",
    "> Important: A little confusing point, $\\nabla$ is the gradient/derivative of both. The point of the loss\n",
    "is that we want to select actions that have `critic` output higher values. We can do this by first calling\n",
    "`CriticLossProcessor` to load `critic` with gradients, then run it again but with the `actor` inputs.\n",
    "We want the `actor` to have the `critic` produce more positive gradients, than negative i.e: Have actions\n",
    "that maximize the critic outputs. The confusing thing is since pytorch has autograd, the actual\n",
    "code is not going to match the math above, for good and bad. \n",
    "\n",
    "TODO: It would be helpful if this documentation can be better explained.\n",
    "\n",
    "> Note: We actually multiply `J` by -1 since the optimizer is trying to make the value \n",
    "as \"small\" as possible, but the actual value we want to be as big as possible. \n",
    "So if we have a `J` of 100 (high reward), it becomes -100, letting the optimizer know that\n",
    "it is moving is the correct direction (the more negative, the better). \n",
    "\"\"\",\n",
    "to=\"Executes the `to` for `critic`,`actor` and will grab the `device` from `kwargs` if it exists.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a969b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(3,1)\n",
    "critic = Critic(3,1)\n",
    "\n",
    "agent = DDPGAgent(actor,max_steps=10000)\n",
    "\n",
    "pipe = GymTransformBlock(agent=agent,n=1000,bs=10)(['Pendulum-v1'])\n",
    "pipe = StepBatcher(pipe)\n",
    "\n",
    "pipe = ActorLossProcessor(pipe,critic,actor)\n",
    "\n",
    "pipe_loss = LossCollector(pipe,main_buffers=[[]])\n",
    "pipe = BasicOptStepper(pipe_loss,actor,1e-3)\n",
    "list(pipe)\n",
    "pipe_loss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039eee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DDPGLearner(\n",
    "    # The actor model to use\n",
    "    actor:Actor,\n",
    "    # The critic model to use\n",
    "    critic:Critic,\n",
    "    # A list of dls, where index=0 is the training dl.\n",
    "    dls:List[DataPipeOrDataLoader],\n",
    "    # Optional logger bases to log training/validation data to.\n",
    "    logger_bases:Optional[List[LoggerBase]]=None,\n",
    "    # The learning rate for the actor. Expected to learn slower than the critic\n",
    "    actor_lr:float=1e-3,\n",
    "    # The optimizer for the actor\n",
    "    actor_opt:torch.optim.Optimizer=Adam,\n",
    "    # The learning rate for the critic. Expected to learn faster than the actor\n",
    "    critic_lr:float=1e-2,\n",
    "    # The optimizer for the critic\n",
    "    # Note that weight decay doesnt seem to be great for \n",
    "    # Pendulum, so we use regular Adam, which has the decay rate\n",
    "    # set to 0. (Lillicrap et al., 2016) would instead use AdamW\n",
    "    critic_opt:torch.optim.Optimizer=Adam,\n",
    "    # Reference: SoftTargetUpdater docs\n",
    "    critic_target_copy_freq:int=1,\n",
    "    # Reference: SoftTargetUpdater docs\n",
    "    actor_target_copy_freq:int=1,\n",
    "    # Reference: SoftTargetUpdater docs\n",
    "    tau:float=0.001,\n",
    "    # Reference: ExperienceReplay docs \n",
    "    bs:int=128,\n",
    "    # Reference: ExperienceReplay docs\n",
    "    max_sz:int=10000,\n",
    "    # Reference: GymStepper docs\n",
    "    nsteps:int=1,\n",
    "    # The device for the entire pipeline to use. Will move the agent, dls, \n",
    "    # and learner to that device.\n",
    "    device:torch.device=None,\n",
    "    # Number of batches per epoch\n",
    "    batches:int=None,\n",
    "    # Any augmentations to the learner\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None,\n",
    "    # Debug mode will output device moves\n",
    "    debug:bool=False\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(actor,dls,batches=batches)\n",
    "    learner = LoggerBasePassThrough(learner,logger_bases)\n",
    "    learner = BatchCollector(learner,batch_on_pipe=LearnerBase)\n",
    "    learner = EpocherCollector(learner)\n",
    "    for logger_base in L(logger_bases): learner = logger_base.connect_source_datapipe(learner)\n",
    "    if logger_bases: \n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner)\n",
    "    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz)\n",
    "    learner = StepBatcher(learner)\n",
    "    learner = SoftTargetUpdater(learner,critic,target_sync=critic_target_copy_freq,tau=tau)\n",
    "    learner = SoftTargetUpdater(learner,actor,target_sync=actor_target_copy_freq,tau=tau)\n",
    "    learner = CriticLossProcessor(learner,critic,actor,nsteps=nsteps)\n",
    "    learner = LossCollector(learner,header='critic-loss')\n",
    "    learner = BasicOptStepper(learner,critic,critic_lr,opt=critic_opt,filter=True,do_zero_grad=False)\n",
    "    learner = ActorLossProcessor(learner,critic,actor,clip_critic_grad=5)\n",
    "    learner = LossCollector(learner,header='actor-loss')\n",
    "    learner = BasicOptStepper(learner,actor,actor_lr,opt=actor_opt,filter=True,do_zero_grad=False)\n",
    "    learner = LearnerHead(learner)\n",
    "    \n",
    "    learner = apply_dp_augmentation_fns(learner,dp_augmentation_fns)\n",
    "    pipe_to_device(learner,device,debug=debug)\n",
    "    for dl in dls: pipe_to_device(dl.datapipe,device,debug=debug)\n",
    "    \n",
    "    return learner\n",
    "\n",
    "DDPGLearner.__doc__=\"\"\"DDPG is a continuous action, actor-critic model, first created in\n",
    "(Lillicrap et al., 2016). The critic estimates a Q value estimate, and the actor\n",
    "attempts to maximize that Q value.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "# Setup Loggers\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "actor = Actor(3,1)\n",
    "critic = Critic(3,1)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = DDPGAgent(actor,[logger_base],max_steps=5000,min_epsilon=0.1)\n",
    "\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    GymTransformBlock(agent=agent,nsteps=2,nskips=2,firstlast=True), \n",
    "    (GymTransformBlock(agent=agent,n=400,nsteps=2,nskips=2,firstlast=True,include_images=True),VSCodeTransformBlock())\n",
    ")\n",
    "dls = L(block.dataloaders(['Pendulum-v1']*1))\n",
    "# Setup the Learner\n",
    "learner = DDPGLearner(actor,critic,dls,logger_bases=[logger_base],\n",
    "                      bs=128,max_sz=20_000,nsteps=2,\n",
    "                      batches=1000)\n",
    "# learner.fit(1)\n",
    "learner.fit(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval:false\n",
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad9b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
