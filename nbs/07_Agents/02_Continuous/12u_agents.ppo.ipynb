{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Union,Dict,Literal,List,Callable,Optional\n",
    "# from typing_extensions import Literal\n",
    "# import typing \n",
    "# from warnings import warn\n",
    "# # Third party libs\n",
    "# import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# from torch.distributions import *\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse_dps\n",
    "# from fastcore.all import test_eq,test_ne,ifnone,L\n",
    "from torch.optim import AdamW,Adam\n",
    "# # Local modules\n",
    "from fastrl.core import SimpleStep\n",
    "# from fastrl.pipes.core import *\n",
    "# from fastrl.torch_core import *\n",
    "from fastrl.layers import Critic\n",
    "# from fastrl.data.block import *\n",
    "# from fastrl.envs.gym import *\n",
    "from fastrl.agents.trpo import Actor\n",
    "# from fastrl.loggers.vscode_visualizers import VSCodeTransformBlock\n",
    "# from fastrl.loggers.jupyter_visualizers import ProgressBarLogger\n",
    "# from fastrl.agents.discrete import EpsilonCollector\n",
    "# from fastrl.agents.core import AgentHead,StepFieldSelector,AgentBase \n",
    "# from fastrl.agents.ddpg import ActionClip,ActionUnbatcher,NumpyConverter,OrnsteinUhlenbeck,SimpleModelRunner\n",
    "# from fastrl.loggers.core import LoggerBase,CacheLoggerBase\n",
    "# from fastrl.dataloader2_ext import InputInjester\n",
    "# from fastrl.loggers.core import LoggerBasePassThrough,BatchCollector,EpocherCollector,RollingTerminatedRewardCollector,EpisodeCollector\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead\n",
    "from fastrl.loggers.core import BatchCollector,EpochCollector,RollingTerminatedRewardCollector,EpisodeCollector\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "from fastrl.agents.ddpg import LossCollector,BasicOptStepper,StepBatcher\n",
    "from fastrl.agents.trpo import CriticLossProcessor\n",
    "# from fastrl.pipes.core import *\n",
    "# from fastrl.pipes.iter.nskip import *\n",
    "# from fastrl.pipes.iter.nstep import *\n",
    "# from fastrl.pipes.iter.firstlast import *\n",
    "# from fastrl.pipes.iter.transforms import *\n",
    "# from fastrl.pipes.map.transforms import *\n",
    "# from fastrl.data.block import *\n",
    "# from fastrl.torch_core import *\n",
    "# from fastrl.layers import *\n",
    "# from fastrl.data.block import *\n",
    "# from fastrl.envs.gym import *\n",
    "# from fastrl.agents.ddpg import LossCollector,BasicOptStepper,StepBatcher\n",
    "# from fastrl.loggers.core import LogCollector\n",
    "# from fastrl.agents.discrete import EpsilonCollector\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# PPO\n",
    "> [Proximate Policy Gradients](https://arxiv.org/pdf/1707.06347.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d688b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PPOActorOptAndLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            actor:Actor,\n",
    "            # The learning rate\n",
    "            actor_lr:float,\n",
    "            critic:Critic,\n",
    "            # The learning rate\n",
    "            critic_lr:float,\n",
    "            # The optimizer to use\n",
    "            actor_opt:torch.optim.Optimizer=AdamW,\n",
    "            # The optimizer to use\n",
    "            critic_opt:torch.optim.Optimizer=AdamW,\n",
    "            ppo_epochs = 10,\n",
    "            ppo_batch_sz = 64,\n",
    "            ppo_eps = 0.2,\n",
    "            # kwargs to be passed to the `opt`\n",
    "            **opt_kwargs\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actor = actor\n",
    "        self.device = None\n",
    "        self.counter = 0\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic = critic\n",
    "        self.critic_lr = critic_lr\n",
    "        self.critic_opt = critic_opt\n",
    "        self.opt_kwargs = opt_kwargs\n",
    "        self.actor_opt = actor_opt\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        self._critic_opt = self.critic_opt(self.critic.parameters(),lr=self.critic_lr,**self.opt_kwargs)\n",
    "        self._actor_opt = self.actor_opt(self.actor.parameters(),lr=self.actor_lr,**self.opt_kwargs)\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.ppo_batch_sz = ppo_batch_sz\n",
    "        self.ppo_eps = ppo_eps\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "                batch.to(self.device)\n",
    "                traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            \n",
    "            dist = self.actor(batch.state)\n",
    "            old_log_prob = dist.log_prob(batch.action).detach()\n",
    "            loss = None\n",
    "\n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "\n",
    "            for epoch in range(self.ppo_epochs):\n",
    "                for ppo_batch in range(0,batch.advantage[m].shape[0],self.ppo_batch_sz):\n",
    "\n",
    "                    states_v = batch.state[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "                    action_v = batch.action[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "                    advantage = traj_adv_v[m][ppo_batch:ppo_batch+self.ppo_batch_sz].reshape(-1,)\n",
    "                    batch_ref_v = batch.next_advantage[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "\n",
    "                    self._critic_opt.zero_grad()\n",
    "                    value_v = self.critic(states_v)\n",
    "                    loss_value_v = self.critic_loss(value_v.squeeze(-1), batch_ref_v.squeeze(-1))\n",
    "                    loss_value_v.backward()\n",
    "                    self._critic_opt.step()\n",
    "\n",
    "                    self._actor_opt.zero_grad()\n",
    "                    \n",
    "                    try:\n",
    "                        dist = self.actor(states_v)\n",
    "                    except Exception as e:\n",
    "                        dist = self.actor(states_v)\n",
    "                    logprob_pi_v = dist.log_prob(action_v) #.detach()\n",
    "                    ratio_v = torch.exp(logprob_pi_v - old_log_prob[m][ppo_batch:ppo_batch+self.ppo_batch_sz]+1e-7)\n",
    "                    surr_obj_v = advantage * ratio_v\n",
    "                    clipped_surr_v = advantage * torch.clamp(ratio_v, 1.0 - self.ppo_eps, 1.0 + self.ppo_eps)\n",
    "                    loss_policy_v = -torch.min(surr_obj_v, clipped_surr_v).mean()\n",
    "                    loss_policy_v.backward()\n",
    "                    self._actor_opt.step()\n",
    "\n",
    "                    if loss is None:\n",
    "                        loss = loss_policy_v\n",
    "                    else:\n",
    "                        loss += loss_policy_v\n",
    "            yield {'loss':loss}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8cae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def PPOLearner(\n",
    "    # The actor model to use\n",
    "    actor:Actor,\n",
    "    # The critic model to use\n",
    "    critic:Critic,\n",
    "    # A list of dls, where index=0 is the training dl.\n",
    "    dls:List[object],\n",
    "    # Optional logger bases to log training/validation data to.\n",
    "    logger_bases:Optional[Callable]=None,\n",
    "    # The learning rate for the actor. Expected to learn slower than the critic\n",
    "    actor_lr:float=1e-4,\n",
    "    # The optimizer for the actor\n",
    "    actor_opt:torch.optim.Optimizer=Adam,\n",
    "    # The learning rate for the critic. Expected to learn faster than the actor\n",
    "    critic_lr:float=1e-3,\n",
    "    # The optimizer for the critic\n",
    "    # Note that weight decay doesnt seem to be great for \n",
    "    # Pendulum, so we use regular Adam, which has the decay rate\n",
    "    # set to 0. (Lillicrap et al., 2016) would instead use AdamW\n",
    "    critic_opt:torch.optim.Optimizer=Adam,\n",
    "    # Reference: GymStepper docs\n",
    "    nsteps:int=1,\n",
    "    # The device for the entire pipeline to use. Will move the agent, dls, \n",
    "    # and learner to that device.\n",
    "    device:torch.device=None,\n",
    "    # Number of batches per epoch\n",
    "    batches:int=None,\n",
    "    # Debug mode will output device moves\n",
    "    debug:bool=False,\n",
    "    ppo_epochs = 10,\n",
    "    ppo_batch_sz = 64,\n",
    "    ppo_eps = 0.2,\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(actor,dls[0])\n",
    "    learner = BatchCollector(learner,batches=batches)\n",
    "    learner = EpochCollector(learner)\n",
    "    if logger_bases: \n",
    "        learner = logger_bases(learner)\n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner).catch_records()\n",
    "    learner = StepBatcher(learner)\n",
    "    # learner = CriticLossProcessor(learner,critic=critic)\n",
    "    # learner = LossCollector(learner,title='critic-loss').catch_records()\n",
    "    # learner = BasicOptStepper(learner,critic,critic_lr,opt=critic_opt,filter=True,do_zero_grad=False)\n",
    "    learner = PPOActorOptAndLossProcessor(learner,actor=actor,actor_lr=actor_lr,\n",
    "                                          critic=critic,critic_lr=critic_lr,ppo_epochs=ppo_epochs,\n",
    "                                          ppo_batch_sz=ppo_batch_sz,ppo_eps=ppo_eps)\n",
    "    learner = LossCollector(learner,title='actor-loss').catch_records()\n",
    "    learner = LearnerHead(learner,(actor,critic))\n",
    "    \n",
    "    return learner\n",
    "\n",
    "PPOLearner.__doc__=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders\n",
    "from fastrl.loggers.core import ProgressBarLogger\n",
    "from fastrl.agents.trpo import TRPOAgent,AdvantageGymDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45814d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53963b72c8ad40e7ba672e89b1e5e646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527061c3da1d4cb292daabeb9cb22b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>episode</th>\n",
       "      <th>rolling_reward</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>actor-loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-24.429207</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.494225</td>\n",
       "      <td>0.066967</td>\n",
       "      <td>-2.6739354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-23.218408</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.028452</td>\n",
       "      <td>-2.8338091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-22.785492</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.466163</td>\n",
       "      <td>0.214959</td>\n",
       "      <td>-1.7720228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-22.401961</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.457776</td>\n",
       "      <td>-0.010530</td>\n",
       "      <td>-1.7371688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-22.117896</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.447333</td>\n",
       "      <td>-0.062435</td>\n",
       "      <td>-3.4784434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>-21.779138</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.435938</td>\n",
       "      <td>-0.172362</td>\n",
       "      <td>-2.7109303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>-21.439993</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.423310</td>\n",
       "      <td>-0.036928</td>\n",
       "      <td>-1.5233021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>-21.141646</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.410206</td>\n",
       "      <td>-0.013177</td>\n",
       "      <td>-2.0156078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>-20.610226</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.392541</td>\n",
       "      <td>0.088212</td>\n",
       "      <td>-2.9441102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>-20.021371</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.376230</td>\n",
       "      <td>0.103705</td>\n",
       "      <td>-3.461331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval:false\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup Logger\n",
    "def logger_bases(pipe):\n",
    "    pipe = pipe.dump_records()\n",
    "    pipe = ProgressBarLogger(pipe)\n",
    "    return pipe\n",
    "# Setup up the core NN\n",
    "actor = Actor(2,1)\n",
    "critic = Critic(2)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = TRPOAgent(actor,do_logging=True,clip_min=-1,clip_max=1)\n",
    "\n",
    "# Setup the Dataloaders\n",
    "dls = dataloaders((\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=200,gamma=0.99,discount=0.99),MountainCarContinuous-v0\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=1,gamma=0.99,discount=0.99)\n",
    "    AdvantageGymDataPipe(['MountainCarContinuous-v0']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=200,gamma=0.99,discount=0.99),\n",
    "    AdvantageGymDataPipe(['MountainCarContinuous-v0']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=1,gamma=0.99,discount=0.99)\n",
    "))\n",
    "# Setup the Learner\n",
    "learner = PPOLearner(actor,critic,dls,logger_bases=logger_bases,batches=10,ppo_batch_sz = 64*2)\n",
    "# learner.fit(1)\n",
    "learner.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddc7107",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/fastrl_user/fastrl/nbs/07_Agents/02_Continuous/12u_agents.ppo.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6a6f736961682f66617374726c222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130302e3131322e35372e313331227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6a6f736961682f66617374726c2f2e646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/fastrl_user/fastrl/nbs/07_Agents/02_Continuous/12u_agents.ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m learner\u001b[39m.\u001b[39;49mvalidate()\n",
      "File \u001b[0;32m~/fastrl/fastrl/learner/core.py:107\u001b[0m, in \u001b[0;36mLearnerHead.validate\u001b[0;34m(self, epochs, batches, show)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate\u001b[39m(\u001b[39mself\u001b[39m,epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,batches\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,show\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataPipe:\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 107\u001b[0m     epocher \u001b[39m=\u001b[39m find_dp(traverse_dps(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_datapipes[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdp_idx]),EpochCollector)\n\u001b[1;32m    108\u001b[0m     epocher\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[1;32m    109\u001b[0m     batcher \u001b[39m=\u001b[39m find_dp(traverse_dps(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_datapipes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp_idx]),BatchCollector)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
