{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Union,Dict,Literal,List\n",
    "# Third party libs\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe\n",
    "from torch.optim import AdamW,Adam\n",
    "# # Local modules\n",
    "from fastrl.core import SimpleStep\n",
    "from fastrl.layers import Critic\n",
    "from fastrl.agents.trpo import Actor\n",
    "from fastrl.loggers.core import ProgressBarLogger\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead\n",
    "from fastrl.loggers.core import BatchCollector,EpochCollector,RollingTerminatedRewardCollector,EpisodeCollector\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "from fastrl.agents.ddpg import LossCollector,BasicOptStepper,StepBatcher\n",
    "from fastrl.agents.trpo import CriticLossProcessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# PPO\n",
    "> [Proximate Policy Gradients](https://arxiv.org/pdf/1707.06347.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d688b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PPOActorOptAndLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            actor:Actor,\n",
    "            # The learning rate\n",
    "            actor_lr:float,\n",
    "            critic:Critic,\n",
    "            # The learning rate\n",
    "            critic_lr:float,\n",
    "            # The optimizer to use\n",
    "            actor_opt:torch.optim.Optimizer=AdamW,\n",
    "            # The optimizer to use\n",
    "            critic_opt:torch.optim.Optimizer=AdamW,\n",
    "            ppo_epochs = 10,\n",
    "            ppo_batch_sz = 64,\n",
    "            ppo_eps = 0.2,\n",
    "            # kwargs to be passed to the `opt`\n",
    "            **opt_kwargs\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actor = actor\n",
    "        self.device = None\n",
    "        self.counter = 0\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic = critic\n",
    "        self.critic_lr = critic_lr\n",
    "        self.critic_opt = critic_opt\n",
    "        self.opt_kwargs = opt_kwargs\n",
    "        self.actor_opt = actor_opt\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        self._critic_opt = self.critic_opt(self.critic.parameters(),lr=self.critic_lr,**self.opt_kwargs)\n",
    "        self._actor_opt = self.actor_opt(self.actor.parameters(),lr=self.actor_lr,**self.opt_kwargs)\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.ppo_batch_sz = ppo_batch_sz\n",
    "        self.ppo_eps = ppo_eps\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "                batch.to(self.device)\n",
    "                traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            \n",
    "            dist = self.actor(batch.state)\n",
    "            old_log_prob = dist.log_prob(batch.action).detach()\n",
    "            loss = None\n",
    "\n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "\n",
    "            for epoch in range(self.ppo_epochs):\n",
    "                for ppo_batch in range(0,batch.advantage[m].shape[0],self.ppo_batch_sz):\n",
    "\n",
    "                    states_v = batch.state[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "                    action_v = batch.action[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "                    advantage = traj_adv_v[m][ppo_batch:ppo_batch+self.ppo_batch_sz].reshape(-1,)\n",
    "                    batch_ref_v = batch.next_advantage[m][ppo_batch:ppo_batch+self.ppo_batch_sz]\n",
    "\n",
    "                    self._critic_opt.zero_grad()\n",
    "                    value_v = self.critic(states_v)\n",
    "                    loss_value_v = self.critic_loss(value_v.squeeze(-1), batch_ref_v.squeeze(-1))\n",
    "                    loss_value_v.backward()\n",
    "                    self._critic_opt.step()\n",
    "\n",
    "                    self._actor_opt.zero_grad()\n",
    "                    \n",
    "                    try:\n",
    "                        dist = self.actor(states_v)\n",
    "                    except Exception as e:\n",
    "                        dist = self.actor(states_v)\n",
    "                    logprob_pi_v = dist.log_prob(action_v) #.detach()\n",
    "                    ratio_v = torch.exp(logprob_pi_v - old_log_prob[m][ppo_batch:ppo_batch+self.ppo_batch_sz]+1e-7)\n",
    "                    surr_obj_v = advantage * ratio_v\n",
    "                    clipped_surr_v = advantage * torch.clamp(ratio_v, 1.0 - self.ppo_eps, 1.0 + self.ppo_eps)\n",
    "                    loss_policy_v = -torch.min(surr_obj_v, clipped_surr_v).mean()\n",
    "                    loss_policy_v.backward()\n",
    "                    self._actor_opt.step()\n",
    "\n",
    "                    if loss is None:\n",
    "                        loss = loss_policy_v\n",
    "                    else:\n",
    "                        loss += loss_policy_v\n",
    "            yield {'loss':loss}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8cae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def PPOLearner(\n",
    "    # The actor model to use\n",
    "    actor:Actor,\n",
    "    # The critic model to use\n",
    "    critic:Critic,\n",
    "    # A list of dls, where index=0 is the training dl.\n",
    "    dls:List[object],\n",
    "    # Optional logger bases to log training/validation data to.\n",
    "    do_logging:bool=True,\n",
    "    # The learning rate for the actor. Expected to learn slower than the critic\n",
    "    actor_lr:float=1e-4,\n",
    "    # The optimizer for the actor\n",
    "    actor_opt:torch.optim.Optimizer=Adam,\n",
    "    # The learning rate for the critic. Expected to learn faster than the actor\n",
    "    critic_lr:float=1e-3,\n",
    "    # The optimizer for the critic\n",
    "    # Note that weight decay doesnt seem to be great for \n",
    "    # Pendulum, so we use regular Adam, which has the decay rate\n",
    "    # set to 0. (Lillicrap et al., 2016) would instead use AdamW\n",
    "    critic_opt:torch.optim.Optimizer=Adam,\n",
    "    # Reference: GymStepper docs\n",
    "    nsteps:int=1,\n",
    "    # The device for the entire pipeline to use. Will move the agent, dls, \n",
    "    # and learner to that device.\n",
    "    device:torch.device=None,\n",
    "    # Number of batches per epoch\n",
    "    batches:int=None,\n",
    "    # Debug mode will output device moves\n",
    "    debug:bool=False,\n",
    "    ppo_epochs = 10,\n",
    "    ppo_batch_sz = 64,\n",
    "    ppo_eps = 0.2,\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase({'actor':actor,'critic':critic},dls[0])\n",
    "    learner = BatchCollector(learner,batches=batches)\n",
    "    learner = EpochCollector(learner)\n",
    "    if do_logging: \n",
    "        learner = learner.dump_records()\n",
    "        learner = ProgressBarLogger(learner)\n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner).catch_records()\n",
    "    learner = StepBatcher(learner)\n",
    "    learner = PPOActorOptAndLossProcessor(learner,actor=actor,actor_lr=actor_lr,\n",
    "                                          critic=critic,critic_lr=critic_lr,ppo_epochs=ppo_epochs,\n",
    "                                          ppo_batch_sz=ppo_batch_sz,ppo_eps=ppo_eps)\n",
    "    learner = LossCollector(learner,title='actor-loss').catch_records()\n",
    "\n",
    "    if len(dls)==2:\n",
    "        val_learner = LearnerBase({'actor':actor,'critic':critic},dls[1]).visualize_vscode()\n",
    "        val_learner = BatchCollector(val_learner,batches=batches)\n",
    "        val_learner = EpochCollector(val_learner).catch_records(drop=True)\n",
    "        return LearnerHead((learner,val_learner))\n",
    "    else:\n",
    "        return LearnerHead(learner)\n",
    "    \n",
    "    return learner\n",
    "\n",
    "PPOLearner.__doc__=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders\n",
    "from fastrl.agents.trpo import TRPOAgent,AdvantageGymDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45814d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup Logger\n",
    "def logger_bases(pipe):\n",
    "    pipe = pipe.dump_records()\n",
    "    pipe = ProgressBarLogger(pipe)\n",
    "    return pipe\n",
    "# Setup up the core NN\n",
    "actor = Actor(2,1)\n",
    "critic = Critic(2)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = TRPOAgent(actor,do_logging=True,clip_min=-1,clip_max=1)\n",
    "\n",
    "# Setup the Dataloaders\n",
    "params = dict(source=['MountainCarContinuous-v0']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,gamma=0.99,discount=0.99)\n",
    "\n",
    "dls = dataloaders((\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=200,gamma=0.99,discount=0.99),\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=1,gamma=0.99,discount=0.99)\n",
    "    AdvantageGymDataPipe(**params,bs=200),\n",
    "    AdvantageGymDataPipe(**params,bs=1,include_images=True)\n",
    "))\n",
    "# Setup the Learner\n",
    "learner = PPOLearner(actor,critic,dls,batches=10,ppo_batch_sz = 64*2)\n",
    "# learner.fit(1)\n",
    "learner.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
