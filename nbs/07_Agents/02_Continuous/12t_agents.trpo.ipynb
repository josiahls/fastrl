{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import *\n",
    "from typing_extensions import Literal\n",
    "import typing \n",
    "from warnings import warn\n",
    "# Third party libs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import *\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse,replace_dp\n",
    "from fastcore.all import test_eq,test_ne,add_docs,store_attr,ifnone,L\n",
    "from torchdata.dataloader2.graph import find_dps,traverse\n",
    "from fastrl.data.dataloader2 import *\n",
    "from torchdata.dataloader2 import DataLoader2,DataLoader2Iterator\n",
    "from torchdata.dataloader2.graph import find_dps,traverse,DataPipe,IterDataPipe,MapDataPipe\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.torch_core import *\n",
    "from fastrl.layers import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.envs.gym import *\n",
    "from fastrl.agents.ddpg import LossCollector,BasicOptStepper,StepBatcher\n",
    "from fastrl.loggers.core import LogCollector\n",
    "from fastrl.agents.discrete import EpsilonCollector\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW,Adam\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead\n",
    "from fastrl.loggers.core import LoggerBasePassThrough,BatchCollector,EpocherCollector,RollingTerminatedRewardCollector,EpisodeCollector\n",
    "\n",
    "from fastrl.agents.ddpg import BasicOptStepper\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeTransformBlock\n",
    "from fastrl.loggers.jupyter_visualizers import ProgressBarLogger\n",
    "from fastrl.layers import Critic\n",
    "from fastrl.agents.discrete import EpsilonCollector\n",
    "from fastrl.agents.core import AgentHead,StepFieldSelector,AgentBase \n",
    "from fastrl.agents.ddpg import ActionClip,ActionUnbatcher,NumpyConverter,OrnsteinUhlenbeck,SimpleModelRunner\n",
    "from fastrl.loggers.core import LoggerBase,CacheLoggerBase\n",
    "from fastrl.dataloader2_ext import InputInjester\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.pipes.iter.nskip import *\n",
    "from fastrl.pipes.iter.nstep import *\n",
    "from fastrl.pipes.iter.firstlast import *\n",
    "from fastrl.pipes.iter.transforms import *\n",
    "from fastrl.pipes.map.transforms import *\n",
    "from fastrl.data.block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "> Trust Region Policy Optimization via online-learning for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5b8f6",
   "metadata": {},
   "source": [
    "[(Schulman et al., 2015) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477).\n",
    "\n",
    "Directly based on [`ikostrikov`'s implimentation](https://github.com/ikostrikov/pytorch-trpo) and\n",
    "coda / explainations in [Shewchuk Cs.Cmu.Edu, 2022, Accessed 19 Nov 2022.](cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c2cba",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageStep(typing.NamedTuple):\n",
    "    state:           torch.FloatTensor=torch.FloatTensor([0])\n",
    "    action:          torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_state:      torch.FloatTensor=torch.FloatTensor([0])\n",
    "    terminated:      torch.BoolTensor=torch.BoolTensor([1])\n",
    "    truncated:       torch.BoolTensor=torch.BoolTensor([1])\n",
    "    reward:          torch.FloatTensor=torch.LongTensor([0])\n",
    "    total_reward:    torch.FloatTensor=torch.FloatTensor([0])\n",
    "    advantage:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_advantage:  torch.FloatTensor=torch.FloatTensor([0])\n",
    "    env_id:          torch.LongTensor=torch.LongTensor([0])\n",
    "    proc_id:         torch.LongTensor=torch.LongTensor([0])\n",
    "    step_n:          torch.LongTensor=torch.LongTensor([0])\n",
    "    episode_n:       torch.LongTensor=torch.LongTensor([0])\n",
    "    image:           torch.FloatTensor=torch.FloatTensor([0])\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).clone() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def detach(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).detach() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def device(self,device='cpu'):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(device=device) for fld in self.__class__._fields}\n",
    "        )\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(*args,**kwargs) for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls,seed=None,**flds):\n",
    "        _flds,_annos = cls._fields,cls.__annotations__\n",
    "\n",
    "        def _random_annos(anno):\n",
    "            t = anno(1)\n",
    "            if anno==torch.BoolTensor: t.random_(2) \n",
    "            else:                      t.random_(100)\n",
    "            return t\n",
    "\n",
    "        return cls(\n",
    "            *(flds.get(\n",
    "                f,_random_annos(_annos[f])\n",
    "            ) for f in _flds)\n",
    "        )\n",
    "\n",
    "add_namedtuple_doc(\n",
    "AdvantageStep,\n",
    "\"\"\"Represents a single step in an environment similar to `SimpleStep` however has\n",
    "an addition field called `advantage`.\"\"\",\n",
    "advantage=\"\"\"Generally characterized as $A(s,a) = Q(s,a) - V(s)$\"\"\",\n",
    "**{f:getattr(SimpleStep,f).__doc__ for f in SimpleStep._fields}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f777630",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(AdvantageStep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62d5ed",
   "metadata": {},
   "source": [
    "## Memory\n",
    "> Policy gradient online models use short term trajectory samples instead of\n",
    "ER / iid memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pipe2device(pipe,device,debug=False):\n",
    "    \"Attempt to move an entire `pipe` and its pipeline to `device`\"\n",
    "    pipes = find_dps(traverse(pipe),dp.iter.IterDataPipe,include_subclasses=True)\n",
    "    for pipe in pipes:\n",
    "        if hasattr(pipe,'to'): \n",
    "            if debug: print(f'Moving {pipe} to {device}')\n",
    "            pipe.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@torch.jit.script\n",
    "def discounted_cumsum_(t:torch.Tensor,gamma:float,reverse:bool=False):\n",
    "    \"\"\"Performs a cumulative sum on `t` where `gamma` is applied for each index\n",
    "    >1.\"\"\"\n",
    "    if reverse:\n",
    "        # We do +2 because +1 is needed to avoid out of index t[idx], and +2 is needed\n",
    "        # to avoid out of index for t[idx+1].\n",
    "        for idx in range(t.size(0)-2,-1,-1):\n",
    "            t[idx] = t[idx] + t[idx+1] * gamma\n",
    "    else:\n",
    "        for idx in range(1,t.size(0)):\n",
    "            t[idx] = t[idx] + t[idx-1] * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_flat_params_from(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params\n",
    "\n",
    "\n",
    "def set_flat_params_to(model, flat_params):\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "        prev_ind += flat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d26899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/2\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "#     discounted_cumsum(torch.ones(500),0.99)\n",
    "# print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageBuffer(dp.iter.IterDataPipe):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "            # A datapipe that produces `StepType`s.\n",
    "            source_datapipe:DataPipe,\n",
    "            # A model that takes in a `state` and outputs a single value \n",
    "            # representing $V$, where as $Q$ is $V + reward$\n",
    "            critic:nn.Module,\n",
    "            # Will accumulate up to `bs` or when the episode has terminated.\n",
    "            bs=1000,\n",
    "            # The discount factor, otherwise known as $\\gamma$, is defined in \n",
    "            # (Shulman et al., 2016) as '... $\\gamma$ introduces bias into\n",
    "            # the policy gradient estimate...'.\n",
    "            discount:float=0.99,\n",
    "            # $\\lambda$ is unqiue to GAE and manages importance to values when \n",
    "            # they are in accurate is defined in (Shulman et al., 2016) as '... $\\lambda$ < 1\n",
    "            # introduces bias only when the value function is inaccurate....'.\n",
    "            gamma:float=0.99\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.bs = bs\n",
    "        self.critic = critic\n",
    "        self.device = None\n",
    "        self.discount = discount\n",
    "        self.gamma = gamma\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({k:v if k!='env_advantage_buffer' else f'{len(self)} elements' \n",
    "                    for k,v in self.__dict__.items()})\n",
    "\n",
    "    def __len__(self): return self._sz_tracker\n",
    "\n",
    "    def update_advantage_buffer(self,step:StepType) -> int:\n",
    "        if self.debug: \n",
    "            print('Adding to advantage buffer: ',step)\n",
    "        env_id = int(step.env_id.detach().cpu())\n",
    "        if env_id not in self.env_advantage_buffer: \n",
    "            self.env_advantage_buffer[env_id] = []\n",
    "        self.env_advantage_buffer[env_id].append(step)\n",
    "        return env_id\n",
    "        \n",
    "    def zip_steps(\n",
    "            self,\n",
    "            steps:List[StepType]\n",
    "        ) -> Tuple[torch.FloatTensor,torch.FloatTensor,torch.BoolTensor]:\n",
    "            step_subset = [(o.reward,o.state,o.truncated or o.terminated) for o in steps]\n",
    "            zipped_fields = zip(*step_subset)\n",
    "            return L(zipped_fields).map(torch.vstack)\n",
    "\n",
    "    def delta_calc(self,reward,v,v_next,done):\n",
    "        return reward + (self.gamma * v * done) - v_next\n",
    "\n",
    "    def __iter__(self) -> AdvantageStep:\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "        for step in self.source_datapipe:\n",
    "            env_id = self.update_advantage_buffer(step)\n",
    "            done = step.truncated or step.terminated\n",
    "            if done or len(self.env_advantage_buffer[env_id])>self.bs:\n",
    "                steps = self.env_advantage_buffer[env_id]\n",
    "                rewards,states,dones = self.zip_steps(steps)\n",
    "                # We vstack the final next_state so we have a complete picture\n",
    "                # of the state transitions and matching reward/done shapes.\n",
    "                values = self.critic(torch.vstack((states,steps[-1].next_state)))\n",
    "                delta = self.delta_calc(rewards,values[:-1],values[1:],dones)\n",
    "                discounted_cumsum_(delta,self.discount*self.gamma,reverse=True)\n",
    "\n",
    "                for _step,gae_advantage,v in zip(*(steps,delta,values)):\n",
    "                    yield AdvantageStep(\n",
    "                        advantage=gae_advantage,\n",
    "                        next_advantage=gae_advantage+v,\n",
    "                        **{f:getattr(_step,f) for f in _step._fields}\n",
    "                    )\n",
    "                self.env_advantage_buffer[env_id].clear()\n",
    "\n",
    "    @classmethod\n",
    "    def insert_dp(cls,critic,old_dp=GymStepper) -> Callable[[DataPipe],DataPipe]:\n",
    "        def _insert_dp(pipe):\n",
    "            v = replace_dp(\n",
    "                traverse(pipe,only_datapipe=True),\n",
    "                find_dp(traverse(pipe,only_datapipe=True),old_dp),\n",
    "                cls(find_dp(traverse(pipe,only_datapipe=True),old_dp),critic=critic)\n",
    "            )\n",
    "            return list(v.values())[0][0]\n",
    "        return _insert_dp\n",
    "\n",
    "add_docs(\n",
    "AdvantageBuffer,\n",
    "\"\"\"Collects an entire episode, calculates the advantage for each step, then\n",
    "yields that episode's `AdvantageStep`s.\n",
    "\n",
    "This is described in the original paper `(Shulman et al., 2016) High-Dimensional \n",
    "Continuous Control Usin Generalized Advantage Estimation`.\n",
    "\n",
    "This algorithm is based on the concept of advantage:\n",
    "\n",
    "$A_{\\pi}(s,a) = Q_{\\pi}(s,a) - V_{\\pi}(s)$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 5 calculates it as:\n",
    "\n",
    "$\\hat{A}_{t}^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}^V$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 4 defines $\\delta$ as:\n",
    "\n",
    "$\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$\n",
    "\"\"\",\n",
    "to=torch.Tensor.to.__doc__,\n",
    "update_advantage_buffer=\"Adds `step` to `env_advantage_buffer` based on the environment id.\",\n",
    "zip_steps=\"\"\"Given `steps`, strip out the `Tuple[reward,state,truncated or terminated]` fields,\n",
    "and `torch.vstack` them.\"\"\",\n",
    "delta_calc=\"\"\"Calculates $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$ which \n",
    "is the advantage difference between state transitions.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(3,0)\n",
    "\n",
    "gym_pipe = GymTransformBlock(\n",
    "    agent=None,seed=0,\n",
    "    dp_augmentation_fns=[AdvantageBuffer.insert_dp(critic=critic)]\n",
    ")(['Pendulum-v1'])\n",
    "\n",
    "for chunk in gym_pipe.header(5):\n",
    "    for step in chunk:\n",
    "        test_eq(type(step),AdvantageStep)\n",
    "        assert step.advantage!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a48c46",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OptionalClampLinear(Module):\n",
    "    def __init__(self,num_inputs,state_dims,fix_variance:bool=False,\n",
    "                 clip_min=0.3,clip_max=10.0):\n",
    "        \"Linear layer or constant block used for std.\"\n",
    "        store_attr()\n",
    "        if not self.fix_variance: \n",
    "            self.fc=nn.Linear(self.num_inputs,self.state_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.fix_variance: \n",
    "            return torch.full((x.shape[0],self.state_dims),1.0)\n",
    "        else:                 \n",
    "            return torch.clamp(nn.Softplus()(self.fc(x)),self.clip_min,self.clip_max)\n",
    "\n",
    "# TODO(josiahls): This is probably a highly generic SimpleGMM tbh. Once we know this\n",
    "# works, we should just rename this to SimpleGMM\n",
    "class Actor(Module):\n",
    "    def __init__(            \n",
    "            self,\n",
    "            state_sz:int,   # The input dim of the state / flattened conv output\n",
    "            action_sz:int,  # The output dim of the actions\n",
    "            hidden:int=400, # Number of neurons connected between the 2 input/output layers\n",
    "            fix_variance:bool=False,\n",
    "            clip_min=0.3,\n",
    "            clip_max=10.0\n",
    "        ):\n",
    "        \"Single-component GMM parameterized by a fully connected layer with optional std layer.\"\n",
    "        store_attr()\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(state_sz, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, action_sz),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        # self.std = OptionalClampLinear(state_sz,action_sz,fix_variance,\n",
    "        #                                clip_min=clip_min,clip_max=clip_max)\n",
    "        # self.std = nn.Linear(state_sz,action_sz)\n",
    "        # self.std.weight.data.fill_(0.5)\n",
    "        # self.std.bias.data.fill_(0.5)\n",
    "        self.std = nn.Parameter(torch.zeros(action_sz)+.5)\n",
    "        \n",
    "    def forward(self,x): return Independent(Normal(self.mu(x),self.std),1)\n",
    "\n",
    "\n",
    "add_docs(\n",
    "Actor,\n",
    "\"\"\"Produces continuous outputs from mean of a Gaussian distribution.\"\"\",\n",
    "forward=\"Mean outputs from a parameterized Gaussian distribution.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f645774",
   "metadata": {},
   "source": [
    "The `Actor` is developed from the description found in `(Schulman et al., 2015)`: \n",
    "\n",
    "    ...we used a Gaussian distribution, where the covariance matrix was diagonal \n",
    "    and independent of the state. A neural network with several fully-connected (dense) \n",
    "    layers maps from the input features to the mean of a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,2)\n",
    "dist = actor(torch.randn(1,4))\n",
    "dist.mean,dist.stddev,dist.log_prob(torch.randn(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07959357",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ff436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class NormalExploration(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "                self,\n",
    "                source_datapipe:DataPipe,\n",
    "                # Based on the `base_agent.model.training`, by default no decrement or step tracking will\n",
    "                # occur during validation steps.\n",
    "                decrement_on_val:bool=False,\n",
    "                # Based on the `base_agent.model.training`, by default random actions will not be attempted\n",
    "                explore_on_val:bool=False,\n",
    "                # Also return the original action prior to exploratory noise\n",
    "                ret_original:bool=False,\n",
    "        ):\n",
    "                self.source_datapipe = source_datapipe\n",
    "                self.decrement_on_val = decrement_on_val\n",
    "                self.explore_on_val = explore_on_val\n",
    "                self.ret_original = ret_original\n",
    "                self.agent_base = None\n",
    "                self.agent_base = find_dp(traverse(self.source_datapipe),AgentBase)\n",
    "                self.model = self.agent_base.model\n",
    "                self.last_std = None \n",
    "                self.last_mean = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        for action in self.source_datapipe:\n",
    "                if not issubclass(action.__class__,Independent):\n",
    "                        raise Exception(f'Expected Independent, got {type(action)}\\n{action}')\n",
    "\n",
    "                # Add a batch dim if missing\n",
    "                if len(action.batch_shape)==0: action = action.expand((1,))\n",
    "\n",
    "                self.last_mean = action.mean\n",
    "                self.last_std = action.stddev\n",
    "                if self.explore_on_val or self.agent_base.model.training:\n",
    "                        if self.ret_original: yield (action.sample(),action.mean)\n",
    "                        else:                 yield action.sample()\n",
    "                else:\n",
    "                        yield action.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,2)\n",
    "agent_base = AgentBase(actor)\n",
    "agent = SimpleModelRunner(agent_base)\n",
    "agent = NormalExploration(agent,explore_on_val=True)\n",
    "agent = ActionClip(agent)\n",
    "agent = AgentHead(agent)\n",
    "for action in agent(torch.randn(3,4)):\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageGymTransformBlock():\n",
    "\n",
    "    def __init__(self,\n",
    "        agent:DataPipe, # An AgentHead\n",
    "        critic:Critic,\n",
    "        seed:Optional[int]=None, # The seed for the gym to use\n",
    "        # Used by `NStepper`, outputs tuples / chunks of assiciated steps\n",
    "        nsteps:int=1, \n",
    "        # Used by `NSkipper` to skip a certain number of steps (agent still gets called for each)\n",
    "        nskips:int=1,\n",
    "        # Whether when nsteps>1 to merge it into a single `StepType`\n",
    "        firstlast:bool=False,\n",
    "        # Functions to run once, at the beginning of the pipeline\n",
    "        type_tfms:Optional[List[Callable]]=None,\n",
    "        # Functions to run over individual steps before batching\n",
    "        item_tfms:Optional[List[Callable]]=None,\n",
    "        # Functions to run over batches (as specified by `bs`)\n",
    "        batch_tfms:Optional[List[Callable]]=None,\n",
    "        # The batch size, which is different from `nsteps` in that firstlast will be \n",
    "        # run prior to batching, and a batch of steps might come from multiple envs,\n",
    "        # where nstep is associated with a single env\n",
    "        bs:int=1,\n",
    "        # The max steps for the advatage buffer to run an environment\n",
    "        max_steps:int=200,\n",
    "        discount=0.99,\n",
    "        gamma:float=0.99,\n",
    "        # The prefered default is for the pipeline to be infinate, and the learner\n",
    "        # decides how much to iter. If this is not None, then the pipeline will run for \n",
    "        # that number of `n`\n",
    "        n:Optional[int]=None,\n",
    "        # Whether to reset all the envs at the same time as opposed to reseting them \n",
    "        # the moment an episode ends. \n",
    "        synchronized_reset:bool=False,\n",
    "        # Should be used only for validation / logging, will grab a render of the gym\n",
    "        # and assign to the `StepType` image field. This data should not be used for training.\n",
    "        # If it images are needed for training, then you should wrap the env instead. \n",
    "        include_images:bool=False,\n",
    "        # If an environment truncates, terminate it.\n",
    "        terminate_on_truncation:bool=True,\n",
    "        # Additional pipelines to insert, replace, remove\n",
    "        dp_augmentation_fns:Tuple[DataPipeAugmentationFn]=None\n",
    "    ) -> None:\n",
    "        \"Basic OpenAi gym `DataPipeGraph` with first-last, nstep, and nskip capability\"\n",
    "        self.agent = agent\n",
    "        store_attr()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        # `source` likely will be an iterable that gets pushed into the pipeline when an \n",
    "        # experiment is actually being run.\n",
    "        source:Any,\n",
    "        # Any parameters needed for the dataloader\n",
    "        num_workers:int=0,\n",
    "        # This param must exist: as_dataloader for the datablock to create dataloaders\n",
    "        as_dataloader:bool=False\n",
    "    ) -> DataPipeOrDataLoader:\n",
    "        _type_tfms = ifnone(self.type_tfms,GymTypeTransform)\n",
    "        \"This is the function that is actually run by `DataBlock`\"\n",
    "        pipe = dp.map.Mapper(source)\n",
    "        pipe = TypeTransformer(pipe,_type_tfms)\n",
    "        pipe = dp.iter.MapToIterConverter(pipe)\n",
    "        pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "        pipe = pipe.cycle() # Cycle through the envs inf\n",
    "        pipe = GymStepper(pipe,agent=self.agent,seed=self.seed,\n",
    "                          include_images=self.include_images,\n",
    "                          terminate_on_truncation=self.terminate_on_truncation,\n",
    "                          synchronized_reset=self.synchronized_reset)\n",
    "        if self.nskips!=1: pipe = NSkipper(pipe,n=self.nskips)\n",
    "        if self.nsteps!=1:\n",
    "            pipe = NStepper(pipe,n=self.nsteps)\n",
    "            if self.firstlast:\n",
    "                pipe = FirstLastMerger(pipe)\n",
    "            else:\n",
    "                pipe = NStepFlattener(pipe) # We dont want to flatten if using FirstLastMerger\n",
    "        pipe = AdvantageBuffer(pipe,critic=self.critic,bs=self.max_steps,\n",
    "                               discount=self.discount,gamma=self.gamma)\n",
    "        if self.n is not None: pipe = pipe.header(limit=self.n)\n",
    "        pipe = ItemTransformer(pipe,self.item_tfms)\n",
    "        pipe = pipe.batch(batch_size=self.bs)\n",
    "        pipe = BatchTransformer(pipe,self.batch_tfms)\n",
    "        \n",
    "        pipe = apply_dp_augmentation_fns(pipe,ifnone(self.dp_augmentation_fns,()))\n",
    "        \n",
    "        if as_dataloader:\n",
    "            pipe = DataLoader2(\n",
    "                datapipe=pipe,\n",
    "                reading_service=PrototypeMultiProcessingReadingService(\n",
    "                    num_workers = num_workers,\n",
    "                    protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "                    protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "                    pipe_type = item_input_pipe_type,\n",
    "                    eventloop = SpawnProcessForDataPipeline\n",
    "                ) if num_workers>0 else None\n",
    "            )\n",
    "        return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3a0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51820a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd0aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProbabilisticStdCollector(LogCollector):\n",
    "    header:str='std'\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.record_pipe = find_dp(traverse(self.source_datapipe),NormalExploration)\n",
    "        self.main_buffers = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # for q in self.main_buffers: q.append(Record('epsilon',None))\n",
    "        for action in self.source_datapipe:\n",
    "            for q in self.main_buffers: \n",
    "                q.append(Record('std',self.record_pipe.last_std.item()))\n",
    "            yield action\n",
    "\n",
    "class ProbabilisticMeanCollector(LogCollector):\n",
    "    header:str='mean'\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.record_pipe = find_dp(traverse(self.source_datapipe),NormalExploration)\n",
    "        self.main_buffers = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # for q in self.main_buffers: q.append(Record('epsilon',None))\n",
    "        for action in self.source_datapipe:\n",
    "            for q in self.main_buffers: \n",
    "                q.append(Record('mean',self.record_pipe.last_mean.item()))\n",
    "            yield action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab218d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TRPOAgent(\n",
    "    model:Actor, # The actor to use for mapping states to actions\n",
    "    # LoggerBases push logs to. If None, logs will be collected and output\n",
    "    # by the dataloader.\n",
    "    logger_bases:Optional[LoggerBase]=None, \n",
    "    clip_min=-1,\n",
    "    clip_max=1,\n",
    "    # Any augmentations to the DDPG agent.\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None\n",
    ")->AgentHead:\n",
    "    \"Produces continuous action outputs.\"\n",
    "    agent_base = AgentBase(model,logger_bases=ifnone(logger_bases,[CacheLoggerBase()]))\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = InputInjester(agent)\n",
    "    agent = SimpleModelRunner(agent)\n",
    "    agent = NormalExploration(agent)\n",
    "    # agent = ProbabilisticStdCollector(agent)\n",
    "    # agent = ProbabilisticMeanCollector(agent)\n",
    "    agent = ActionClip(agent,clip_min=clip_min,clip_max=clip_max)\n",
    "    agent = ActionUnbatcher(agent)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = AgentHead(agent)\n",
    "    \n",
    "    agent = apply_dp_augmentation_fns(agent,dp_augmentation_fns)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac25225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.loggers.vscode_visualizers import VSCodeTransformBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "torch.manual_seed(0)\n",
    "\n",
    "actor = Actor(3,1)\n",
    "critic = Critic(3)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = TRPOAgent(actor)\n",
    "\n",
    "pipe = AdvantageGymTransformBlock(agent=agent,n=100,seed=None,include_images=True,critic=critic)(['Pendulum-v1'])\n",
    "pipe = VSCodeTransformBlock()(pipe)\n",
    "\n",
    "pipe2device(pipe,'cpu',debug=True)\n",
    "\n",
    "L(pipe);\n",
    "pipe.show(step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81bdf",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe248ac",
   "metadata": {},
   "source": [
    "![](../../images/(Schulman%20et%20al.%2C%202017)%20%5BTRPO%5D%20Trust%20Region%20Policy%20Optimization%20Algorithm%201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c473768",
   "metadata": {},
   "source": [
    "We start with finding the direction of conjugate gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(\n",
    "    [[3.,2.],[2.,6.]]\n",
    ")\n",
    "b = torch.tensor([[2.],[-8.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d3067",
   "metadata": {},
   "source": [
    "Ref [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf), but\n",
    "$A$ is the gradients of the model, and $b$ (typically the bias) is the loss.\n",
    "\n",
    "The below function is pretty much example `B2` pg 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def conjugate_gradients(\n",
    "    # A function that takes the direction `d` and applies it to `A`.\n",
    "    # The simplest example of this found would be:\n",
    "    # `lambda d:A@d`\n",
    "    Ad_f:Callable[[torch.Tensor],torch.Tensor],  \n",
    "    # The bias or in TRPO's case the loss.\n",
    "    b:torch.Tensor, \n",
    "    # Number of steps to go for assuming we are not less than `residual_tol`.\n",
    "    nsteps:int, \n",
    "    # If the residual is less than this, then we have arrived at the local minimum.\n",
    "    # Note that (Shewchuk, 1994) they mention that this should be E^2 * rdotr_0\n",
    "    residual_tol=1e-10, \n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # The final direction to go in.\n",
    "    x = torch.zeros(b.size()).to(device)\n",
    "    # Would typically be b - Ax, however in TRPO's case this has already been \n",
    "    # done in the loss function.\n",
    "    r = b.clone()\n",
    "    # The first direction is the first residual.\n",
    "    d = b.clone()\n",
    "    rdotr = r.T @ r # \\sigma_{new} pg50\n",
    "    for i in range(nsteps):\n",
    "        _Ad = Ad_f(d) # _Ad is also considered `q`\n",
    "        # Determines the size / rate / step size of the direction\n",
    "        alpha = rdotr / (d.T @ _Ad)\n",
    "\n",
    "        x += alpha * d\n",
    "        # [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) pg 49:\n",
    "        #\n",
    "        # The fast recursive formula for the residual is usually used, but once every 50 iterations, the exact residual\n",
    "        # is recalculated to remove accumulated floating point error. Of course, the number 50 is arbitrary; for large\n",
    "        # n \\sqrt{n}, ©\n",
    "        # might be appropriate.\n",
    "        #\n",
    "        # @josiah: This is kind of weird since we are using `Ad_f`. Maybe we can\n",
    "        # have an optional param for A direction to do the residual reset?\n",
    "        #\n",
    "        # if nsteps > 50 and i % int(torch.sqrt(i)) == 0:\n",
    "        #     r = b - Ax\n",
    "        # else:\n",
    "        r -= alpha * _Ad\n",
    "        new_rdotr = r.T @ r\n",
    "        beta = new_rdotr / rdotr\n",
    "        d = r + beta * d\n",
    "        rdotr = new_rdotr\n",
    "        # Same as \\sigma_{new} < E^2\\sigma\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "add_docs(\n",
    "conjugate_gradients,\n",
    "\"\"\"Conjugating Gradients builds on the idea of Conjugate Directions.\n",
    "\n",
    "As noted in:\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "\n",
    "We want \"everytime we take a step, we got it right the first time\" pg 21. \n",
    "\n",
    "In otherwords, we have a model, and we have the gradients and the loss. Using the \n",
    "loss, what is the the smartest way to change/optimize the gradients?\n",
    "\n",
    "`Conjugation` is the act of makeing the `parameter space / gradient space` easier to \n",
    "optimize over. In technical terms, we find `nsteps` directions to change the gradients\n",
    "toward that are orthogonal to each other and to the `parameter space / gradient space`.\n",
    "\n",
    "In otherwords, what is the direction that is most optimal, and what is the \n",
    "direction that if used to find `x` will reduce `Ax - b` to 0. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjugate_gradients(\n",
    "    lambda d:A@d,\n",
    "    b - A@torch.tensor([[50.],[50.]]),\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def backtrack_line_search(\n",
    "    # A Tensor of gradients or weights to optimize\n",
    "    x:torch.Tensor,\n",
    "    # The residual that when applied to `x`, hopefully optimizes it closer to the \n",
    "    # solution/ i.e. is orthogonal.\n",
    "    r:torch.Tensor,\n",
    "    # An error function that outputs the new error given the `x_new, where\n",
    "    # `x_new` is passed as a param, and the error is returned as a float.\n",
    "    # This error is compared, and expected greater than 0.\n",
    "    error_f:Callable[[torch.Tensor],float],\n",
    "    # The region of improvement we expect the see.\n",
    "    expected_improvement_rate:torch.Tensor,\n",
    "    # The minimal amount of improvement we expect to see.\n",
    "    accaptance_tolerance:float=0.1,\n",
    "    # The number of increments to attempt to improve `x`. \n",
    "    # Each \"backtrack\", the step size on the weights will be larger.\n",
    "    n_max_backtracks:int=10\n",
    "):\n",
    "    e = error_f(x)\n",
    "    # print(\"fval before\", e.item())\n",
    "    for (n_back,alpha) in enumerate(.5**torch.arange(0,n_max_backtracks)):\n",
    "        x_new = x + alpha * r \n",
    "        e_new = error_f(x_new)\n",
    "        improvement = e - e_new\n",
    "        expected_improvement = expected_improvement_rate * alpha \n",
    "        ratio = improvement / expected_improvement\n",
    "        if ratio.item() > accaptance_tolerance and improvement.item() > 0:\n",
    "            # print(\"fval after\", e_new.item(),' on ',n_back)\n",
    "            return True, x_new\n",
    "    return False, x\n",
    "\n",
    "add_docs(\n",
    "backtrack_line_search,\n",
    "\"\"\"Backtrack line search attempts an update to a set of weights/gradients `x` `n_max_backtracks` times.\n",
    "\n",
    "Each backtrack updates the weights/gradients a little more aggressively, and checks if `error_f`\n",
    "decreases / improves. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f29974",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "actor = Actor(4,2)\n",
    "dist = actor(torch.randn(1,4))\n",
    "old_log_prob_of_a = dist.log_prob(torch.randn(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def actor_prob_loss(weights,s,a,r,actor,old_log_prob):\n",
    "    if weights is not None:\n",
    "        set_flat_params_to(actor,weights)\n",
    "    dist = actor(s)\n",
    "    log_prob = dist.log_prob(a)\n",
    "    # loss = -r * torch.exp(log_prob-old_log_prob) \n",
    "    loss = -r.squeeze(1) * torch.exp(log_prob-old_log_prob) \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9af2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_prob_loss(None,torch.randn(1,4),torch.randn(1,2),torch.randn(1,1),actor,old_log_prob_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf80f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pre_hessian_kl(\n",
    "    model:Actor, # An Actor or any model that outputs a probability distribution\n",
    "    x:torch.Tensor # Input into the model\n",
    "):\n",
    "    r\"\"\"\n",
    "    Provides a KL conculation for the 2nd dirivative hessian to be calculated later.\n",
    "\n",
    "    It is important to note that this function will return a tensor of 0, however\n",
    "    the goal is to do autograd as opposed to doing anything with the value directly.\n",
    "\n",
    "    The \"confusing\" part of the code can be found in [4]:\n",
    "\n",
    "        \"For two univariate normal distributions p and q the above simplifies to:\"\n",
    "\n",
    "    $D_{\\text{KL}}\\left({\\mathcal {p}}\\parallel {\\mathcal {q}}\\right)=\\log {\\frac {\\sigma _{2}}{\\sigma _{1}}}+{\\frac {\\sigma _{1}^{2}+(\\mu _{1}-\\mu _{2})^{2}}{2\\sigma _{2}^{2}}}-{\\frac {1}{2}}$\n",
    "\n",
    "    Notes:\n",
    "    - [1] https://github.com/ikostrikov/pytorch-trpo/issues/2\n",
    "    - [2] [(Schulman et al., 2015) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "    \n",
    "    Appendix C:\n",
    "    \n",
    "        One could alternatively use a generic method for calculating Hessian-vector products using \n",
    "        reverse mode automatic differentiation ((Wright & Nocedal, 1999), chapter 8), computing the \n",
    "        Hessian of DKL with respect to θ. This method would be slightly less efficient as it does \n",
    "        not exploit the fact that the second derivatives of μ(x) (i.e., the second term in Equation (57))\n",
    "        can be ignored, but may be substantially easier to implement.\n",
    "\n",
    "    - [3] http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf\n",
    "    - [4] https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#cite_note-27\n",
    "    \"\"\"\n",
    "    dist = model(x)\n",
    "    mu_v = dist.mean\n",
    "    logstd_v = torch.log(dist.stddev)\n",
    "    mu0_v = mu_v.detach()\n",
    "    logstd0_v = logstd_v.detach()\n",
    "    std_v = torch.exp(logstd_v)\n",
    "    std0_v = std_v.detach()\n",
    "    kl = logstd_v - logstd0_v + (std0_v ** 2 + (mu0_v - mu_v) ** 2) / (2.0 * std_v ** 2) - 0.5\n",
    "    return kl.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a47df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_hessian_kl(actor,torch.randn(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def auto_flat(outputs,inputs,contiguous=False,create_graph=False)->torch.Tensor:\n",
    "    \"Calculates the gradients and flattens them into a single tensor\"\n",
    "    grads = torch.autograd.grad(outputs,inputs,create_graph=create_graph)\n",
    "    # TODO: Does it always need to be contiguous?\n",
    "    if contiguous:\n",
    "        return torch.cat([grad.contiguous().view(-1) for grad in grads])\n",
    "    else:\n",
    "        return torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "def forward_pass(\n",
    "        weights:torch.Tensor,\n",
    "        s:torch.Tensor,\n",
    "        actor:Actor,\n",
    "        damping:float=0.1\n",
    "    ):\n",
    "    kl = pre_hessian_kl(actor,s)\n",
    "    kl = kl.mean()\n",
    "\n",
    "    # Calculate the 1st derivative hessian\n",
    "    flat_grad_kl = auto_flat(kl,actor.parameters(),create_graph=True)\n",
    "\n",
    "    kl_v = (flat_grad_kl * weights.detach()).sum()\n",
    "    # Calculate the 2nd derivative hessian\n",
    "    flat_grad_grad_kl = auto_flat(kl_v,actor.parameters(),contiguous=True).data\n",
    "\n",
    "    return flat_grad_grad_kl + weights * damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b45efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,1)\n",
    "\n",
    "forward_pass(\n",
    "    get_flat_params_from(actor),    \n",
    "    torch.randn(1,4),\n",
    "    actor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e2ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83668f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aca8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CriticLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            critic:Critic, # The critic to optimize\n",
    "            # The loss function to use\n",
    "            loss:nn.Module=nn.MSELoss,\n",
    "            # The discount factor of `q`. Typically does not need to be changed,\n",
    "            # and determines the importants of earlier state qs verses later state qs\n",
    "            discount:float=0.99,\n",
    "            # If the environment has `nsteps>1`, it is recommended to change this\n",
    "            # param to reflect that so the reward estimates are more accurate.\n",
    "            nsteps:int=1\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.critic = critic\n",
    "        self.loss = loss()\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.critic.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "\n",
    "                batch.to(self.device)\n",
    "\n",
    "                # traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "            self.critic.zero_grad()\n",
    "            pred = self.critic(batch.state[m])\n",
    "            yield {'loss':self.loss(pred,batch.next_advantage[m])}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActorOptAndLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            actor:Actor, # The actor to optimize\n",
    "            max_kl:float=0.01\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actor = actor\n",
    "        self.device = None\n",
    "        self.max_kl = max_kl\n",
    "        self.counter = 0\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "                batch.to(self.device)\n",
    "                traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            \n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "            dist = self.actor(batch.state[m])\n",
    "            old_log_prob = dist.log_prob(batch.action[m]).detach()\n",
    "\n",
    "            loss_fn = partial(\n",
    "                actor_prob_loss,\n",
    "                s=batch.state[m],\n",
    "                a=batch.action[m],\n",
    "                r=traj_adv_v[m],\n",
    "                actor=self.actor,\n",
    "                old_log_prob=old_log_prob\n",
    "            )\n",
    "            self.counter += 1\n",
    "            # Calculate gradient backprop on initial loss function.\n",
    "            # Since the `actor` has not been updated yet, then loss is \n",
    "            # basically just going to be the `-traj_adv_v.mean()`.\n",
    "            loss = loss_fn(None)\n",
    "            loss_grad = auto_flat(loss,self.actor.parameters()).data\n",
    "            assert loss_grad.sum()!=0\n",
    " \n",
    "            forward_pass_fn = partial(\n",
    "                forward_pass,\n",
    "                s=batch.state[m],\n",
    "                actor=self.actor\n",
    "            )\n",
    "            # -loss_grad will be the `b` variable. Out goal is to find the gradient\n",
    "            # update direction that gets the output of `forward_pass_fn` to\n",
    "            # have an orthogonal step size to hit that loss_grad.\n",
    "            # The step direction (d) is going to be constrained by the f``KLdiv.\n",
    "            d = conjugate_gradients(forward_pass_fn,-loss_grad,10)\n",
    "\n",
    "            shs = 0.5 * (d * forward_pass_fn(d)).sum(0,keepdim=True)\n",
    "            lm = torch.sqrt(shs/self.max_kl)\n",
    "            full_step = d/lm[0]\n",
    "            neggdotstepdir = (-loss_grad * d).sum(0, keepdim=True)\n",
    "\n",
    "            prev_params = get_flat_params_from(self.actor)\n",
    "            success,params = backtrack_line_search(prev_params,full_step,loss_fn,neggdotstepdir/lm[0])\n",
    "            if success:\n",
    "                set_flat_params_to(self.actor,params)\n",
    "\n",
    "            yield {'loss':loss}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50899950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TRPOLearner(\n",
    "    # The actor model to use\n",
    "    actor:Actor,\n",
    "    # The critic model to use\n",
    "    critic:Critic,\n",
    "    # A list of dls, where index=0 is the training dl.\n",
    "    dls:List[DataPipeOrDataLoader],\n",
    "    # Optional logger bases to log training/validation data to.\n",
    "    logger_bases:Optional[List[LoggerBase]]=None,\n",
    "    # The learning rate for the actor. Expected to learn slower than the critic\n",
    "    actor_lr:float=1e-3,\n",
    "    # The optimizer for the actor\n",
    "    actor_opt:torch.optim.Optimizer=Adam,\n",
    "    # The learning rate for the critic. Expected to learn faster than the actor\n",
    "    critic_lr:float=1e-2,\n",
    "    # The optimizer for the critic\n",
    "    # Note that weight decay doesnt seem to be great for \n",
    "    # Pendulum, so we use regular Adam, which has the decay rate\n",
    "    # set to 0. (Lillicrap et al., 2016) would instead use AdamW\n",
    "    critic_opt:torch.optim.Optimizer=Adam,\n",
    "    # Reference: GymStepper docs\n",
    "    nsteps:int=1,\n",
    "    # The device for the entire pipeline to use. Will move the agent, dls, \n",
    "    # and learner to that device.\n",
    "    device:torch.device=None,\n",
    "    # Number of batches per epoch\n",
    "    batches:int=None,\n",
    "    # Any augmentations to the learner\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None,\n",
    "    # Debug mode will output device moves\n",
    "    debug:bool=False\n",
    ") -> LearnerHead:\n",
    "    warn(\"TRPO only kind of converges. There is a likely a bug, however I am unable to identify until after PPO implimentation\")\n",
    "\n",
    "    learner = LearnerBase(actor,dls,batches=batches)\n",
    "    learner = LoggerBasePassThrough(learner,logger_bases)\n",
    "    learner = BatchCollector(learner,batch_on_pipe=LearnerBase)\n",
    "    learner = EpocherCollector(learner)\n",
    "    for logger_base in L(logger_bases): learner = logger_base.connect_source_datapipe(learner)\n",
    "    if logger_bases: \n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner)\n",
    "    learner = StepBatcher(learner)\n",
    "    learner = CriticLossProcessor(learner,critic=critic)\n",
    "    learner = LossCollector(learner,header='critic-loss')\n",
    "    learner = BasicOptStepper(learner,critic,critic_lr,opt=critic_opt,filter=True,do_zero_grad=False)\n",
    "    learner = ActorOptAndLossProcessor(learner,actor)\n",
    "    learner = LossCollector(learner,header='actor-loss',filter=True)\n",
    "    learner = LearnerHead(learner)\n",
    "    \n",
    "    learner = apply_dp_augmentation_fns(learner,dp_augmentation_fns)\n",
    "    pipe2device(learner,device,debug=debug)\n",
    "    for dl in dls: pipe2device(dl.datapipe,device,debug=debug)\n",
    "    \n",
    "    return learner\n",
    "\n",
    "TRPOLearner.__doc__=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14367a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "# env='HalfCheetahBulletEnv-v0'\n",
    "\n",
    "# Setup Logger\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "actor = Actor(3,1)\n",
    "critic = Critic(3)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = TRPOAgent(actor,[logger_base],clip_min=-2,clip_max=2)\n",
    "\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    (AdvantageGymTransformBlock(agent=agent,critic=critic,bs=200,max_steps=200,gamma=0.95,discount=0.9)), \n",
    "    (AdvantageGymTransformBlock(agent=agent,n=400,include_images=True,seed=0,critic=critic,bs=200,gamma=0.95,discount=0.9,max_steps=200),VSCodeTransformBlock())\n",
    ")\n",
    "dls = L(block.dataloaders(['Pendulum-v1']*1))\n",
    "# Setup the Learner\n",
    "learner = TRPOLearner(actor,critic,dls,logger_bases=[logger_base],\n",
    "                      batches=10,critic_lr=0.01)\n",
    "# learner.fit(1)\n",
    "learner.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c87913",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afa0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
