{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import Callable\n",
    "# Third party libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse,replace_dp\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.torch_core import *\n",
    "from fastrl.layers import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.envs.gym import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "> Trust Region Policy Optimization via online-learning for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5b8f6",
   "metadata": {},
   "source": [
    "[(Schulman et al., 2017) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62d5ed",
   "metadata": {},
   "source": [
    "## Memory\n",
    "> Policy gradient online models use short term trajectory samples instead of\n",
    "ER / iid memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageBuffer(dp.iter.IterDataPipe):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            # Will accumulate up to `bs` or when the episode has terminated.\n",
    "            bs=1,\n",
    "            # If the `self.device` is not cpu, and `store_as_cpu=True`, then\n",
    "            # calls to `sample()` will dynamically move them to `self.device`, and\n",
    "            # next `sample()` will move them back to cpu before producing new samples.\n",
    "            # This can be slower, but can save vram.\n",
    "            # If `store_as_cpu=False`, then samples stay on `self.device`\n",
    "            #\n",
    "            # If being run with n_workers>0, shared_memory, and fork, this MUST be true. This is needed because\n",
    "            # otherwise the tensors in the memory will remain shared with the tensors created in the \n",
    "            # dataloader.\n",
    "            store_as_cpu:bool=True\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.bs = bs\n",
    "        self.store_as_cpu = store_as_cpu\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({k:v if k!='memory' else f'{len(self)} elements' for k,v in self.__dict__.items()})\n",
    "\n",
    "    def __len__(self): return self._sz_tracker\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for step in self.source_datapipe:\n",
    "            print('GAE step')\n",
    "            # if self.debug: print('Adding to advantage buffer: ',b)\n",
    "            \n",
    "            # if not issubclass(b.__class__,(StepType,list,tuple)):\n",
    "            #     raise Exception(f'Expected typing.NamedTuple,list,tuple object got {type(step)}\\n{step}')\n",
    "            \n",
    "            # if issubclass(b.__class__,StepType):   self.add(b)\n",
    "            # elif issubclass(b.__class__,(list,tuple)): \n",
    "            #     for step in b: self.add(step)\n",
    "            # else:\n",
    "            #     raise Exception(f'This should not have occured: {self.__dict__}')\n",
    "        \n",
    "            # if self._sz_tracker<self.bs: continue\n",
    "            yield step \n",
    "\n",
    "    @classmethod\n",
    "    def insert_dp(cls,old_dp=GymStepper) -> Callable[[DataPipe],DataPipe]:\n",
    "        def _insert_dp(pipe):\n",
    "            v = replace_dp(\n",
    "                traverse(pipe,only_datapipe=True),\n",
    "                find_dp(traverse(pipe,only_datapipe=True),old_dp),\n",
    "                cls(find_dp(traverse(pipe,only_datapipe=True),old_dp))\n",
    "            )\n",
    "            return list(v.values())[0][0]\n",
    "        return _insert_dp\n",
    "\n",
    "add_docs(\n",
    "AdvantageBuffer,\n",
    "\"\"\"Samples entire trajectories instead of individual time steps.\"\"\",\n",
    "to=torch.Tensor.to.__doc__\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_pipe = GymTransformBlock(\n",
    "    agent=None,seed=0,\n",
    "    dp_augmentation_fns=[AdvantageBuffer.insert_dp()]\n",
    ")(['Pendulum-v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchdata/datapipes/iter/util/header.py:60: UserWarning: The length of this HeaderIterDataPipe is inferred to be equal to its limit.The actual value may be smaller if the actual length of source_datapipe is smaller than the limit.\n",
      "  \"The length of this HeaderIterDataPipe is inferred to be equal to its limit.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n",
      "GAE step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[SimpleStep(state=tensor([ 0.6520,  0.7582, -0.4604]), action=tensor([0.5478]), next_state=tensor([0.6448, 0.7644, 0.1904]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.7621), total_reward=tensor(-0.7621), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(1), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.6448, 0.7644, 0.1904]), action=tensor([-0.9209]), next_state=tensor([0.6205, 0.7842, 0.6256]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.7615), total_reward=tensor(-1.5236), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(2), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.6205, 0.7842, 0.6256]), action=tensor([-1.8361]), next_state=tensor([0.5831, 0.8124, 0.9383]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.8549), total_reward=tensor(-2.3785), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(3), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.5831, 0.8124, 0.9383]), action=tensor([-1.9339]), next_state=tensor([0.5309, 0.8474, 1.2575]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.9910), total_reward=tensor(-3.3695), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(4), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.5309, 0.8474, 1.2575]), action=tensor([1.2531]), next_state=tensor([0.4400, 0.8980, 2.0810]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.1821), total_reward=tensor(-4.5516), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(5), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.4400, 0.8980, 2.0810]), action=tensor([1.6510]), next_state=tensor([0.3008, 0.9537, 3.0022]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.6795), total_reward=tensor(-6.2310), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.3008, 0.9537, 3.0022]), action=tensor([0.4265]), next_state=tensor([0.1162, 0.9932, 3.7814]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.5025), total_reward=tensor(-8.7335), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.1162, 0.9932, 3.7814]), action=tensor([0.9180]), next_state=tensor([-0.1165,  0.9932,  4.6641]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.5460), total_reward=tensor(-12.2795), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.1165,  0.9932,  4.6641]), action=tensor([0.1745]), next_state=tensor([-0.3788,  0.9255,  5.4351]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.0233), total_reward=tensor(-17.3028), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3788,  0.9255,  5.4351]), action=tensor([1.7403]), next_state=tensor([-0.6504,  0.7596,  6.3903]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.7961), total_reward=tensor(-24.0989), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.6504,  0.7596,  6.3903]), action=tensor([1.2634]), next_state=tensor([-0.8750,  0.4840,  7.1495]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.2783), total_reward=tensor(-33.3772), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(11), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8750,  0.4840,  7.1495]), action=tensor([-1.9890]), next_state=tensor([-0.9896,  0.1441,  7.2142]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.0657), total_reward=tensor(-45.4429), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(12), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9896,  0.1441,  7.2142]), action=tensor([1.4296]), next_state=tensor([-0.9731, -0.2302,  7.5367]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-14.1887), total_reward=tensor(-59.6315), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(13), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9731, -0.2302,  7.5367]), action=tensor([-1.8657]), next_state=tensor([-0.8329, -0.5534,  7.0842]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-14.1478), total_reward=tensor(-73.7793), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(14), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8329, -0.5534,  7.0842]), action=tensor([0.9186]), next_state=tensor([-0.6004, -0.7997,  6.8069]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-11.5480), total_reward=tensor(-85.3273), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(15), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.6004, -0.7997,  6.8069]), action=tensor([-1.2974]), next_state=tensor([-0.3366, -0.9416,  6.0125]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.5403), total_reward=tensor(-94.8675), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(16), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3366, -0.9416,  6.0125]), action=tensor([1.4527]), next_state=tensor([-0.0671, -0.9977,  5.5242]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.2811), total_reward=tensor(-102.1486), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(17), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.0671, -0.9977,  5.5242]), action=tensor([0.1658]), next_state=tensor([ 0.1720, -0.9851,  4.8007]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.7345), total_reward=tensor(-107.8831), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(18), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.1720, -0.9851,  4.8007]), action=tensor([-0.8012]), next_state=tensor([ 0.3616, -0.9323,  3.9418]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.2595), total_reward=tensor(-112.1426), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(19), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.3616, -0.9323,  3.9418]), action=tensor([-0.3093]), next_state=tensor([ 0.5054, -0.8629,  3.1961]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.9958), total_reward=tensor(-115.1384), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(20), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5054, -0.8629,  3.1961]), action=tensor([-1.8867]), next_state=tensor([ 0.5997, -0.8002,  2.2659]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.1088), total_reward=tensor(-117.2471), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(21), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5997, -0.8002,  2.2659]), action=tensor([-1.5029]), next_state=tensor([ 0.6557, -0.7550,  1.4403]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.3763), total_reward=tensor(-118.6235), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(22), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.6557, -0.7550,  1.4403]), action=tensor([0.6825]), next_state=tensor([ 0.6918, -0.7221,  0.9764]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.9401), total_reward=tensor(-119.5636), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(23), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.6918, -0.7221,  0.9764]), action=tensor([0.5888]), next_state=tensor([ 0.7104, -0.7038,  0.5231]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.7467), total_reward=tensor(-120.3103), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(24), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.7104, -0.7038,  0.5231]), action=tensor([0.4615]), next_state=tensor([ 0.7127, -0.7015,  0.0645]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.6371), total_reward=tensor(-120.9474), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(25), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.7127, -0.7015,  0.0645]), action=tensor([-0.4653]), next_state=tensor([ 0.6938, -0.7202, -0.5314]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.6051), total_reward=tensor(-121.5526), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(26), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.6938, -0.7202, -0.5314]), action=tensor([1.9888]), next_state=tensor([ 0.6654, -0.7465, -0.7732]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.6787), total_reward=tensor(-122.2312), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(27), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.6654, -0.7465, -0.7732]), action=tensor([1.9233]), next_state=tensor([ 0.6256, -0.7802, -1.0445]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.7736), total_reward=tensor(-123.0049), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(28), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.6256, -0.7802, -1.0445]), action=tensor([0.7422]), next_state=tensor([ 0.5646, -0.8254, -1.5183]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.9106), total_reward=tensor(-123.9155), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(29), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5646, -0.8254, -1.5183]), action=tensor([0.6018]), next_state=tensor([ 0.4773, -0.8787, -2.0471]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.1735), total_reward=tensor(-125.0889), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(30), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.4773, -0.8787, -2.0471]), action=tensor([0.7538]), next_state=tensor([ 0.3597, -0.9331, -2.5931]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.5714), total_reward=tensor(-126.6603), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(31), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.3597, -0.9331, -2.5931]), action=tensor([-0.4443]), next_state=tensor([ 0.1986, -0.9801, -3.3595]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.1195), total_reward=tensor(-128.7798), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(32), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.1986, -0.9801, -3.3595]), action=tensor([-1.4596]), next_state=tensor([-0.0157, -0.9999, -4.3135]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.0100), total_reward=tensor(-131.7897), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(33), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.0157, -0.9999, -4.3135]), action=tensor([0.8860]), next_state=tensor([-0.2592, -0.9658, -4.9305]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.3784), total_reward=tensor(-136.1682), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(34), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2592, -0.9658, -4.9305]), action=tensor([0.1014]), next_state=tensor([-0.5178, -0.8555, -5.6397]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.7911), total_reward=tensor(-141.9593), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(35), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5178, -0.8555, -5.6397]), action=tensor([-0.7590]), next_state=tensor([-0.7604, -0.6494, -6.3952]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.6545), total_reward=tensor(-149.6138), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(36), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7604, -0.6494, -6.3952]), action=tensor([-0.0567]), next_state=tensor([-0.9351, -0.3544, -6.8907]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-10.0180), total_reward=tensor(-159.6317), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(37), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9351, -0.3544, -6.8907]), action=tensor([1.5580]), next_state=tensor([-0.9999, -0.0161, -6.9228]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.4753), total_reward=tensor(-172.1070), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(38), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9999, -0.0161, -6.9228]), action=tensor([1.7362]), next_state=tensor([-0.9500,  0.3123, -6.6745]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-14.5641), total_reward=tensor(-186.6711), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(39), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9500,  0.3123, -6.6745]), action=tensor([-0.5688]), next_state=tensor([-0.7998,  0.6003, -6.5256]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.4302), total_reward=tensor(-199.1013), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(40), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7998,  0.6003, -6.5256]), action=tensor([0.2861]), next_state=tensor([-0.5853,  0.8108, -6.0325]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-10.4970), total_reward=tensor(-209.5983), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(41), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5853,  0.8108, -6.0325]), action=tensor([-0.7125]), next_state=tensor([-0.3417,  0.9398, -5.5313]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-8.4624), total_reward=tensor(-218.0608), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(42), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3417,  0.9398, -5.5313]), action=tensor([0.3772]), next_state=tensor([-0.1100,  0.9939, -4.7698]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.7442), total_reward=tensor(-224.8050), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(43), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.1100,  0.9939, -4.7698]), action=tensor([-0.6484]), next_state=tensor([ 0.0957,  0.9954, -4.1216]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.1014), total_reward=tensor(-229.9064), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(44), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0957,  0.9954, -4.1216]), action=tensor([-0.4335]), next_state=tensor([ 0.2647,  0.9643, -3.4401]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.8745), total_reward=tensor(-233.7809), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(45), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.2647,  0.9643, -3.4401]), action=tensor([1.5611]), next_state=tensor([ 0.3820,  0.9242, -2.4827]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.8836), total_reward=tensor(-236.6645), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(46), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.3820,  0.9242, -2.4827]), action=tensor([-1.0914]), next_state=tensor([ 0.4703,  0.8825, -1.9533]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.0072), total_reward=tensor(-238.6716), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(47), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.4703,  0.8825, -1.9533]), action=tensor([0.4927]), next_state=tensor([ 0.5231,  0.8523, -1.2175]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.5507), total_reward=tensor(-240.2223), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(48), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5231,  0.8523, -1.2175]), action=tensor([-1.6639]), next_state=tensor([ 0.5579,  0.8299, -0.8279]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.1920), total_reward=tensor(-241.4143), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(49), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5579,  0.8299, -0.8279]), action=tensor([1.3306]), next_state=tensor([ 0.5582,  0.8297, -0.0059]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.0285), total_reward=tensor(-242.4428), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(50), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.5582,  0.8297, -0.0059]), action=tensor([1.1484]), next_state=tensor([0.5250, 0.8511, 0.7886]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-0.9590), total_reward=tensor(-243.4018), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(51), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.5250, 0.8511, 0.7886]), action=tensor([-1.0425]), next_state=tensor([0.4700, 0.8827, 1.2706]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.0997), total_reward=tensor(-244.5014), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(52), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.4700, 0.8827, 1.2706]), action=tensor([1.5059]), next_state=tensor([0.3721, 0.9282, 2.1585]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.3335), total_reward=tensor(-245.8349), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(53), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.3721, 0.9282, 2.1585]), action=tensor([-1.7657]), next_state=tensor([0.2492, 0.9685, 2.5898]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-1.8839), total_reward=tensor(-247.7188), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(54), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.2492, 0.9685, 2.5898]), action=tensor([-0.6555]), next_state=tensor([0.0908, 0.9959, 3.2178]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.4108), total_reward=tensor(-250.1296), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(55), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([0.0908, 0.9959, 3.2178]), action=tensor([-1.3989]), next_state=tensor([-0.0966,  0.9953,  3.7548]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.2273), total_reward=tensor(-253.3569), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(56), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.0966,  0.9953,  3.7548]), action=tensor([-0.1986]), next_state=tensor([-0.3149,  0.9491,  4.4715]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.1908), total_reward=tensor(-257.5477), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(57), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3149,  0.9491,  4.4715]), action=tensor([1.1853]), next_state=tensor([-0.5551,  0.8318,  5.3612]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.5774), total_reward=tensor(-263.1251), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(58), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5551,  0.8318,  5.3612]), action=tensor([-1.0774]), next_state=tensor([-0.7705,  0.6375,  5.8234]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.5376), total_reward=tensor(-270.6627), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(59), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7705,  0.6375,  5.8234]), action=tensor([-1.7919]), next_state=tensor([-0.9251,  0.3798,  6.0327]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.3989), total_reward=tensor(-280.0616), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(60), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9251,  0.3798,  6.0327]), action=tensor([-0.3818]), next_state=tensor([-0.9971,  0.0765,  6.2603]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-11.2132), total_reward=tensor(-291.2747), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(61), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9971,  0.0765,  6.2603]), action=tensor([-1.2059]), next_state=tensor([-0.9736, -0.2283,  6.1367]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-13.3151), total_reward=tensor(-304.5898), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(62), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9736, -0.2283,  6.1367]), action=tensor([-1.6370]), next_state=tensor([-0.8697, -0.4937,  5.7200]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.2443), total_reward=tensor(-316.8341), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(63), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8697, -0.4937,  5.7200]), action=tensor([0.3213]), next_state=tensor([-0.7065, -0.7077,  5.3979]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-10.1641), total_reward=tensor(-326.9983), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(64), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7065, -0.7077,  5.3979]), action=tensor([-0.8052]), next_state=tensor([-0.5204, -0.8539,  4.7464]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-8.4624), total_reward=tensor(-335.4607), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(65), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5204, -0.8539,  4.7464]), action=tensor([0.6880]), next_state=tensor([-0.3305, -0.9438,  4.2092]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.7396), total_reward=tensor(-342.2003), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(66), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3305, -0.9438,  4.2092]), action=tensor([-1.2019]), next_state=tensor([-0.1699, -0.9855,  3.3210]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.4122), total_reward=tensor(-347.6124), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(67), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.1699, -0.9855,  3.3210]), action=tensor([1.7685]), next_state=tensor([-0.0284, -0.9996,  2.8472]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.1391), total_reward=tensor(-351.7516), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(68), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.0284, -0.9996,  2.8472]), action=tensor([-0.5396]), next_state=tensor([ 0.0723, -0.9974,  2.0166]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.3684), total_reward=tensor(-355.1200), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(69), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0723, -0.9974,  2.0166]), action=tensor([-1.5780]), next_state=tensor([ 0.1237, -0.9923,  1.0318]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.6543), total_reward=tensor(-357.7743), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(70), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.1237, -0.9923,  1.0318]), action=tensor([0.5164]), next_state=tensor([ 0.1418, -0.9899,  0.3650]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.1999), total_reward=tensor(-359.9742), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(71), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.1418, -0.9899,  0.3650]), action=tensor([1.7086]), next_state=tensor([ 0.1358, -0.9907, -0.1211]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.0570), total_reward=tensor(-362.0312), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(72), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.1358, -0.9907, -0.1211]), action=tensor([-0.2385]), next_state=tensor([ 0.0911, -0.9958, -0.8999]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.0596), total_reward=tensor(-364.0908), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(73), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0911, -0.9958, -0.8999]), action=tensor([1.8184]), next_state=tensor([ 0.0225, -0.9997, -1.3740]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.2735), total_reward=tensor(-366.3643), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(74), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0225, -0.9997, -1.3740]), action=tensor([-0.0004]), next_state=tensor([-0.0836, -0.9965, -2.1239]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-2.5860), total_reward=tensor(-368.9503), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(75), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.0836, -0.9965, -2.1239]), action=tensor([-0.2991]), next_state=tensor([-0.2275, -0.9738, -2.9162]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.1885), total_reward=tensor(-372.1389), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(76), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2275, -0.9738, -2.9162]), action=tensor([0.4809]), next_state=tensor([-0.3970, -0.9178, -3.5744]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.0917), total_reward=tensor(-376.2306), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(77), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3970, -0.9178, -3.5744]), action=tensor([1.9804]), next_state=tensor([-0.5700, -0.8216, -3.9657]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.1980), total_reward=tensor(-381.4286), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(78), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5700, -0.8216, -3.9657]), action=tensor([1.7958]), next_state=tensor([-0.7326, -0.6807, -4.3125]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.3165), total_reward=tensor(-387.7451), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(79), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7326, -0.6807, -4.3125]), action=tensor([-0.1598]), next_state=tensor([-0.8745, -0.4850, -4.8470]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.5859), total_reward=tensor(-395.3311), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(80), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8745, -0.4850, -4.8470]), action=tensor([1.0309]), next_state=tensor([-0.9680, -0.2508, -5.0561]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.2951), total_reward=tensor(-404.6262), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(81), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9680, -0.2508, -5.0561]), action=tensor([-0.0103]), next_state=tensor([-1.0000,  0.0088, -5.2457]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-10.8974), total_reward=tensor(-415.5236), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(82), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-1.0000,  0.0088, -5.2457]), action=tensor([0.1172]), next_state=tensor([-0.9638,  0.2666, -5.2216]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.5663), total_reward=tensor(-428.0899), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(83), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9638,  0.2666, -5.2216]), action=tensor([1.1431]), next_state=tensor([-0.8716,  0.4902, -4.8502]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-10.9747), total_reward=tensor(-439.0646), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(84), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8716,  0.4902, -4.8502]), action=tensor([-0.3414]), next_state=tensor([-0.7391,  0.6736, -4.5337]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.2654), total_reward=tensor(-448.3300), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(85), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7391,  0.6736, -4.5337]), action=tensor([0.9379]), next_state=tensor([-0.5951,  0.8037, -3.8878]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.8285), total_reward=tensor(-456.1585), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(86), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.5951,  0.8037, -3.8878]), action=tensor([0.8446]), next_state=tensor([-0.4613,  0.8873, -3.1584]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.3882), total_reward=tensor(-462.5466), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(87), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.4613,  0.8873, -3.1584]), action=tensor([1.7282]), next_state=tensor([-0.3595,  0.9331, -2.2337]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-5.2040), total_reward=tensor(-467.7506), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(88), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3595,  0.9331, -2.2337]), action=tensor([-1.5403]), next_state=tensor([-0.2759,  0.9612, -1.7649]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.2593), total_reward=tensor(-472.0099), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(89), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2759,  0.9612, -1.7649]), action=tensor([0.9161]), next_state=tensor([-0.2320,  0.9727, -0.9066]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.7360), total_reward=tensor(-475.7459), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(90), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2320,  0.9727, -0.9066]), action=tensor([1.7097]), next_state=tensor([-0.2359,  0.9718,  0.0794]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.3431), total_reward=tensor(-479.0889), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(91), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2359,  0.9718,  0.0794]), action=tensor([1.8717]), next_state=tensor([-0.2884,  0.9575,  1.0890]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.2764), total_reward=tensor(-482.3654), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(92), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.2884,  0.9575,  1.0890]), action=tensor([-1.9412]), next_state=tensor([-0.3601,  0.9329,  1.5159]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.5946), total_reward=tensor(-485.9600), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(93), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.3601,  0.9329,  1.5159]), action=tensor([1.4546]), next_state=tensor([-0.4707,  0.8823,  2.4338]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-3.9924), total_reward=tensor(-489.9524), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(94), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.4707,  0.8823,  2.4338]), action=tensor([1.9248]), next_state=tensor([-0.6126,  0.7904,  3.3842]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-4.8433), total_reward=tensor(-494.7957), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(95), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.6126,  0.7904,  3.3842]), action=tensor([1.8288]), next_state=tensor([-0.7655,  0.6434,  4.2514]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-6.1220), total_reward=tensor(-500.9176), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(96), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.7655,  0.6434,  4.2514]), action=tensor([-1.4049]), next_state=tensor([-0.8903,  0.4554,  4.5232]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-7.7760), total_reward=tensor(-508.6937), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(97), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.8903,  0.4554,  4.5232]), action=tensor([1.8905]), next_state=tensor([-0.9769,  0.2137,  5.1483]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-9.1721), total_reward=tensor(-517.8658), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(98), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9769,  0.2137,  5.1483]), action=tensor([1.5597]), next_state=tensor([-0.9981, -0.0617,  5.5425]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-11.2158), total_reward=tensor(-529.0815), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(99), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([-0.9981, -0.0617,  5.5425]), action=tensor([1.2895]), next_state=tensor([-0.9407, -0.3394,  5.6896]), terminated=tensor(False), truncated=tensor(False), reward=tensor(-12.5589), total_reward=tensor(-541.6404), env_id=tensor(140197664970960), proc_id=tensor(7259), step_n=tensor(100), episode_n=tensor(1), image=tensor([0.]))]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gym_pipe.header(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class A:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
