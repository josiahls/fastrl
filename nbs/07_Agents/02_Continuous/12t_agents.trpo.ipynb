{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import *\n",
    "from typing_extensions import Literal\n",
    "import typing \n",
    "# Third party libs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import *\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse,replace_dp\n",
    "from fastcore.all import test_eq,test_ne\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.torch_core import *\n",
    "from fastrl.layers import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.envs.gym import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "> Trust Region Policy Optimization via online-learning for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5b8f6",
   "metadata": {},
   "source": [
    "[(Schulman et al., 2015) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477).\n",
    "\n",
    "Directly based on [`ikostrikov`'s implimentation](https://github.com/ikostrikov/pytorch-trpo) and\n",
    "coda / explainations in [Shewchuk Cs.Cmu.Edu, 2022, Accessed 19 Nov 2022.](cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c2cba",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageStep(typing.NamedTuple):\n",
    "    state:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    action:      torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_state:  torch.FloatTensor=torch.FloatTensor([0])\n",
    "    terminated:  torch.BoolTensor=torch.BoolTensor([1])\n",
    "    truncated:   torch.BoolTensor=torch.BoolTensor([1])\n",
    "    reward:      torch.FloatTensor=torch.LongTensor([0])\n",
    "    total_reward:torch.FloatTensor=torch.FloatTensor([0])\n",
    "    advantage:   torch.FloatTensor=torch.FloatTensor([0])\n",
    "    env_id:      torch.LongTensor=torch.LongTensor([0])\n",
    "    proc_id:     torch.LongTensor=torch.LongTensor([0])\n",
    "    step_n:      torch.LongTensor=torch.LongTensor([0])\n",
    "    episode_n:   torch.LongTensor=torch.LongTensor([0])\n",
    "    image:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).clone() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def detach(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).detach() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def device(self,device='cpu'):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(device=device) for fld in self.__class__._fields}\n",
    "        )\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(*args,**kwargs) for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls,seed=None,**flds):\n",
    "        _flds,_annos = cls._fields,cls.__annotations__\n",
    "\n",
    "        def _random_annos(anno):\n",
    "            t = anno(1)\n",
    "            if anno==torch.BoolTensor: t.random_(2) \n",
    "            else:                      t.random_(100)\n",
    "            return t\n",
    "\n",
    "        return cls(\n",
    "            *(flds.get(\n",
    "                f,_random_annos(_annos[f])\n",
    "            ) for f in _flds)\n",
    "        )\n",
    "\n",
    "add_namedtuple_doc(\n",
    "AdvantageStep,\n",
    "\"\"\"Represents a single step in an environment similar to `SimpleStep` however has\n",
    "an addition field called `advantage`.\"\"\",\n",
    "advantage=\"\"\"Generally characterized as $A(s,a) = Q(s,a) - V(s)$\"\"\",\n",
    "**{f:getattr(SimpleStep,f).__doc__ for f in SimpleStep._fields}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f777630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdvantageStep\n",
       "\n",
       ">      AdvantageStep (state:torch.FloatTensor=tensor([0.]),\n",
       ">                     action:torch.FloatTensor=tensor([0.]),\n",
       ">                     next_state:torch.FloatTensor=tensor([0.]),\n",
       ">                     terminated:torch.BoolTensor=tensor([True]),\n",
       ">                     truncated:torch.BoolTensor=tensor([True]),\n",
       ">                     reward:torch.FloatTensor=tensor([0]),\n",
       ">                     total_reward:torch.FloatTensor=tensor([0.]),\n",
       ">                     advantage:torch.FloatTensor=tensor([0.]),\n",
       ">                     env_id:torch.LongTensor=tensor([0]),\n",
       ">                     proc_id:torch.LongTensor=tensor([0]),\n",
       ">                     step_n:torch.LongTensor=tensor([0]),\n",
       ">                     episode_n:torch.LongTensor=tensor([0]),\n",
       ">                     image:torch.FloatTensor=tensor([0.]))\n",
       "\n",
       "Represents a single step in an environment similar to `SimpleStep` however has\n",
       "an addition field called `advantage`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       " - **state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the initial state of the environment and the previous state.\n",
       " - **action**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The action that was taken to transition from `state` to `next_state`\n",
       " - **next_state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the next state, and the last state in the environment\n",
       " - **terminated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment such as reaching a goal or 'living long enough' as \n",
       "                    described by the MDP.\n",
       "                    Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155\n",
       " - **truncated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment that can be seen as an out of bounds condition either\n",
       "                   literally going out of bounds, breaking rules, or exceeding the timelimit allowed by the MDP.\n",
       "                   Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155'\n",
       " - **reward**:`<class 'torch.FloatTensor'>`  = `tensor([0])`The single reward for this step.\n",
       " - **total_reward**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The total accumulated reward for this episode up to this step.\n",
       " - **advantage**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Generally characterized as $A(s,a) = Q(s,a) - V(s)$\n",
       " - **env_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The environment this step came from (useful for debugging)\n",
       " - **proc_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The process this step came from (useful for debugging)\n",
       " - **step_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The step number in a given episode.\n",
       " - **episode_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The episode this environment is currently running through.\n",
       " - **image**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Intended for display and logging only. If the intention is to use images for training an\n",
       "               agent, then use a env wrapper instead."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdvantageStep\n",
       "\n",
       ">      AdvantageStep (state:torch.FloatTensor=tensor([0.]),\n",
       ">                     action:torch.FloatTensor=tensor([0.]),\n",
       ">                     next_state:torch.FloatTensor=tensor([0.]),\n",
       ">                     terminated:torch.BoolTensor=tensor([True]),\n",
       ">                     truncated:torch.BoolTensor=tensor([True]),\n",
       ">                     reward:torch.FloatTensor=tensor([0]),\n",
       ">                     total_reward:torch.FloatTensor=tensor([0.]),\n",
       ">                     advantage:torch.FloatTensor=tensor([0.]),\n",
       ">                     env_id:torch.LongTensor=tensor([0]),\n",
       ">                     proc_id:torch.LongTensor=tensor([0]),\n",
       ">                     step_n:torch.LongTensor=tensor([0]),\n",
       ">                     episode_n:torch.LongTensor=tensor([0]),\n",
       ">                     image:torch.FloatTensor=tensor([0.]))\n",
       "\n",
       "Represents a single step in an environment similar to `SimpleStep` however has\n",
       "an addition field called `advantage`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       " - **state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the initial state of the environment and the previous state.\n",
       " - **action**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The action that was taken to transition from `state` to `next_state`\n",
       " - **next_state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the next state, and the last state in the environment\n",
       " - **terminated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment such as reaching a goal or 'living long enough' as \n",
       "                    described by the MDP.\n",
       "                    Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155\n",
       " - **truncated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment that can be seen as an out of bounds condition either\n",
       "                   literally going out of bounds, breaking rules, or exceeding the timelimit allowed by the MDP.\n",
       "                   Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155'\n",
       " - **reward**:`<class 'torch.FloatTensor'>`  = `tensor([0])`The single reward for this step.\n",
       " - **total_reward**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The total accumulated reward for this episode up to this step.\n",
       " - **advantage**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Generally characterized as $A(s,a) = Q(s,a) - V(s)$\n",
       " - **env_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The environment this step came from (useful for debugging)\n",
       " - **proc_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The process this step came from (useful for debugging)\n",
       " - **step_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The step number in a given episode.\n",
       " - **episode_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The episode this environment is currently running through.\n",
       " - **image**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Intended for display and logging only. If the intention is to use images for training an\n",
       "               agent, then use a env wrapper instead."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdvantageStep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62d5ed",
   "metadata": {},
   "source": [
    "## Memory\n",
    "> Policy gradient online models use short term trajectory samples instead of\n",
    "ER / iid memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@torch.jit.script\n",
    "def discounted_cumsum_(t:torch.Tensor,gamma:float,reverse:bool=False):\n",
    "    \"\"\"Performs a cumulative sum on `t` where `gamma` is applied for each index\n",
    "    >1.\"\"\"\n",
    "    if reverse:\n",
    "        # We do +2 because +1 is needed to avoid out of index t[idx], and +2 is needed\n",
    "        # to avoid out of index for t[idx+1].\n",
    "        for idx in range(t.size(0)-2,-1,-1):\n",
    "            t[idx] = t[idx] + t[idx+1] * gamma\n",
    "    else:\n",
    "        for idx in range(1,t.size(0)):\n",
    "            t[idx] = t[idx] + t[idx-1] * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d26899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/2\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "#     discounted_cumsum(torch.ones(500),0.99)\n",
    "# print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageBuffer(dp.iter.IterDataPipe):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "            # A datapipe that produces `StepType`s.\n",
    "            source_datapipe:DataPipe,\n",
    "            # A model that takes in a `state` and outputs a single value \n",
    "            # representing $V$, where as $Q$ is $V + reward$\n",
    "            critic:nn.Module,\n",
    "            # Will accumulate up to `bs` or when the episode has terminated.\n",
    "            bs=1000,\n",
    "            # The discount factor, otherwise known as $\\gamma$, is defined in \n",
    "            # (Shulman et al., 2016) as '... $\\gamma$ introduces bias into\n",
    "            # the policy gradient estimate...'.\n",
    "            discount:float=0.99,\n",
    "            # $\\lambda$ is unqiue to GAE and manages importance to values when \n",
    "            # they are in accurate is defined in (Shulman et al., 2016) as '... $\\lambda$ < 1\n",
    "            # introduces bias only when the value function is inaccurate....'.\n",
    "            gamma:float=0.99\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.bs = bs\n",
    "        self.critic = critic\n",
    "        self.device = None\n",
    "        self.discount = discount\n",
    "        self.gamma = gamma\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({k:v if k!='env_advantage_buffer' else f'{len(self)} elements' \n",
    "                    for k,v in self.__dict__.items()})\n",
    "\n",
    "    def __len__(self): return self._sz_tracker\n",
    "\n",
    "    def update_advantage_buffer(self,step:StepType) -> int:\n",
    "        if self.debug: \n",
    "            print('Adding to advantage buffer: ',step)\n",
    "        env_id = int(step.env_id.detach().cpu())\n",
    "        if env_id not in self.env_advantage_buffer: \n",
    "            self.env_advantage_buffer[env_id] = []\n",
    "        self.env_advantage_buffer[env_id].append(step)\n",
    "        return env_id\n",
    "        \n",
    "    def zip_steps(\n",
    "            self,\n",
    "            steps:List[StepType]\n",
    "        ) -> Tuple[torch.FloatTensor,torch.FloatTensor,torch.BoolTensor]:\n",
    "            step_subset = [(o.reward,o.state,o.truncated or o.terminated) for o in steps]\n",
    "            zipped_fields = zip(*step_subset)\n",
    "            return L(zipped_fields).map(torch.vstack)\n",
    "\n",
    "    def delta_calc(self,reward,v,v_next,done):\n",
    "        return reward + (self.gamma * v * done) - v_next\n",
    "\n",
    "    def __iter__(self) -> AdvantageStep:\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "        for step in self.source_datapipe:\n",
    "            env_id = self.update_advantage_buffer(step)\n",
    "            done = step.truncated or step.terminated\n",
    "            if done or len(self.env_advantage_buffer[env_id])>self.bs:\n",
    "                steps = self.env_advantage_buffer[env_id]\n",
    "                rewards,states,dones = self.zip_steps(steps)\n",
    "                # We vstack the final next_state so we have a complete picture\n",
    "                # of the state transitions and matching reward/done shapes.\n",
    "                values = self.critic(torch.vstack((states,steps[-1].next_state)))\n",
    "                delta = self.delta_calc(rewards,values[:-1],values[1:],dones)\n",
    "                discounted_cumsum_(delta,self.discount*self.gamma,reverse=True)\n",
    "\n",
    "                for _step,gae_advantage in zip(*(steps,delta)):\n",
    "                    yield AdvantageStep(\n",
    "                        advantage=gae_advantage,\n",
    "                        **{f:getattr(_step,f) for f in _step._fields}\n",
    "                    )\n",
    "\n",
    "    @classmethod\n",
    "    def insert_dp(cls,critic,old_dp=GymStepper) -> Callable[[DataPipe],DataPipe]:\n",
    "        def _insert_dp(pipe):\n",
    "            v = replace_dp(\n",
    "                traverse(pipe,only_datapipe=True),\n",
    "                find_dp(traverse(pipe,only_datapipe=True),old_dp),\n",
    "                cls(find_dp(traverse(pipe,only_datapipe=True),old_dp),critic=critic)\n",
    "            )\n",
    "            return list(v.values())[0][0]\n",
    "        return _insert_dp\n",
    "\n",
    "add_docs(\n",
    "AdvantageBuffer,\n",
    "\"\"\"Collects an entire episode, calculates the advantage for each step, then\n",
    "yields that episode's `AdvantageStep`s.\n",
    "\n",
    "This is described in the original paper `(Shulman et al., 2016) High-Dimensional \n",
    "Continuous Control Usin Generalized Advantage Estimation`.\n",
    "\n",
    "This algorithm is based on the concept of advantage:\n",
    "\n",
    "$A_{\\pi}(s,a) = Q_{\\pi}(s,a) - V_{\\pi}(s)$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 5 calculates it as:\n",
    "\n",
    "$\\hat{A}_{t}^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}^V$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 4 defines $\\delta$ as:\n",
    "\n",
    "$\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$\n",
    "\"\"\",\n",
    "to=torch.Tensor.to.__doc__,\n",
    "update_advantage_buffer=\"Adds `step` to `env_advantage_buffer` based on the environment id.\",\n",
    "zip_steps=\"\"\"Given `steps`, strip out the `Tuple[reward,state,truncated or terminated]` fields,\n",
    "and `torch.vstack` them.\"\"\",\n",
    "delta_calc=\"\"\"Calculates $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$ which \n",
    "is the advantage difference between state transitions.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.layers import Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(3,0)\n",
    "\n",
    "gym_pipe = GymTransformBlock(\n",
    "    agent=None,seed=0,\n",
    "    dp_augmentation_fns=[AdvantageBuffer.insert_dp(critic=critic)]\n",
    ")(['Pendulum-v1'])\n",
    "\n",
    "for chunk in gym_pipe.header(5):\n",
    "    for step in chunk:\n",
    "        test_eq(type(step),AdvantageStep)\n",
    "        assert step.advantage!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a48c46",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OptionalClampLinear(Module):\n",
    "    def __init__(self,num_inputs,state_dims,fix_variance:bool=False,\n",
    "                 clip_min=0.3,clip_max=10.0):\n",
    "        \"Linear layer or constant block used for std.\"\n",
    "        store_attr()\n",
    "        if not self.fix_variance: \n",
    "            self.fc=nn.Linear(self.num_inputs,self.state_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.fix_variance: \n",
    "            return torch.full((x.shape[0],self.state_dims),1.0)\n",
    "        else:                 \n",
    "            return torch.clamp(nn.Softplus()(self.fc(x)),self.clip_min,self.clip_max)\n",
    "\n",
    "# TODO(josiahls): This is probably a highly generic SimpleGMM tbh. Once we know this\n",
    "# works, we should just rename this to SimpleGMM\n",
    "class Actor(Module):\n",
    "    def __init__(            \n",
    "            self,\n",
    "            state_sz:int,   # The input dim of the state / flattened conv output\n",
    "            action_sz:int,  # The output dim of the actions\n",
    "            hidden:int=400, # Number of neurons connected between the 2 input/output layers\n",
    "            fix_variance:bool=False\n",
    "        ):\n",
    "        \"Single-component GMM parameterized by a fully connected layer with optional std layer.\"\n",
    "        store_attr()\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(state_sz, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, action_sz),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.std = OptionalClampLinear(state_sz,action_sz,fix_variance)\n",
    "        \n",
    "    def forward(self,x): return Independent(Normal(self.mu(x),self.std(x)),1)\n",
    "\n",
    "\n",
    "add_docs(\n",
    "Actor,\n",
    "\"\"\"Produces continuous outputs from mean of a Gaussian distribution.\"\"\",\n",
    "forward=\"Mean outputs from a parameterized Gaussian distribution.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f645774",
   "metadata": {},
   "source": [
    "The `Actor` is developed from the description found in `(Schulman et al., 2015)`: \n",
    "\n",
    "    ...we used a Gaussian distribution, where the covariance matrix was diagonal \n",
    "    and independent of the state. A neural network with several fully-connected (dense) \n",
    "    layers maps from the input features to the mean of a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1615, -0.0329]], grad_fn=<TanhBackward0>),\n",
       " tensor([[1.0072, 0.5305]], grad_fn=<SqrtBackward0>),\n",
       " tensor([-1.6457], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(4,2)\n",
    "dist = actor(torch.randn(1,4))\n",
    "dist.mean,dist.stddev,dist.log_prob(torch.randn(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81bdf",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe248ac",
   "metadata": {},
   "source": [
    "![](../../images/(Schulman%20et%20al.%2C%202017)%20%5BTRPO%5D%20Trust%20Region%20Policy%20Optimization%20Algorithm%201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c473768",
   "metadata": {},
   "source": [
    "We start with finding the direction of conjugate gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(\n",
    "    [[3.,2.],[2.,6.]]\n",
    ")\n",
    "b = torch.tensor([[2.],[-8.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d3067",
   "metadata": {},
   "source": [
    "Ref [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf), but\n",
    "$A$ is the gradients of the model, and $b$ (typically the bias) is the loss.\n",
    "\n",
    "The below function is pretty much example `B2` pg 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def conjugate_gradients(\n",
    "    # A function that takes the direction `d` and applies it to `A`.\n",
    "    # The simplest example of this found would be:\n",
    "    # `lambda d:A@d`\n",
    "    Ad_f:Callable[[torch.Tensor],torch.Tensor],  \n",
    "    # The bias or in TRPO's case the loss.\n",
    "    b:torch.Tensor, \n",
    "    # Number of steps to go for assuming we are not less than `residual_tol`.\n",
    "    nsteps:int, \n",
    "    # If the residual is less than this, then we have arrived at the local minimum.\n",
    "    # Note that (Shewchuk, 1994) they mention that this should be E^2 * rdotr_0\n",
    "    residual_tol=1e-10, \n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # The final direction to go in.\n",
    "    x = torch.zeros(b.size()).to(device)\n",
    "    # Would typically be b - Ax, however in TRPO's case this has already been \n",
    "    # done in the loss function.\n",
    "    r = b.clone()\n",
    "    # The first direction is the first residual.\n",
    "    d = b.clone()\n",
    "    rdotr = r.T @ r # \\sigma_{new} pg50\n",
    "    for i in range(nsteps):\n",
    "        _Ad = Ad_f(d) # _Ad is also considered `q`\n",
    "        # Determines the size / rate / step size of the direction\n",
    "        alpha = rdotr / (d.T @ _Ad)\n",
    "\n",
    "        x += alpha * d\n",
    "        # [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) pg 49:\n",
    "        #\n",
    "        # The fast recursive formula for the residual is usually used, but once every 50 iterations, the exact residual\n",
    "        # is recalculated to remove accumulated floating point error. Of course, the number 50 is arbitrary; for large\n",
    "        # n \\sqrt{n}, ©\n",
    "        # might be appropriate.\n",
    "        #\n",
    "        # @josiah: This is kind of weird since we are using `Ad_f`. Maybe we can\n",
    "        # have an optional param for A direction to do the residual reset?\n",
    "        #\n",
    "        # if nsteps > 50 and i % int(torch.sqrt(i)) == 0:\n",
    "        #     r = b - Ax\n",
    "        # else:\n",
    "        r -= alpha * _Ad\n",
    "        new_rdotr = r.T @ r\n",
    "        beta = new_rdotr / rdotr\n",
    "        d = r + beta * d\n",
    "        rdotr = new_rdotr\n",
    "        # Same as \\sigma_{new} < E^2\\sigma\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "add_docs(\n",
    "conjugate_gradients,\n",
    "\"\"\"Conjugating Gradients builds on the idea of Conjugate Directions.\n",
    "\n",
    "As noted in:\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "\n",
    "We want \"everytime we take a step, we got it right the first time\" pg 21. \n",
    "\n",
    "In otherwords, we have a model, and we have the gradients and the loss. Using the \n",
    "loss, what is the the smartest way to change/optimize the gradients?\n",
    "\n",
    "`Conjugation` is the act of makeing the `parameter space / gradient space` easier to \n",
    "optimize over. In technical terms, we find `nsteps` directions to change the gradients\n",
    "toward that are orthogonal to each other and to the `parameter space / gradient space`.\n",
    "\n",
    "In otherwords, what is the direction that is most optimal, and what is the \n",
    "direction that if used to find `x` will reduce `Ax - b` to 0. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-48.],\n",
       "        [-52.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conjugate_gradients(\n",
    "    lambda d:A@d,\n",
    "    b - A@torch.tensor([[50.],[50.]]),\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def backtrack_line_search(\n",
    "    x:torch.Tensor,\n",
    "    r:torch.Tensor,\n",
    "    error_f:Callable,\n",
    "    expected_improvement_rate:torch.Tensor,\n",
    "    accaptance_tolerance:float,\n",
    "    n_max_backtracks:int=10\n",
    "):\n",
    "    e = error_f(x)\n",
    "    for (n_back,alpha) in enumerate(.5**torch.range(n_max_backtracks)):\n",
    "        x_new = x + alpha * r \n",
    "        e_new = error_f(x_new)\n",
    "        improvement = e - e_new\n",
    "        expected_improvement = expected_improvement_rate * alpha \n",
    "\n",
    "        ratio = improvement / expected_improvement\n",
    "        if ratio.item() > accaptance_tolerance and improvement.item() > 0:\n",
    "            return True, x_new\n",
    "    return False, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
