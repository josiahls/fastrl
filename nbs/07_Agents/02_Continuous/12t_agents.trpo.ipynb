{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import NamedTuple,List,Tuple,Optional,Dict,Literal,Callable,Union\n",
    "from functools import partial\n",
    "# import typing \n",
    "from warnings import warn\n",
    "# Third party libs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Independent,Normal\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse_dps,find_dps\n",
    "from fastcore.all import add_docs,store_attr\n",
    "import gymnasium as gym\n",
    "# Local modules\n",
    "from fastrl.core import add_namedtuple_doc,SimpleStep,StepTypes\n",
    "from fastrl.pipes.core import find_dp\n",
    "from fastrl.loggers.core import Record,not_record,_RECORD_CATCH_LIST\n",
    "from fastrl.torch_core import Module,evaluating\n",
    "from fastrl.layers import Critic\n",
    "from fastrl.envs.gym import GymStepper\n",
    "from fastrl.pipes.iter.nskip import NSkipper\n",
    "from fastrl.pipes.iter.nstep import NStepper,NStepFlattener\n",
    "from fastrl.pipes.iter.firstlast import FirstLastMerger\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "from fastrl.agents.ddpg import LossCollector,BasicOptStepper,StepBatcher\n",
    "from fastrl.learner.core import LearnerBase,LearnerHead\n",
    "from fastrl.loggers.core import BatchCollector,EpochCollector,RollingTerminatedRewardCollector,EpisodeCollector\n",
    "from fastrl.memory.advantage_buffer import AdvantageBuffer,AdvantageStep\n",
    "from fastrl.loggers.core import ProgressBarLogger,EpochCollector,BatchCollector\n",
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe\n",
    "from fastrl.agents.core import AgentHead,StepFieldSelector,AgentBase\n",
    "from fastrl.agents.ddpg import ActionClip,ActionUnbatcher,NumpyConverter,SimpleModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "> Trust Region Policy Optimization via online-learning for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5b8f6",
   "metadata": {},
   "source": [
    "[(Schulman et al., 2015) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477).\n",
    "\n",
    "Directly based on [`ikostrikov`'s implimentation](https://github.com/ikostrikov/pytorch-trpo) and\n",
    "coda / explainations in [Shewchuk Cs.Cmu.Edu, 2022, Accessed 19 Nov 2022.](cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c2cba",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62d5ed",
   "metadata": {},
   "source": [
    "## Memory\n",
    "> Policy gradient online models use short term trajectory samples instead of\n",
    "ER / iid memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pipe2device(pipe,device,debug=False):\n",
    "    \"Attempt to move an entire `pipe` and its pipeline to `device`\"\n",
    "    pipes = find_dps(traverse_dps(pipe),dp.iter.IterDataPipe)\n",
    "    for pipe in pipes:\n",
    "        if hasattr(pipe,'to'): \n",
    "            if debug: print(f'Moving {pipe} to {device}')\n",
    "            pipe.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@torch.jit.script\n",
    "def discounted_cumsum_(t:torch.Tensor,gamma:float,reverse:bool=False):\n",
    "    \"\"\"Performs a cumulative sum on `t` where `gamma` is applied for each index\n",
    "    >1.\"\"\"\n",
    "    if reverse:\n",
    "        # We do +2 because +1 is needed to avoid out of index t[idx], and +2 is needed\n",
    "        # to avoid out of index for t[idx+1].\n",
    "        for idx in range(t.size(0)-2,-1,-1):\n",
    "            t[idx] = t[idx] + t[idx+1] * gamma\n",
    "    else:\n",
    "        for idx in range(1,t.size(0)):\n",
    "            t[idx] = t[idx] + t[idx-1] * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_flat_params_from(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params\n",
    "\n",
    "\n",
    "def set_flat_params_to(model, flat_params):\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "        prev_ind += flat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d26899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/2\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "#     discounted_cumsum(torch.ones(500),0.99)\n",
    "# print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def advantage_step_first_last_merge(steps:List[AdvantageStep],gamma):\n",
    "    fstep,lstep = steps[0],steps[-1]\n",
    "\n",
    "    reward = fstep.reward\n",
    "    for step in steps[1:]:\n",
    "        reward *= gamma\n",
    "        reward += step.reward\n",
    "\n",
    "    advantage = fstep.advantage\n",
    "    for step in steps[1:]:\n",
    "        advantage *= gamma\n",
    "        advantage += step.advantage\n",
    "\n",
    "    next_advantage = fstep.next_advantage\n",
    "    for step in steps[1:]:\n",
    "        next_advantage *= gamma\n",
    "        next_advantage += step.next_advantage\n",
    "        \n",
    "    yield AdvantageStep(\n",
    "        state=fstep.state.clone().detach(),\n",
    "        next_state=lstep.next_state.clone().detach(),\n",
    "        action=fstep.action,\n",
    "        episode_n=fstep.episode_n,\n",
    "        image=fstep.image,\n",
    "        reward=reward,\n",
    "        raw_action=fstep.raw_action,\n",
    "        terminated=lstep.terminated,\n",
    "        truncated=lstep.truncated,\n",
    "        total_reward=lstep.total_reward,\n",
    "        env_id=lstep.env_id,\n",
    "        proc_id=lstep.proc_id,\n",
    "        step_n=lstep.step_n,\n",
    "        advantage=advantage,\n",
    "        next_advantage=next_advantage,\n",
    "        batch_size=[]\n",
    "    )\n",
    "\n",
    "def AdvantageGymDataPipe(\n",
    "    source,\n",
    "    # AdvantageBuffer: Critic Module to valuate the advantage of state-action pairs\n",
    "    critic: Module,\n",
    "    agent:DataPipe=None, # An AgentHead\n",
    "    seed:Optional[int]=None, # The seed for the gym to use\n",
    "    # Used by `NStepper`, outputs tuples / chunks of assiciated steps\n",
    "    nsteps:int=1, \n",
    "    # Used by `NSkipper` to skip a certain number of steps (agent still gets called for each)\n",
    "    nskips:int=1,\n",
    "    # Whether when nsteps>1 to merge it into a single `StepType`\n",
    "    firstlast:bool=False,\n",
    "    # The batch size, which is different from `nsteps` in that firstlast will be \n",
    "    # run prior to batching, and a batch of steps might come from multiple envs,\n",
    "    # where nstep is associated with a single env\n",
    "    bs:int=1,\n",
    "    # The prefered default is for the pipeline to be infinate, and the learner\n",
    "    # decides how much to iter. If this is not None, then the pipeline will run for \n",
    "    # that number of `n`\n",
    "    n:Optional[int]=None,\n",
    "    # Whether to reset all the envs at the same time as opposed to reseting them \n",
    "    # the moment an episode ends. \n",
    "    synchronized_reset:bool=False,\n",
    "    # Should be used only for validation / logging, will grab a render of the gym\n",
    "    # and assign to the `StepType` image field. This data should not be used for training.\n",
    "    # If it images are needed for training, then you should wrap the env instead. \n",
    "    include_images:bool=False,\n",
    "    # If an environment truncates, terminate it.\n",
    "    terminate_on_truncation:bool=True,\n",
    "    # \n",
    "    adv_bs: int = 1000,\n",
    "    discount: float = 0.99,\n",
    "    gamma: float = 0.99\n",
    ") -> Callable:\n",
    "    \"Basic `gymnasium` `DataPipeGraph` with first-last, nstep, and nskip capability\"\n",
    "    pipe = dp.iter.IterableWrapper(source)\n",
    "    if include_images:\n",
    "        pipe = pipe.map(partial(gym.make,render_mode='rgb_array'))\n",
    "    else:\n",
    "        pipe = pipe.map(gym.make)\n",
    "    # pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "    pipe = pipe.pickleable_in_memory_cache()\n",
    "    pipe = pipe.cycle() # Cycle through the envs inf\n",
    "    pipe = GymStepper(pipe,agent=agent,seed=seed,\n",
    "                        include_images=include_images,\n",
    "                        terminate_on_truncation=terminate_on_truncation,\n",
    "                        synchronized_reset=synchronized_reset)\n",
    "    pipe = AdvantageBuffer(pipe,critic=critic,bs=adv_bs,discount=discount,gamma=gamma,nsteps=nsteps)\n",
    "    if nskips!=1: pipe = NSkipper(pipe,n=nskips)\n",
    "    if nsteps!=1:\n",
    "        pipe = NStepper(pipe,n=nsteps)\n",
    "        if firstlast:\n",
    "            pipe = FirstLastMerger(pipe,merge_behavior=advantage_step_first_last_merge)\n",
    "        else:\n",
    "            pipe = NStepFlattener(pipe) # We dont want to flatten if using FirstLastMerger\n",
    "    if n is not None: pipe = pipe.header(limit=n)\n",
    "    pipe  = pipe.batch(batch_size=bs)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(3,0)\n",
    "\n",
    "gym_pipe = AdvantageGymDataPipe(\n",
    "    ['Pendulum-v1'],agent=None,seed=0,critic=critic\n",
    ")\n",
    "\n",
    "for chunk in gym_pipe.header(5):\n",
    "    for step in chunk:\n",
    "        test_eq(type(step),AdvantageStep)\n",
    "        assert step.advantage!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a48c46",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a48c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OptionalClampLinear(Module):\n",
    "    def __init__(self,num_inputs,state_dims,fix_variance:bool=False,\n",
    "                 clip_min=0.3,clip_max=10.0):\n",
    "        \"Linear layer or constant block used for std.\"\n",
    "        store_attr()\n",
    "        if not self.fix_variance: \n",
    "            self.fc=nn.Linear(self.num_inputs,self.state_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.fix_variance: \n",
    "            return torch.full((x.shape[0],self.state_dims),1.0)\n",
    "        else:                 \n",
    "            return torch.clamp(nn.Softplus()(self.fc(x)),self.clip_min,self.clip_max)\n",
    "\n",
    "# TODO(josiahls): This is probably a highly generic SimpleGMM tbh. Once we know this\n",
    "# works, we should just rename this to SimpleGMM\n",
    "class Actor(Module):\n",
    "    def __init__(            \n",
    "            self,\n",
    "            state_sz:int,   # The input dim of the state / flattened conv output\n",
    "            action_sz:int,  # The output dim of the actions\n",
    "            hidden:int=400, # Number of neurons connected between the 2 input/output layers\n",
    "            fix_variance:bool=False,\n",
    "            clip_min=0.3,\n",
    "            clip_max=10.0\n",
    "        ):\n",
    "        \"Single-component GMM parameterized by a fully connected layer with optional std layer.\"\n",
    "        store_attr()\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(state_sz, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, action_sz),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        # self.std = OptionalClampLinear(state_sz,action_sz,fix_variance,\n",
    "        #                                clip_min=clip_min,clip_max=clip_max)\n",
    "        # self.std = nn.Linear(state_sz,action_sz)\n",
    "        # self.std.weight.data.fill_(0.5)\n",
    "        # self.std.bias.data.fill_(0.5)\n",
    "        self.std = nn.Parameter(torch.zeros(action_sz)+.5)\n",
    "        \n",
    "    def forward(self,x): return Independent(Normal(self.mu(x),self.std),1)\n",
    "\n",
    "\n",
    "add_docs(\n",
    "Actor,\n",
    "\"\"\"Produces continuous outputs from mean of a Gaussian distribution.\"\"\",\n",
    "forward=\"Mean outputs from a parameterized Gaussian distribution.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f645774",
   "metadata": {},
   "source": [
    "The `Actor` is developed from the description found in `(Schulman et al., 2015)`: \n",
    "\n",
    "    ...we used a Gaussian distribution, where the covariance matrix was diagonal \n",
    "    and independent of the state. A neural network with several fully-connected (dense) \n",
    "    layers maps from the input features to the mean of a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f645774",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,2)\n",
    "dist = actor(torch.randn(1,4))\n",
    "dist.mean,dist.stddev,dist.log_prob(torch.randn(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07959357",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07959357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class NormalExploration(dp.iter.IterDataPipe):\n",
    "    def __init__(\n",
    "                self,\n",
    "                source_datapipe:DataPipe,\n",
    "                # Based on the `base_agent.model.training`, by default no decrement or step tracking will\n",
    "                # occur during validation steps.\n",
    "                decrement_on_val:bool=False,\n",
    "                # Based on the `base_agent.model.training`, by default random actions will not be attempted\n",
    "                explore_on_val:bool=False,\n",
    "                # Also return the original action prior to exploratory noise\n",
    "                ret_original:bool=False,\n",
    "        ):\n",
    "                self.source_datapipe = source_datapipe\n",
    "                self.decrement_on_val = decrement_on_val\n",
    "                self.explore_on_val = explore_on_val\n",
    "                self.ret_original = ret_original\n",
    "                self.agent_base = None\n",
    "                self.model = None\n",
    "                self.last_std = None \n",
    "                self.last_mean = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.agent_base = find_dp(traverse_dps(self.source_datapipe),AgentBase)\n",
    "        self.model = self.agent_base.model\n",
    "        for action in self.source_datapipe:\n",
    "                if not issubclass(action.__class__,Independent):\n",
    "                        raise Exception(f'Expected Independent, got {type(action)}\\n{action}')\n",
    "\n",
    "                # Add a batch dim if missing\n",
    "                if len(action.batch_shape)==0: action = action.expand((1,))\n",
    "\n",
    "                self.last_mean = action.mean\n",
    "                self.last_std = action.stddev\n",
    "                if self.explore_on_val or self.agent_base.model.training:\n",
    "                        if self.ret_original: yield (action.sample(),action.mean)\n",
    "                        else:                 yield action.sample()\n",
    "                else:\n",
    "                        yield action.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,2)\n",
    "agent_base = AgentBase(actor)\n",
    "agent = SimpleModelRunner(agent_base)\n",
    "agent = NormalExploration(agent,explore_on_val=True)\n",
    "agent = ActionClip(agent)\n",
    "agent = AgentHead(agent)\n",
    "for action in agent(torch.randn(3,4)):\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProbabilisticStdCollector(dp.iter.IterDataPipe):\n",
    "    title:str='std'\n",
    "    def __init__(self,\n",
    "            source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.record_pipe = find_dp(traverse_dps(self.source_datapipe),NormalExploration)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # for q in self.main_buffers: q.append(Record('epsilon',None))\n",
    "        for action in self.source_datapipe:\n",
    "            if not_record(action):\n",
    "                yield Record(self.title,self.record_pipe.last_std.item())\n",
    "            yield action\n",
    "\n",
    "class ProbabilisticMeanCollector(dp.iter.IterDataPipe):\n",
    "    title:str='mean'\n",
    "    def __init__(self,\n",
    "            source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.record_pipe = find_dp(traverse_dps(self.source_datapipe),NormalExploration)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # for q in self.main_buffers: q.append(Record('epsilon',None))\n",
    "        for action in self.source_datapipe:\n",
    "            if not_record(action):\n",
    "                yield Record(self.title,self.record_pipe.last_mean.item())\n",
    "            yield action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab218d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TRPOAgent(\n",
    "    model:Actor, # The actor to use for mapping states to actions\n",
    "    # LoggerBases push logs to. If None, logs will be collected and output\n",
    "    # by the dataloader.\n",
    "    do_logging:bool=False, \n",
    "    clip_min=-1,\n",
    "    clip_max=1\n",
    ") -> AgentHead:\n",
    "    \"Produces continuous action outputs.\"\n",
    "    agent_base = AgentBase(model)\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = SimpleModelRunner(agent)\n",
    "    agent = NormalExploration(agent)\n",
    "    if do_logging:\n",
    "        agent = ProbabilisticStdCollector(agent)\n",
    "        agent = ProbabilisticMeanCollector(agent).catch_records()\n",
    "    agent = ActionClip(agent,clip_min=clip_min,clip_max=clip_max)\n",
    "    agent = ActionUnbatcher(agent)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = AgentHead(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac25225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.loggers.vscode_visualizers import VSCodeDataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "torch.manual_seed(0)\n",
    "\n",
    "actor = Actor(3,1)\n",
    "critic = Critic(3)\n",
    "\n",
    "# Setup the Agent\n",
    "agent = TRPOAgent(actor)\n",
    "\n",
    "pipe = AdvantageGymDataPipe(['Pendulum-v1'],agent=agent,n=100,seed=None,include_images=True,critic=critic)\n",
    "pipe = VSCodeDataPipe(pipe)\n",
    "\n",
    "pipe2device(pipe,'cpu',debug=True)\n",
    "\n",
    "list(pipe);\n",
    "pipe.show(step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81bdf",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81bdf",
   "metadata": {},
   "source": [
    "![](../../images/(Schulman%20et%20al.%2C%202017)%20%5BTRPO%5D%20Trust%20Region%20Policy%20Optimization%20Algorithm%201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe248ac",
   "metadata": {},
   "source": [
    "We start with finding the direction of conjugate gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c473768",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(\n",
    "    [[3.,2.],[2.,6.]]\n",
    ")\n",
    "b = torch.tensor([[2.],[-8.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d3067",
   "metadata": {},
   "source": [
    "Ref [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf), but\n",
    "$A$ is the gradients of the model, and $b$ (typically the bias) is the loss.\n",
    "\n",
    "The below function is pretty much example `B2` pg 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def conjugate_gradients(\n",
    "    # A function that takes the direction `d` and applies it to `A`.\n",
    "    # The simplest example of this found would be:\n",
    "    # `lambda d:A@d`\n",
    "    Ad_f:Callable[[torch.Tensor],torch.Tensor],  \n",
    "    # The bias or in TRPO's case the loss.\n",
    "    b:torch.Tensor, \n",
    "    # Number of steps to go for assuming we are not less than `residual_tol`.\n",
    "    nsteps:int, \n",
    "    # If the residual is less than this, then we have arrived at the local minimum.\n",
    "    # Note that (Shewchuk, 1994) they mention that this should be E^2 * rdotr_0\n",
    "    residual_tol=1e-10, \n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # The final direction to go in.\n",
    "    x = torch.zeros(b.size()).to(device)\n",
    "    # Would typically be b - Ax, however in TRPO's case this has already been \n",
    "    # done in the loss function.\n",
    "    r = b.clone()\n",
    "    # The first direction is the first residual.\n",
    "    d = b.clone()\n",
    "    rdotr = r.T @ r # \\sigma_{new} pg50\n",
    "    for i in range(nsteps):\n",
    "        _Ad = Ad_f(d) # _Ad is also considered `q`\n",
    "        # Determines the size / rate / step size of the direction\n",
    "        alpha = rdotr / (d.T @ _Ad)\n",
    "\n",
    "        x += alpha * d\n",
    "        # [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) pg 49:\n",
    "        #\n",
    "        # The fast recursive formula for the residual is usually used, but once every 50 iterations, the exact residual\n",
    "        # is recalculated to remove accumulated floating point error. Of course, the number 50 is arbitrary; for large\n",
    "        # n \\sqrt{n}, ©\n",
    "        # might be appropriate.\n",
    "        #\n",
    "        # @josiah: This is kind of weird since we are using `Ad_f`. Maybe we can\n",
    "        # have an optional param for A direction to do the residual reset?\n",
    "        #\n",
    "        # if nsteps > 50 and i % int(torch.sqrt(i)) == 0:\n",
    "        #     r = b - Ax\n",
    "        # else:\n",
    "        r -= alpha * _Ad\n",
    "        new_rdotr = r.T @ r\n",
    "        beta = new_rdotr / rdotr\n",
    "        d = r + beta * d\n",
    "        rdotr = new_rdotr\n",
    "        # Same as \\sigma_{new} < E^2\\sigma\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "add_docs(\n",
    "conjugate_gradients,\n",
    "\"\"\"Conjugating Gradients builds on the idea of Conjugate Directions.\n",
    "\n",
    "As noted in:\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "\n",
    "We want \"everytime we take a step, we got it right the first time\" pg 21. \n",
    "\n",
    "In otherwords, we have a model, and we have the gradients and the loss. Using the \n",
    "loss, what is the the smartest way to change/optimize the gradients?\n",
    "\n",
    "`Conjugation` is the act of makeing the `parameter space / gradient space` easier to \n",
    "optimize over. In technical terms, we find `nsteps` directions to change the gradients\n",
    "toward that are orthogonal to each other and to the `parameter space / gradient space`.\n",
    "\n",
    "In otherwords, what is the direction that is most optimal, and what is the \n",
    "direction that if used to find `x` will reduce `Ax - b` to 0. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjugate_gradients(\n",
    "    lambda d:A@d,\n",
    "    b - A@torch.tensor([[50.],[50.]]),\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def backtrack_line_search(\n",
    "    # A Tensor of gradients or weights to optimize\n",
    "    x:torch.Tensor,\n",
    "    # The residual that when applied to `x`, hopefully optimizes it closer to the \n",
    "    # solution/ i.e. is orthogonal.\n",
    "    r:torch.Tensor,\n",
    "    # An error function that outputs the new error given the `x_new, where\n",
    "    # `x_new` is passed as a param, and the error is returned as a float.\n",
    "    # This error is compared, and expected greater than 0.\n",
    "    error_f:Callable[[torch.Tensor],float],\n",
    "    # The region of improvement we expect the see.\n",
    "    expected_improvement_rate:torch.Tensor,\n",
    "    # The minimal amount of improvement we expect to see.\n",
    "    accaptance_tolerance:float=0.1,\n",
    "    # The number of increments to attempt to improve `x`. \n",
    "    # Each \"backtrack\", the step size on the weights will be larger.\n",
    "    n_max_backtracks:int=10\n",
    "):\n",
    "    e = error_f(x)\n",
    "    # print(\"fval before\", e.item())\n",
    "    for (n_back,alpha) in enumerate(.5**torch.arange(0,n_max_backtracks)):\n",
    "        x_new = x + alpha * r \n",
    "        e_new = error_f(x_new)\n",
    "        improvement = e - e_new\n",
    "        expected_improvement = expected_improvement_rate * alpha \n",
    "        ratio = improvement / expected_improvement\n",
    "        if ratio.item() > accaptance_tolerance and improvement.item() > 0:\n",
    "            # print(\"fval after\", e_new.item(),' on ',n_back)\n",
    "            return True, x_new\n",
    "    return False, x\n",
    "\n",
    "add_docs(\n",
    "backtrack_line_search,\n",
    "\"\"\"Backtrack line search attempts an update to a set of weights/gradients `x` `n_max_backtracks` times.\n",
    "\n",
    "Each backtrack updates the weights/gradients a little more aggressively, and checks if `error_f`\n",
    "decreases / improves. \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f29974",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "actor = Actor(4,2)\n",
    "dist = actor(torch.randn(1,4))\n",
    "old_log_prob_of_a = dist.log_prob(torch.randn(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def actor_prob_loss(weights,s,a,r,actor,old_log_prob):\n",
    "    if weights is not None:\n",
    "        set_flat_params_to(actor,weights)\n",
    "    dist = actor(s)\n",
    "    log_prob = dist.log_prob(a)\n",
    "    # loss = -r * torch.exp(log_prob-old_log_prob) \n",
    "    loss = -r.squeeze(1) * torch.exp(log_prob-old_log_prob) \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9af2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_prob_loss(None,torch.randn(1,4),torch.randn(1,2),torch.randn(1,1),actor,old_log_prob_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf80f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pre_hessian_kl(\n",
    "    model:Actor, # An Actor or any model that outputs a probability distribution\n",
    "    x:torch.Tensor # Input into the model\n",
    "):\n",
    "    r\"\"\"\n",
    "    Provides a KL conculation for the 2nd dirivative hessian to be calculated later.\n",
    "\n",
    "    It is important to note that this function will return a tensor of 0, however\n",
    "    the goal is to do autograd as opposed to doing anything with the value directly.\n",
    "\n",
    "    The \"confusing\" part of the code can be found in [4]:\n",
    "\n",
    "        \"For two univariate normal distributions p and q the above simplifies to:\"\n",
    "\n",
    "    $D_{\\text{KL}}\\left({\\mathcal {p}}\\parallel {\\mathcal {q}}\\right)=\\log {\\frac {\\sigma _{2}}{\\sigma _{1}}}+{\\frac {\\sigma _{1}^{2}+(\\mu _{1}-\\mu _{2})^{2}}{2\\sigma _{2}^{2}}}-{\\frac {1}{2}}$\n",
    "\n",
    "    Notes:\n",
    "    - [1] https://github.com/ikostrikov/pytorch-trpo/issues/2\n",
    "    - [2] [(Schulman et al., 2015) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "    \n",
    "    Appendix C:\n",
    "    \n",
    "        One could alternatively use a generic method for calculating Hessian-vector products using \n",
    "        reverse mode automatic differentiation ((Wright & Nocedal, 1999), chapter 8), computing the \n",
    "        Hessian of DKL with respect to θ. This method would be slightly less efficient as it does \n",
    "        not exploit the fact that the second derivatives of μ(x) (i.e., the second term in Equation (57))\n",
    "        can be ignored, but may be substantially easier to implement.\n",
    "\n",
    "    - [3] http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf\n",
    "    - [4] https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#cite_note-27\n",
    "    \"\"\"\n",
    "    dist = model(x)\n",
    "    mu_v = dist.mean\n",
    "    logstd_v = torch.log(dist.stddev)\n",
    "    mu0_v = mu_v.detach()\n",
    "    logstd0_v = logstd_v.detach()\n",
    "    std_v = torch.exp(logstd_v)\n",
    "    std0_v = std_v.detach()\n",
    "    kl = logstd_v - logstd0_v + (std0_v ** 2 + (mu0_v - mu_v) ** 2) / (2.0 * std_v ** 2) - 0.5\n",
    "    return kl.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a47df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_hessian_kl(actor,torch.randn(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def auto_flat(outputs,inputs,contiguous=False,create_graph=False) -> torch.Tensor:\n",
    "    \"Calculates the gradients and flattens them into a single tensor\"\n",
    "    grads = torch.autograd.grad(outputs,inputs,create_graph=create_graph)\n",
    "    # TODO: Does it always need to be contiguous?\n",
    "    if contiguous:\n",
    "        return torch.cat([grad.contiguous().view(-1) for grad in grads])\n",
    "    else:\n",
    "        return torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "def forward_pass(\n",
    "        weights:torch.Tensor,\n",
    "        s:torch.Tensor,\n",
    "        actor:Actor,\n",
    "        damping:float=0.1\n",
    "    ):\n",
    "    kl = pre_hessian_kl(actor,s)\n",
    "    kl = kl.mean()\n",
    "\n",
    "    # Calculate the 1st derivative hessian\n",
    "    flat_grad_kl = auto_flat(kl,actor.parameters(),create_graph=True)\n",
    "\n",
    "    kl_v = (flat_grad_kl * weights.detach()).sum()\n",
    "    # Calculate the 2nd derivative hessian\n",
    "    flat_grad_grad_kl = auto_flat(kl_v,actor.parameters(),contiguous=True).data\n",
    "\n",
    "    return flat_grad_grad_kl + weights * damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b45efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(4,1)\n",
    "\n",
    "forward_pass(\n",
    "    get_flat_params_from(actor),    \n",
    "    torch.randn(1,4),\n",
    "    actor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aca8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CriticLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            critic:Critic, # The critic to optimize\n",
    "            # The loss function to use\n",
    "            loss:nn.Module=nn.MSELoss,\n",
    "            # The discount factor of `q`. Typically does not need to be changed,\n",
    "            # and determines the importants of earlier state qs verses later state qs\n",
    "            discount:float=0.99,\n",
    "            # If the environment has `nsteps>1`, it is recommended to change this\n",
    "            # param to reflect that so the reward estimates are more accurate.\n",
    "            nsteps:int=1\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.critic = critic\n",
    "        self.loss = loss()\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.critic.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "                batch.to(self.device)\n",
    "\n",
    "                # traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "            self.critic.zero_grad()\n",
    "            pred = self.critic(batch.state[m])\n",
    "            yield {'loss':self.loss(pred,batch.next_advantage[m].to(dtype=torch.float32))}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActorOptAndLossProcessor(dp.iter.IterDataPipe):\n",
    "    debug:bool=False\n",
    "\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe, # The parent datapipe that should yield step types\n",
    "            actor:Actor, # The actor to optimize\n",
    "            max_kl:float=0.01\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.actor = actor\n",
    "        self.device = None\n",
    "        self.max_kl = max_kl\n",
    "        self.counter = 0\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.actor.to(**kwargs)\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __iter__(self) -> Union[Dict[Literal['loss'],torch.Tensor],SimpleStep]:\n",
    "        for batch in self.source_datapipe:\n",
    "            # Slow needs better strategy\n",
    "            with torch.no_grad():\n",
    "                batch = batch.clone()\n",
    "                batch.to(self.device)\n",
    "                traj_adv_v = (batch.advantage - torch.mean(batch.advantage)) / torch.std(batch.advantage)\n",
    "            \n",
    "            m = batch.terminated.reshape(-1,)==False\n",
    "            dist = self.actor(batch.state[m])\n",
    "            old_log_prob = dist.log_prob(batch.action[m]).detach()\n",
    "\n",
    "            loss_fn = partial(\n",
    "                actor_prob_loss,\n",
    "                s=batch.state[m],\n",
    "                a=batch.action[m],\n",
    "                r=traj_adv_v[m],\n",
    "                actor=self.actor,\n",
    "                old_log_prob=old_log_prob\n",
    "            )\n",
    "            self.counter += 1\n",
    "            # Calculate gradient backprop on initial loss function.\n",
    "            # Since the `actor` has not been updated yet, then loss is \n",
    "            # basically just going to be the `-traj_adv_v.mean()`.\n",
    "            loss = loss_fn(None)\n",
    "            loss_grad = auto_flat(loss,self.actor.parameters()).data\n",
    "            assert loss_grad.sum()!=0\n",
    " \n",
    "            forward_pass_fn = partial(\n",
    "                forward_pass,\n",
    "                s=batch.state[m],\n",
    "                actor=self.actor\n",
    "            )\n",
    "            # -loss_grad will be the `b` variable. Out goal is to find the gradient\n",
    "            # update direction that gets the output of `forward_pass_fn` to\n",
    "            # have an orthogonal step size to hit that loss_grad.\n",
    "            # The step direction (d) is going to be constrained by the f``KLdiv.\n",
    "            d = conjugate_gradients(forward_pass_fn,-loss_grad,10)\n",
    "\n",
    "            shs = 0.5 * (d * forward_pass_fn(d)).sum(0,keepdim=True)\n",
    "            lm = torch.sqrt(shs/self.max_kl)\n",
    "            full_step = d/lm[0]\n",
    "            neggdotstepdir = (-loss_grad * d).sum(0, keepdim=True)\n",
    "\n",
    "            prev_params = get_flat_params_from(self.actor)\n",
    "            success,params = backtrack_line_search(prev_params,full_step,loss_fn,neggdotstepdir/lm[0])\n",
    "            if success:\n",
    "                set_flat_params_to(self.actor,params)\n",
    "\n",
    "            yield {'loss':loss}\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50899950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def TRPOLearner(\n",
    "    # The actor model to use\n",
    "    actor:Actor,\n",
    "    # The critic model to use\n",
    "    critic:Critic,\n",
    "    # A list of dls, where index=0 is the training dl.\n",
    "    dls:List[object],\n",
    "    # Optional logger bases to log training/validation data to.\n",
    "    do_logging:bool=True,\n",
    "    # The learning rate for the critic. Expected to learn faster than the actor\n",
    "    critic_lr:float=1e-2,\n",
    "    # The optimizer for the critic\n",
    "    # Note that weight decay doesnt seem to be great for \n",
    "    # Pendulum, so we use regular Adam, which has the decay rate\n",
    "    # set to 0. (Lillicrap et al., 2016) would instead use AdamW\n",
    "    critic_opt:torch.optim.Optimizer=Adam,\n",
    "    # Reference: GymStepper docs\n",
    "    nsteps:int=1,\n",
    "    # The device for the entire pipeline to use. Will move the agent, dls, \n",
    "    # and learner to that device.\n",
    "    device:torch.device=None,\n",
    "    # Number of batches per epoch\n",
    "    batches:int=None,\n",
    "    # Debug mode will output device moves\n",
    "    debug:bool=False\n",
    ") -> LearnerHead:\n",
    "    warn(\"TRPO only kind of converges. There is a likely a bug, however I am unable to identify until after PPO implimentation\")\n",
    "\n",
    "    learner = LearnerBase({'actor':actor,'critic':critic},dls[0])\n",
    "    learner = BatchCollector(learner,batches=batches)\n",
    "    learner = EpochCollector(learner)\n",
    "    if do_logging: \n",
    "        learner = learner.dump_records()\n",
    "        learner = ProgressBarLogger(learner)\n",
    "        learner = RollingTerminatedRewardCollector(learner)\n",
    "        learner = EpisodeCollector(learner).catch_records()\n",
    "    learner = StepBatcher(learner)\n",
    "    learner = CriticLossProcessor(learner,critic=critic)\n",
    "    learner = LossCollector(learner,title='critic-loss').catch_records()\n",
    "    learner = BasicOptStepper(learner,critic,critic_lr,opt=critic_opt,filter=True,do_zero_grad=False)\n",
    "    learner = ActorOptAndLossProcessor(learner,actor)\n",
    "    learner = LossCollector(learner,title='actor-loss').catch_records()\n",
    "\n",
    "    if len(dls)==2:\n",
    "        val_learner = LearnerBase({'actor':actor,'critic':critic},dls[1]).visualize_vscode()\n",
    "        val_learner = BatchCollector(val_learner,batches=batches)\n",
    "        val_learner = EpochCollector(val_learner).catch_records(drop=True)\n",
    "        return LearnerHead((learner,val_learner))\n",
    "    else:\n",
    "        return LearnerHead(learner)\n",
    "\n",
    "TRPOLearner.__doc__=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.dataloading.core import dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14367a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "actor = Actor(2,1)\n",
    "critic = Critic(2)\n",
    "\n",
    "# Setup the Agent\n",
    "# agent = TRPOAgent(actor,do_logging=True,clip_min=-2,clip_max=2)\n",
    "agent = TRPOAgent(actor,do_logging=True,clip_min=-1,clip_max=1)\n",
    "\n",
    "# Setup the Dataloaders\n",
    "params = dict(source=['MountainCarContinuous-v0']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,gamma=0.99,discount=0.99)\n",
    "\n",
    "dls = dataloaders((\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=200,gamma=0.99,discount=0.99),\n",
    "    # AdvantageGymDataPipe(['Pendulum-v1']*1,agent=agent,critic=critic,nsteps=2,nskips=2,firstlast=True,bs=1,gamma=0.99,discount=0.99)\n",
    "    AdvantageGymDataPipe(**params,bs=200),\n",
    "    AdvantageGymDataPipe(**params,bs=1,include_images=True)\n",
    "))\n",
    "# Setup the Learner\n",
    "learner = TRPOLearner(actor,critic,dls,batches=10,critic_lr=0.01)\n",
    "# learner.fit(1)\n",
    "learner.fit(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "learner.validate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e85344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    mean = sum(data) / len(data)\n",
    "    variance = sum([(x - mean) ** 2 for x in data]) / len(data)\n",
    "    stddev = variance ** 0.5\n",
    "    return [(x - mean) / stddev for x in data]\n",
    "\n",
    "\n",
    "def visualize_advantage_steps(steps: [AdvantageStep]):\n",
    "    # Extract relevant data from steps\n",
    "    rewards = [step.reward.item() for step in steps]\n",
    "    advantages = [step.advantage.item() for step in steps]\n",
    "    next_advantages = [step.next_advantage.item() for step in steps]\n",
    "    action = [step.action.item() for step in steps]\n",
    "    critic_values = [na - a for na, a in zip(next_advantages, advantages)]\n",
    "\n",
    "    # Normalize the data\n",
    "    rewards = normalize(rewards)\n",
    "    advantages = normalize(advantages)\n",
    "    action = normalize(action)\n",
    "    next_advantages = normalize(next_advantages)\n",
    "    critic_values = normalize(critic_values)\n",
    "\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # ax.plot(rewards, label=\"Rewards\", color=\"blue\")\n",
    "    ax.plot(advantages, label=\"Advantages\", color=\"green\")\n",
    "    ax.plot(next_advantages, label=\"Next Advantages\", color=\"red\")\n",
    "    ax.plot(critic_values, label=\"Critic Value Estimates\", color=\"purple\")\n",
    "    # ax.plot(action, label=\"Action Value Estimates\", color=\"orange\")\n",
    "    \n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_title(\"Visualization of Advantage Steps\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afa0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
