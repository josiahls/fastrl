{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import *\n",
    "from typing_extensions import Literal\n",
    "import typing \n",
    "# Third party libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe,traverse,replace_dp\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.torch_core import *\n",
    "from fastrl.layers import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.envs.gym import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "> Trust Region Policy Optimization via online-learning for continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5b8f6",
   "metadata": {},
   "source": [
    "[(Schulman et al., 2017) [TRPO] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c2cba",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageStep(typing.NamedTuple):\n",
    "    state:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    action:      torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_state:  torch.FloatTensor=torch.FloatTensor([0])\n",
    "    terminated:  torch.BoolTensor=torch.BoolTensor([1])\n",
    "    truncated:   torch.BoolTensor=torch.BoolTensor([1])\n",
    "    reward:      torch.FloatTensor=torch.LongTensor([0])\n",
    "    total_reward:torch.FloatTensor=torch.FloatTensor([0])\n",
    "    advantage:   torch.FloatTensor=torch.FloatTensor([0])\n",
    "    env_id:      torch.LongTensor=torch.LongTensor([0])\n",
    "    proc_id:     torch.LongTensor=torch.LongTensor([0])\n",
    "    step_n:      torch.LongTensor=torch.LongTensor([0])\n",
    "    episode_n:   torch.LongTensor=torch.LongTensor([0])\n",
    "    image:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).clone() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def detach(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).detach() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def device(self,device='cpu'):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(device=device) for fld in self.__class__._fields}\n",
    "        )\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(*args,**kwargs) for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls,seed=None,**flds):\n",
    "        _flds,_annos = cls._fields,cls.__annotations__\n",
    "\n",
    "        def _random_annos(anno):\n",
    "            t = anno(1)\n",
    "            if anno==torch.BoolTensor: t.random_(2) \n",
    "            else:                      t.random_(100)\n",
    "            return t\n",
    "\n",
    "        return cls(\n",
    "            *(flds.get(\n",
    "                f,_random_annos(_annos[f])\n",
    "            ) for f in _flds)\n",
    "        )\n",
    "\n",
    "add_namedtuple_doc(\n",
    "AdvantageStep,\n",
    "\"\"\"Represents a single step in an environment similar to `SimpleStep` however has\n",
    "an addition field called `advantage`.\"\"\",\n",
    "advantage=\"\"\"Generally characterized as $A(s,a) = Q(s,a) - V(s)$\"\"\",\n",
    "**{f:getattr(SimpleStep,f).__doc__ for f in SimpleStep._fields}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f777630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdvantageStep\n",
       "\n",
       ">      AdvantageStep (state:torch.FloatTensor=tensor([0.]),\n",
       ">                     action:torch.FloatTensor=tensor([0.]),\n",
       ">                     next_state:torch.FloatTensor=tensor([0.]),\n",
       ">                     terminated:torch.BoolTensor=tensor([True]),\n",
       ">                     truncated:torch.BoolTensor=tensor([True]),\n",
       ">                     reward:torch.FloatTensor=tensor([0]),\n",
       ">                     total_reward:torch.FloatTensor=tensor([0.]),\n",
       ">                     advantage:torch.FloatTensor=tensor([0.]),\n",
       ">                     env_id:torch.LongTensor=tensor([0]),\n",
       ">                     proc_id:torch.LongTensor=tensor([0]),\n",
       ">                     step_n:torch.LongTensor=tensor([0]),\n",
       ">                     episode_n:torch.LongTensor=tensor([0]),\n",
       ">                     image:torch.FloatTensor=tensor([0.]))\n",
       "\n",
       "Represents a single step in an environment similar to `SimpleStep` however has\n",
       "an addition field called `advantage`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       " - **state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the initial state of the environment and the previous state.\n",
       " - **action**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The action that was taken to transition from `state` to `next_state`\n",
       " - **next_state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the next state, and the last state in the environment\n",
       " - **terminated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment such as reaching a goal or 'living long enough' as \n",
       "                    described by the MDP.\n",
       "                    Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155\n",
       " - **truncated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment that can be seen as an out of bounds condition either\n",
       "                   literally going out of bounds, breaking rules, or exceeding the timelimit allowed by the MDP.\n",
       "                   Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155'\n",
       " - **reward**:`<class 'torch.FloatTensor'>`  = `tensor([0])`The single reward for this step.\n",
       " - **total_reward**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The total accumulated reward for this episode up to this step.\n",
       " - **advantage**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Generally characterized as $A(s,a) = Q(s,a) - V(s)$\n",
       " - **env_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The environment this step came from (useful for debugging)\n",
       " - **proc_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The process this step came from (useful for debugging)\n",
       " - **step_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The step number in a given episode.\n",
       " - **episode_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The episode this environment is currently running through.\n",
       " - **image**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Intended for display and logging only. If the intention is to use images for training an\n",
       "               agent, then use a env wrapper instead."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdvantageStep\n",
       "\n",
       ">      AdvantageStep (state:torch.FloatTensor=tensor([0.]),\n",
       ">                     action:torch.FloatTensor=tensor([0.]),\n",
       ">                     next_state:torch.FloatTensor=tensor([0.]),\n",
       ">                     terminated:torch.BoolTensor=tensor([True]),\n",
       ">                     truncated:torch.BoolTensor=tensor([True]),\n",
       ">                     reward:torch.FloatTensor=tensor([0]),\n",
       ">                     total_reward:torch.FloatTensor=tensor([0.]),\n",
       ">                     advantage:torch.FloatTensor=tensor([0.]),\n",
       ">                     env_id:torch.LongTensor=tensor([0]),\n",
       ">                     proc_id:torch.LongTensor=tensor([0]),\n",
       ">                     step_n:torch.LongTensor=tensor([0]),\n",
       ">                     episode_n:torch.LongTensor=tensor([0]),\n",
       ">                     image:torch.FloatTensor=tensor([0.]))\n",
       "\n",
       "Represents a single step in an environment similar to `SimpleStep` however has\n",
       "an addition field called `advantage`.\n",
       "\n",
       "Parameters:\n",
       "\n",
       " - **state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the initial state of the environment and the previous state.\n",
       " - **action**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The action that was taken to transition from `state` to `next_state`\n",
       " - **next_state**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Both the next state, and the last state in the environment\n",
       " - **terminated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment such as reaching a goal or 'living long enough' as \n",
       "                    described by the MDP.\n",
       "                    Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155\n",
       " - **truncated**:`<class 'torch.BoolTensor'>`  = `tensor([True])`Represents an ending condition for an environment that can be seen as an out of bounds condition either\n",
       "                   literally going out of bounds, breaking rules, or exceeding the timelimit allowed by the MDP.\n",
       "                   Good reference is: https://github.com/openai/gym/blob/39b8661cb09f19cb8c8d2f59b57417517de89cb0/gym/core.py#L151-L155'\n",
       " - **reward**:`<class 'torch.FloatTensor'>`  = `tensor([0])`The single reward for this step.\n",
       " - **total_reward**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`The total accumulated reward for this episode up to this step.\n",
       " - **advantage**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Generally characterized as $A(s,a) = Q(s,a) - V(s)$\n",
       " - **env_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The environment this step came from (useful for debugging)\n",
       " - **proc_id**:`<class 'torch.LongTensor'>`  = `tensor([0])`The process this step came from (useful for debugging)\n",
       " - **step_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The step number in a given episode.\n",
       " - **episode_n**:`<class 'torch.LongTensor'>`  = `tensor([0])`The episode this environment is currently running through.\n",
       " - **image**:`<class 'torch.FloatTensor'>`  = `tensor([0.])`Intended for display and logging only. If the intention is to use images for training an\n",
       "               agent, then use a env wrapper instead."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdvantageStep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62d5ed",
   "metadata": {},
   "source": [
    "## Memory\n",
    "> Policy gradient online models use short term trajectory samples instead of\n",
    "ER / iid memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageBuffer(dp.iter.IterDataPipe):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "            source_datapipe:DataPipe,\n",
    "            # Will accumulate up to `bs` or when the episode has terminated.\n",
    "            bs=1000,\n",
    "            # If the `self.device` is not cpu, and `store_as_cpu=True`, then\n",
    "            # calls to `sample()` will dynamically move them to `self.device`, and\n",
    "            # next `sample()` will move them back to cpu before producing new samples.\n",
    "            # This can be slower, but can save vram.\n",
    "            # If `store_as_cpu=False`, then samples stay on `self.device`\n",
    "            #\n",
    "            # If being run with n_workers>0, shared_memory, and fork, this MUST be true. This is needed because\n",
    "            # otherwise the tensors in the memory will remain shared with the tensors created in the \n",
    "            # dataloader.\n",
    "            store_as_cpu:bool=True\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.bs = bs\n",
    "        self.store_as_cpu = store_as_cpu\n",
    "        self.device = None\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({k:v if k!='memory' else f'{len(self)} elements' for k,v in self.__dict__.items()})\n",
    "\n",
    "    def __len__(self): return self._sz_tracker\n",
    "    \n",
    "    def __iter__(self) -> AdvantageStep:\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "        for step in self.source_datapipe:\n",
    "            if self.debug: print('Adding to advantage buffer: ',step)\n",
    "            env_id = int(step.env_id.detach().cpu())\n",
    "            if env_id not in self.env_advantage_buffer: \n",
    "                self.env_advantage_buffer[env_id] = []\n",
    "            self.env_advantage_buffer[env_id].append(step)\n",
    "\n",
    "            if any((step.truncated,step.terminated,len(self.env_advantage_buffer)>self.bs)):\n",
    "                yield AdvantageStep()\n",
    "\n",
    "    @classmethod\n",
    "    def insert_dp(cls,old_dp=GymStepper) -> Callable[[DataPipe],DataPipe]:\n",
    "        def _insert_dp(pipe):\n",
    "            v = replace_dp(\n",
    "                traverse(pipe,only_datapipe=True),\n",
    "                find_dp(traverse(pipe,only_datapipe=True),old_dp),\n",
    "                cls(find_dp(traverse(pipe,only_datapipe=True),old_dp))\n",
    "            )\n",
    "            return list(v.values())[0][0]\n",
    "        return _insert_dp\n",
    "\n",
    "add_docs(\n",
    "AdvantageBuffer,\n",
    "\"\"\"Samples entire trajectories instead of individual time steps.\"\"\",\n",
    "to=torch.Tensor.to.__doc__\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_pipe = GymTransformBlock(\n",
    "    agent=None,seed=0,\n",
    "    dp_augmentation_fns=[AdvantageBuffer.insert_dp()]\n",
    ")(['Pendulum-v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(gym_pipe.header(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
