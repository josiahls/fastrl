{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python native modules\n",
    "\n",
    "# Third party libs\n",
    "import torch\n",
    "import numpy as np\n",
    "# Local modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# Conjugation\n",
    "> Notes and functions illistrated by [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) and will\n",
    "be referenced through this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe652e",
   "metadata": {},
   "source": [
    "The [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) bases \n",
    "all the example problems on sample problem 4..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dfd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(\n",
    "    [[3.,2.],[2.,6.]]\n",
    ")\n",
    "b = torch.tensor([[2.],[-8.]])\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b356a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x minimizes the following function `f`\n",
    "x = torch.tensor([[2.],[-2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751c060",
   "metadata": {},
   "source": [
    "## Basics\n",
    "We define a quadratic function whose minimum value output is -10. The challenge is\n",
    "to pretend we don't know this, and automatically figure out what value of `x` is needed\n",
    "to figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ec694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return (1/2) * x.T @ A @ x - b.T @ x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf79748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67706caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(np.array([x for x in np.ndindex(20,20)])).float()-10\n",
    "\n",
    "def plot3d(xx,f):\n",
    "    return px.scatter_3d(\n",
    "        x=xx[:,0],\n",
    "        y=xx[:,1],\n",
    "        z = [f(x.reshape(-1,1)).numpy()[0][0] for x in xx]\n",
    "    )\n",
    "plot3d(xx,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(x):\n",
    "    return (1/2) * A.T @ x + (1/2) * A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daac4f7",
   "metadata": {},
   "source": [
    "Using `f_prime` above, we can gradients as vectors. For example given the location..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(np.array([x for x in np.ndindex(20,20)])).float()-15\n",
    "xx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212b063",
   "metadata": {},
   "source": [
    "We get the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime(xx[0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0949d58",
   "metadata": {},
   "source": [
    "Which is extremely large relative to the original point. We can make a guess that\n",
    "this is pretty far away from the solution. We can get the magnitude via.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitude(derivative_x,x):\n",
    "    return torch.linalg.norm(x-derivative_x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "'Magnitude: '+str(get_magnitude(f_prime(xx[0].reshape(-1,1)),xx[0])),\n",
    "'\\nGadient Vector: '+str(f_prime(xx[0].reshape(-1,1))),\n",
    "'\\nOriginal point location: '+str(xx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d3225",
   "metadata": {},
   "source": [
    "Like we said earlier, using the `f_prime` function, finding which points are \n",
    "farthur from the solution is pretty easy. Remember that solution is `[2,-2]`, \n",
    "so we would expect points closer to here to have a smaller magnitude..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e205bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "'Magnitude: '+str(get_magnitude(f_prime(xx[-1].reshape(-1,1)),xx[-1])),\n",
    "'\\nGadient Vector: '+str(f_prime(xx[-1].reshape(-1,1))),\n",
    "'\\nOriginal point location: '+str(xx[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecbc3b",
   "metadata": {},
   "source": [
    "When we plot the magnitudes of the gradients we get a nice slope where the solution `[2,-2]` is\n",
    "the lowest value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(xx,f):\n",
    "    return px.scatter_3d(\n",
    "        x=xx[:,0],\n",
    "        y=xx[:,1],\n",
    "        z = [get_magnitude(f_prime(x.reshape(-1,1)),x) for x in xx]\n",
    "    )\n",
    "plot3d(xx,f_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f008804",
   "metadata": {},
   "source": [
    "Although the above equations give off the impression that this is only useful for a \"path finding\" \n",
    "kind of scenario, this actually scales out into more general parameter optimization. \n",
    "\n",
    "The solution `[2,-2]` could instead be many dimensions `[2,-2,1,6,3,...]` in which\n",
    "case the algorithm of choice becomes more important since the solution search \n",
    "could be much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948895e",
   "metadata": {},
   "source": [
    "## Automatically finding the solution: Method of Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017acac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(np.array([x for x in np.ndindex(100,100)])).float()-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d6f71",
   "metadata": {},
   "source": [
    "Note: [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) defines `error` as $error\\, e_{(i)} = x_{(i)} - x$ which is used\n",
    "to indicate the distance from the solution... However if we already know the solution, then why bother with anything discuessed in the paper? Isn't the whole \n",
    "point in all this that we don't know the solution?  Or maybe the $x$ in this scenario is the \"ideal\" solution,\n",
    "but we don't know whether it is actually possible.\n",
    "\n",
    "In machine learning we techinically have the solution already known, however we don't actually want to immediately optimize to it. In other words,\n",
    "there can be many $x$s and we want to optimize to all of them in a general way. In other-other words, we want to get close-enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x,x_i):\n",
    "    return x_i - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1a60f",
   "metadata": {},
   "source": [
    "We also have residual $r_i$ which can have multiple definitions:\n",
    "\n",
    "$r_i = b - Ax_i$ is how far we are from the correct value of $b$\n",
    "\n",
    "$r_i = -Ae_i$ is the error transformed by $A$ into the same space $b$\n",
    "\n",
    "$r_i = -f'(x_i)$ is the direction of steepest descent \n",
    "\n",
    "However `Shewchuk, 1994` notes that only the last definition applies for non-lienar problems. \n",
    "This makes sense since the first definition can be found by setting $r_i$ to zero,\n",
    "the second would require knowledge of `e` and `x`. \n",
    "\n",
    "With that considered, we define the residual as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48393c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_linear_residual(x_i):\n",
    "    return -f_prime(x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3ffbe",
   "metadata": {},
   "source": [
    "We can define a basic stepping function as $x_1 = x_0 + \\alpha r_0$\n",
    "\n",
    "Where $\\alpha$ indicates how big of a step we should take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_step(x_i,r_func,alpha):\n",
    "    return x_i + alpha * r_func(x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b5de1",
   "metadata": {},
   "source": [
    "In this step example we have `xx[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d2673",
   "metadata": {},
   "source": [
    "Since this is `[-50,-50]`, it is very far from the solution `[-2,2]`, so the residual will be \n",
    "pretty massive, and likely can over shoot the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_step(xx[0].reshape(-1,1),non_linear_residual,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d206b",
   "metadata": {},
   "source": [
    "We define `naive_line_search` just take a step for `n_iterations` with a struct \n",
    "alpha. One obvious change would be to make `alpha` more dynamic such as start big,\n",
    "but then get small if we think we are getting to a solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_line_search(x,n_iterations,alpha=0.01):\n",
    "    steps = []\n",
    "    for i in range(n_iterations):\n",
    "        steps.append(x)\n",
    "        x = sd_step(x,non_linear_residual,alpha)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0c22e",
   "metadata": {},
   "source": [
    "We find that the line search eventually optimizes to the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_line_search(xx[0].reshape(-1,1),100,0.1)[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(xx,f,line_search_func):\n",
    "    fig = go.Figure()\n",
    "    mags = [get_magnitude(f_prime(x.reshape(-1,1)),x) for x in xx]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=xx[:,0],\n",
    "            y=xx[:,1],\n",
    "            z = mags,\n",
    "            mode='markers',\n",
    "            name=\"Gradient Magnitudes\"\n",
    "        )\n",
    "    )\n",
    "    steps = torch.vstack([o.reshape(1,-1) for o in line_search_func(xx[40].reshape(-1,1),100,0.1)])\n",
    "    # print(steps)\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=steps[:,0],\n",
    "            y=steps[:,1],\n",
    "            z = [get_magnitude(f_prime(x.reshape(-1,1)),x)+100 for x in steps],\n",
    "            mode='lines+markers',\n",
    "            name=\"Steps taken\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "plot3d(xx,f_prime,naive_line_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377af11",
   "metadata": {},
   "source": [
    "Success! We see that the final red dot is on the solution `[-2,2]` given an alpha of `0.1` and 100 steps. \n",
    "However how did we choose `alpha`? There might be a more automatic way of picking this.\n",
    "\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) pg 6 choice of alpha should be such that $\\frac{d}{d\\alpha}f(x_1) = 0$.\n",
    "Which if we remember that $x_1 = x_0 + \\alpha r_0$, then $\\frac{d}{d\\alpha}f(x_1) = 0$ turns into $f'(x_1)^{T}r_0 = 0$ implying that $f'(x_1)^{T}$ and $r_0$ are orthogonal to each other.\n",
    "\n",
    "In the end, the final equation is $\\alpha = \\frac{r^{T}_{0}r_{0}}{r^{T}_{0}A r_{0}}$\n",
    "\n",
    "Note that $r^{T}_{0}A r_{0}$ and/or $r^{T}_{0}r_{0}$ will be zero if perfectly orthogonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_step2(x_i,r,alpha):\n",
    "    return x_i + alpha * r\n",
    "\n",
    "def alpha_calc_line_search(x,n_iterations,alpha=0.01):\n",
    "    steps = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        steps.append(x)\n",
    "        r = non_linear_residual(x)\n",
    "        # $\\alpha = \\frac{r^{T}_{0}r_{0}}{r^{T}_{0}A r_{0}}$\n",
    "        alpha = (r.T@r) / (r.T @ (A@r))[0][0]\n",
    "        x = sd_step2(x,r,alpha)\n",
    "    return steps\n",
    "\n",
    "plot3d(xx,f_prime,alpha_calc_line_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ef401",
   "metadata": {},
   "source": [
    "By simply changing how `alpha` is calculated, we end up taking orthogonal jumps between gradients resulting in \n",
    "much faster convergence, and removing the need for a togglable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80eec0",
   "metadata": {},
   "source": [
    "## Automatically finding the solution: Jacobi Iteration\n",
    "Note pg 11 [Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) considers\n",
    "Jacobi iteration more useful.\n",
    "\n",
    "In this instance we take `A` and break it into 2 sub matricies: `E` and `D`.\n",
    "Where `D` has the diagonal elements, while non-diagonal are zero, and \n",
    "`E` has the diagonal elements as zero, and non-diagonal are kept. \n",
    "\n",
    "Both `E` and `D` are the same shape as `A` and in fact `A` can be reconstructed from them\n",
    "via $A=E+D$\n",
    "\n",
    "The Jacobi Method is illistrated as:\n",
    "\n",
    "$$\n",
    "Ax = b\n",
    "\\\\\n",
    "Dx = -Ex + b\n",
    "\\\\\n",
    "x = -D^{-1}+D^{-1}b\n",
    "\\\\\n",
    "x = Bx + z\n",
    "$$\n",
    "where \n",
    "$$\n",
    "B = -D^{-1}E\n",
    "\\\\\n",
    "z = D^{-1}b\n",
    "$$\n",
    "\n",
    "Which this considered, we get our x search space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(np.array([x for x in np.ndindex(100,100)])).float()-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd24b2",
   "metadata": {},
   "source": [
    "We split `A` into `D` and `E`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_d_e():\n",
    "    D = A*torch.eye(A.shape[0])\n",
    "    E = A.clone()\n",
    "    non_zero = torch.eye(A.shape[0]).nonzero()\n",
    "    E[non_zero[:,0],non_zero[:,1]] = 0\n",
    "    return D,E\n",
    "D,E = calculate_d_e()\n",
    "A,D,E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d429a",
   "metadata": {},
   "source": [
    "Now we can create the iterative step process. This is illustrated as:\n",
    "$$\n",
    "x_{i+1} = Bx_{i}+z\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1186ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_step(x_i):\n",
    "    B = -D.inverse()@E \n",
    "    z = D.inverse()@b \n",
    "    return B@x_i + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobi_step(xx[0].reshape(-1,1)),xx[0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450def82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_line_search(x,n_iterations):\n",
    "    steps = []\n",
    "    for i in range(n_iterations):\n",
    "        steps.append(x)\n",
    "        x = jacobi_step(x)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5df397",
   "metadata": {},
   "source": [
    "So if we do a basic line search, after 20-25 iterations we are able to solve `Ax-b = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6fe09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobi_line_search(xx[0].reshape(-1,1),25)[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac782e",
   "metadata": {},
   "source": [
    "A couple iteresting notes from the author:\n",
    "\n",
    "    splitting A differently — that is,\n",
    "    by choosing a different  and  — we could have derived the Gauss-Seidel method, or the method of\n",
    "    Successive Over-Relaxation (SOR). Our hope is that we have chosen a splitting for which has a small\n",
    "    spectral radius. Here, I chose the Jacobi splitting arbitrarily for simplicity.\n",
    "\n",
    "So there are other methods that use different strategies to splitting `A` that we could try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0539d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(xx):\n",
    "    fig = go.Figure()\n",
    "    mags = [get_magnitude(f_prime(x.reshape(-1,1)),x) for x in xx]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=xx[:,0],\n",
    "            y=xx[:,1],\n",
    "            z = mags,\n",
    "            mode='markers',\n",
    "            name=\"Gradient Magnitudes\"\n",
    "        )\n",
    "    )\n",
    "    steps = torch.vstack([o.reshape(1,-1) for o in jacobi_line_search(xx[40].reshape(-1,1),100)])\n",
    "    # print(steps)\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=steps[:,0],\n",
    "            y=steps[:,1],\n",
    "            z = [get_magnitude(f_prime(x.reshape(-1,1)),x)+100 for x in steps],\n",
    "            mode='lines+markers',\n",
    "            name=\"Steps taken\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "plot3d(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab1df8",
   "metadata": {},
   "source": [
    "> Warning: On pg 12 it is noted that \"Unfortunately,Jacobi does not converge for every A, or even for every positive-definite A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063bbf7",
   "metadata": {},
   "source": [
    "### Eigen Vectors\n",
    "The above method requires anylysis via eigen vectors to verify that we can even use\n",
    "Jacobi effectively. `numpy` has `np.linalg.eig` and `torch` has a \n",
    "function with a similar name that can do this.\n",
    "\n",
    "The main point in this section is that $\\rho(B) < 1$ which is defined as the spectral radius. \n",
    "Greater than 1 means that the Jacobi optimization will not converge. \n",
    "If $\\rho(B) < 1$, then we can gauge how quickly we will converge based on the \n",
    "eigenvector being used.\n",
    "\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) uses an example of finding and using an eigenvectors / eigenvalues. \n",
    "\n",
    "[3Blue1Brown](https://www.youtube.com/watch?v=PFDu9oVAE-g&ab_channel=3Blue1Brown) does a good visual breakdown of what they are.\n",
    "\n",
    "Given `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd73420",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d0eb2",
   "metadata": {},
   "source": [
    "We will be finding eigenvectors `v` and eigenvalues $\\lambda$\n",
    "\n",
    "$Av = \\lambda v = \\lambda I v$\n",
    "\n",
    "alternatively can be represented as:\n",
    "\n",
    "$(\\lambda I - A)v = 0$\n",
    "\n",
    "[Shewchuk, 1994](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) pg 12 also notes that \n",
    "\"eigenvectors are nonzero, so $\\lambda I - A$ must be singular then...\"\n",
    "\n",
    "$(\\lambda I - A)v = 0$\n",
    "\n",
    "$$\n",
    "det \n",
    "\\begin{pmatrix}\n",
    "3 & 2 \\\\\n",
    "2 & 6 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Results in the 2 degree polynomial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.polynomial.Polynomial([14,-9,1]);p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d118c",
   "metadata": {},
   "source": [
    "which if we factor out we get $(\\lambda-7)(\\lambda-2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fe009",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.roots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f569bda",
   "metadata": {},
   "source": [
    "Which each of these are the eigenvalues, now we just need the eigenvectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vector(mat,eigenvalue):\n",
    "    \"$\\lambda I - A$ part of the $(\\lambda I - A)v$\"\n",
    "    return eigenvalue * torch.eye(mat.shape[0]) - mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_vector(A,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a55328",
   "metadata": {},
   "source": [
    "We can find the eigen vectors be solving the equation below...\n",
    "\n",
    "$$\n",
    "(\\lambda I - A)v = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "4 & -2 \\\\\n",
    "-2 & 1 \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \n",
    "\\end{bmatrix}\n",
    "\n",
    "= 4v_1 - 2v_2\n",
    "\n",
    "= 0 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ba566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_d_e(mat):\n",
    "    D = mat*torch.eye(mat.shape[0])\n",
    "    E = mat.clone()\n",
    "    non_zero = torch.eye(mat.shape[0]).nonzero()\n",
    "    E[non_zero[:,0],non_zero[:,1]] = 0\n",
    "    return D,E\n",
    "D,E = calculate_d_e(A)\n",
    "A,D,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_step(x_i):\n",
    "    B = -D.inverse()@E \n",
    "    z = D.inverse()@b \n",
    "    return B,B@x_i + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,x_i = jacobi_step(xx[0]);B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceec247",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values,eigen_vectors = np.linalg.eig(B);eigen_values,eigen_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86464559",
   "metadata": {},
   "source": [
    "`np.linalg.eig` normalizes the `eigen_vectors` to be unit length. We do a \n",
    "vvery simple un-normalize just so we can match the book. \n",
    "\n",
    "Some of the rational can be found [here](https://stackoverflow.com/a/47803336/4577212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vectors /= eigen_vectors[1,:]\n",
    "eigen_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dbad4",
   "metadata": {},
   "source": [
    "Eigenvectors of `B` are $[\\sqrt{2},1]^T$ and $[-\\sqrt{2},1]^T$ where eigenvalues are respectively\n",
    "$-\\sqrt{2}/3$ and $\\sqrt{2}/3$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print here to show that the np.linalg.eig produces the same values as in the book\n",
    "print('Eigen values: ',-np.sqrt(2)/3,' ',np.sqrt(2)/3)\n",
    "print('Eigen vectors: ',[np.sqrt(2),1],' ',[-np.sqrt(2),1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69f997",
   "metadata": {},
   "source": [
    "#### Now what?\n",
    "\n",
    "Lets output the eigen vector components during a regular Jacobi optimization sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8143b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_line_search(x,n_iterations):\n",
    "    steps,errs = [],[]\n",
    "    for i in range(n_iterations):\n",
    "        old_x = x\n",
    "        B,x = jacobi_step(x)\n",
    "        err = torch.hstack((x,old_x)) # The book has x_i - x, however we need a vector.\n",
    "        if err.sum()!=0:\n",
    "            errs.append(err)\n",
    "            steps.append(old_x)\n",
    "    return steps,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c947de",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pal = px.colors.sequential.Rainbow\n",
    "col_pal_iterator = itertools.cycle(col_pal)\n",
    "\n",
    "def plot_arrow(fig,x1,y1,x2,y2,z1=1,z2=1,color=None, row=2, col=1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[x1,x2],\n",
    "            y=[y1,y2],\n",
    "            z = [z1,z2],\n",
    "            mode='lines',\n",
    "            name=f'eig of:\\n {x2.item()}\\n{y2.item()}',\n",
    "            showlegend=False,\n",
    "            line=dict(color=color)\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(xx):\n",
    "    fig = make_subplots(rows=3, cols=1,\n",
    "                        specs=[[{\"type\": \"scene\"}], [{\"type\": \"scene\"}], [{\"type\": \"scene\"}]])\n",
    "    mags = [get_magnitude(f_prime(x.reshape(-1,1)),x) for x in xx]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=xx[:,0],\n",
    "            y=xx[:,1],\n",
    "            z = mags,\n",
    "            mode='markers',\n",
    "            name=\"Gradient Magnitudes\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    step_errs = torch.vstack([torch.hstack((step.reshape(1,-1),err.reshape(1,-1))) for step,err in zip(*jacobi_line_search(xx[40].reshape(-1,1),100))])\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=step_errs[:,0],\n",
    "            y=step_errs[:,1],\n",
    "            z = [get_magnitude(f_prime(x.reshape(-1,1)),x)+100 for x in step_errs[:,:2]],\n",
    "            mode='lines+markers',\n",
    "            name=\"Steps taken\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=step_errs[:,2],\n",
    "            y=step_errs[:,3],\n",
    "            z = [1 for x in step_errs],\n",
    "            mode='lines+markers',\n",
    "            name=\"Error / Change in X taken\",\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    for err in step_errs[:5]:\n",
    "        # We are kind of feeding the wrong value into here. The error\n",
    "        # is a locationless magnitude of x1,y1,x2,y2. Really we should be feeding\n",
    "        # those into here, since we want to know the transformation matrix of those.\n",
    "        eigv,eig_vec = torch.linalg.eig(\n",
    "            torch.vstack(\n",
    "                (err[2:4].reshape(1,-1),\n",
    "                 err[2:4].reshape(1,-1))\n",
    "        ))\n",
    "        eigv,eig_vec = eigv.float(),eig_vec.float()\n",
    "        # We calculate the actual scaled eigen vectors, then offset them to the \n",
    "        # primary vector\n",
    "        e1 = err[2:4][0] + eig_vec[:,0] * eigv[0]\n",
    "        e2 = err[2:4][1] + eig_vec[:,1] * eigv[1]\n",
    "\n",
    "        color = next(col_pal_iterator)\n",
    "\n",
    "        # First eigen vector\n",
    "        plot_arrow(\n",
    "            fig,\n",
    "            x1=e1[0],  # arrows' head\n",
    "            y1=e1[1],  # arrows' head\n",
    "            x2=err[2:4][0],  # arrows' tail\n",
    "            y2=err[2:4][1],  # arrows' tail\n",
    "            color=color\n",
    "        )\n",
    "        # Second eigen vector\n",
    "        plot_arrow(\n",
    "            fig,\n",
    "            x1=e2[0],  # arrows' head\n",
    "            y1=e2[1],  # arrows' head\n",
    "            x2=err[2:4][0],  # arrows' tail\n",
    "            y2=err[2:4][1],  # arrows' tail\n",
    "            color=color\n",
    "        )\n",
    "        ### Lets also plot the eigen vector components by themselves ###\n",
    "        # First eigen vector\n",
    "        plot_arrow(\n",
    "            fig,\n",
    "            x1=e1[0],  # arrows' head\n",
    "            y1=e1[1],  # arrows' head\n",
    "            x2=err[2:4][0],  # arrows' tail\n",
    "            y2=err[2:4][1],  # arrows' tail\n",
    "            color=color,\n",
    "            row=3,col=1\n",
    "        )\n",
    "        # Second eigen vector\n",
    "        plot_arrow(\n",
    "            fig,\n",
    "            x1=e2[0],  # arrows' head\n",
    "            y1=e2[1],  # arrows' head\n",
    "            x2=err[2:4][0],  # arrows' tail\n",
    "            y2=err[2:4][1],  # arrows' tail\n",
    "            color=color,\n",
    "            row=3,col=1\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=1800,\n",
    "        margin=dict(\n",
    "            l=50,\n",
    "            r=50,\n",
    "            b=100,\n",
    "            t=100,\n",
    "            pad=4\n",
    "        ),\n",
    "        # paper_bgcolor=\"LightSteelBlue\",\n",
    "    )\n",
    "    return fig\n",
    "plot3d(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be171ea1",
   "metadata": {},
   "source": [
    "### General Convergence  6.2\n",
    "pg 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7435b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_w(x1,x2):\n",
    "    \"Based on equation 25, pg 18\"\n",
    "    error = (x1 - x2).reshape(-1,2,1)\n",
    "\n",
    "    eigv,eigvec = torch.linalg.eig(torch.hstack((error,error)).reshape(-1,2,2))\n",
    "    eigv,eigvec = eigv.float(),eigvec.float()\n",
    "    # So pg 15 has a var: \\gamma_j ... I think that this is the same as \n",
    "    # error since error is basically the difference of components of x1 and x2\n",
    "    e_i = (error * eigv.unsqueeze(-1)).sum(1)\n",
    "    \n",
    "    w_squared = 1 - (\n",
    "        (((error[:,0]**2)*(eigv[:,0,None]**2) + (error[:,1]**2)*(eigv[:,1,None]**2))**2)\n",
    "        /\n",
    "        (\n",
    "            ((error[:,0]**2)*(eigv[:,0,None]) + (error[:,1]**2)*(eigv[:,1,None])) \n",
    "            * ((error[:,0]**2)*(eigv[:,0,None]**3) + (error[:,1]**2)*(eigv[:,1,None]**3))\n",
    "        )\n",
    "    )\n",
    "    return w_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcfdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps,err = jacobi_line_search(xx[40].reshape(-1,1),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714daaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_w(\n",
    "    torch.hstack(steps[:-1]).T,\n",
    "    torch.hstack(steps[1:]).T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3d(xx):\n",
    "    fig = make_subplots(rows=1, cols=1,\n",
    "                        specs=[[{\"type\": \"scene\"}]])\n",
    "    steps,errors = jacobi_line_search(xx[40].reshape(-1,1),100)\n",
    "\n",
    "    step_errs = torch.vstack([torch.hstack((step.reshape(1,-1),err.reshape(1,-1))) for step,err in zip(*(steps,errors))])\n",
    "\n",
    "\n",
    "    ws = calculate_w(\n",
    "        torch.hstack(steps[:-1]).T,\n",
    "        torch.hstack(steps[1:]).T\n",
    "    )\n",
    "\n",
    "    ws = torch.vstack((ws[0],ws))\n",
    "    ws[torch.isnan(ws)] = 0\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=step_errs[:,2],\n",
    "            y=step_errs[:,3],\n",
    "            z = [w.item() for w in ws],\n",
    "            mode='lines+markers',\n",
    "            name=\"Error / Change in X taken (adjusted by 100)\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=step_errs[:,2],\n",
    "            y=step_errs[:,3],\n",
    "            z = [0.0000001 for x in step_errs],\n",
    "            mode='lines+markers',\n",
    "            name=\"Error / Change in X taken\",\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        margin=dict(\n",
    "            l=50,\n",
    "            r=50,\n",
    "            b=100,\n",
    "            t=100,\n",
    "            pad=4\n",
    "        ),\n",
    "        # paper_bgcolor=\"LightSteelBlue\",\n",
    "    )\n",
    "    return fig\n",
    "plot3d(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
