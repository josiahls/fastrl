{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp memory.advantage_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from typing import NamedTuple,Dict,Literal,List,Union,Tuple\n",
    "# Third party libs\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchdata.datapipes as dp \n",
    "from torchdata.dataloader2.graph import DataPipe\n",
    "from fastrl.torch_core import evaluating\n",
    "from fastcore.all import add_docs\n",
    "# Local modules\n",
    "from fastrl.core import StepTypes,add_namedtuple_doc,SimpleStep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# Advantage Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44897c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageStep(NamedTuple):\n",
    "    state:           torch.FloatTensor=torch.FloatTensor([0])\n",
    "    action:          torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_state:      torch.FloatTensor=torch.FloatTensor([0])\n",
    "    terminated:      torch.BoolTensor=torch.BoolTensor([1])\n",
    "    truncated:       torch.BoolTensor=torch.BoolTensor([1])\n",
    "    reward:          torch.FloatTensor=torch.LongTensor([0])\n",
    "    total_reward:    torch.FloatTensor=torch.FloatTensor([0])\n",
    "    advantage:       torch.FloatTensor=torch.FloatTensor([0])\n",
    "    next_advantage:  torch.FloatTensor=torch.FloatTensor([0])\n",
    "    env_id:          torch.LongTensor=torch.LongTensor([0])\n",
    "    proc_id:         torch.LongTensor=torch.LongTensor([0])\n",
    "    step_n:          torch.LongTensor=torch.LongTensor([0])\n",
    "    episode_n:       torch.LongTensor=torch.LongTensor([0])\n",
    "    image:           torch.FloatTensor=torch.FloatTensor([0])\n",
    "    raw_action:      torch.FloatTensor=torch.FloatTensor([0])\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).clone() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def detach(self):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).detach() for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    def device(self,device='cpu'):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(device=device) for fld in self.__class__._fields}\n",
    "        )\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        return self.__class__(\n",
    "            **{fld:getattr(self,fld).to(*args,**kwargs) for fld in self.__class__._fields}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls,seed=None,**flds):\n",
    "        _flds,_annos = cls._fields,cls.__annotations__\n",
    "\n",
    "        def _random_annos(anno):\n",
    "            t = anno(1)\n",
    "            if anno==torch.BoolTensor: t.random_(2) \n",
    "            else:                      t.random_(100)\n",
    "            return t\n",
    "\n",
    "        return cls(\n",
    "            *(flds.get(\n",
    "                f,_random_annos(_annos[f])\n",
    "            ) for f in _flds)\n",
    "        )\n",
    "    \n",
    "StepTypes.register(AdvantageStep)\n",
    "\n",
    "add_namedtuple_doc(\n",
    "AdvantageStep,\n",
    "\"\"\"Represents a single step in an environment similar to `SimpleStep` however has\n",
    "an addition field called `advantage`.\"\"\",\n",
    "advantage=\"\"\"Generally characterized as $A(s,a) = Q(s,a) - V(s)$\"\"\",\n",
    "**{f:getattr(SimpleStep,f).__doc__ for f in SimpleStep._fields}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@torch.jit.script\n",
    "def discounted_cumsum_(t:torch.Tensor,gamma:float,reverse:bool=False):\n",
    "    \"\"\"Performs a cumulative sum on `t` where `gamma` is applied for each index\n",
    "    >1.\"\"\"\n",
    "    if reverse:\n",
    "        # We do +2 because +1 is needed to avoid out of index t[idx], and +2 is needed\n",
    "        # to avoid out of index for t[idx+1].\n",
    "        for idx in range(t.size(0)-2,-1,-1):\n",
    "            t[idx] = t[idx] + t[idx+1] * gamma\n",
    "    else:\n",
    "        for idx in range(1,t.size(0)):\n",
    "            t[idx] = t[idx] + t[idx-1] * gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdvantageBuffer(dp.iter.IterDataPipe):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "            # A datapipe that produces `StepType`s.\n",
    "            source_datapipe:DataPipe,\n",
    "            # A model that takes in a `state` and outputs a single value \n",
    "            # representing $V$, where as $Q$ is $V + reward$\n",
    "            critic:nn.Module,\n",
    "            # Will accumulate up to `bs` or when the episode has terminated.\n",
    "            bs=1000,\n",
    "            # The discount factor, otherwise known as $\\gamma$, is defined in \n",
    "            # (Shulman et al., 2016) as '... $\\gamma$ introduces bias into\n",
    "            # the policy gradient estimate...'.\n",
    "            discount:float=0.99,\n",
    "            # $\\lambda$ is unqiue to GAE and manages importance to values when \n",
    "            # they are in accurate is defined in (Shulman et al., 2016) as '... $\\lambda$ < 1\n",
    "            # introduces bias only when the value function is inaccurate....'.\n",
    "            gamma:float=0.99,\n",
    "            nsteps:int = 1\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.bs = bs\n",
    "        self.critic = critic\n",
    "        self.device = None\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.gamma = gamma\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "\n",
    "    def to(self,*args,**kwargs):\n",
    "        self.device = kwargs.get('device',None)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str({k:v if k!='env_advantage_buffer' else f'{len(self)} elements' \n",
    "                    for k,v in self.__dict__.items()})\n",
    "\n",
    "    def __len__(self): return self._sz_tracker\n",
    "\n",
    "    def update_advantage_buffer(self,step:StepTypes.types) -> int:\n",
    "        if self.debug: \n",
    "            print('Adding to advantage buffer: ',step)\n",
    "        env_id = int(step.env_id.detach().cpu())\n",
    "        if env_id not in self.env_advantage_buffer: \n",
    "            self.env_advantage_buffer[env_id] = []\n",
    "        self.env_advantage_buffer[env_id].append(step)\n",
    "        return env_id\n",
    "        \n",
    "    def zip_steps(\n",
    "        self,steps:List[Union[StepTypes.types]]\n",
    "    ) -> Tuple[torch.FloatTensor,torch.FloatTensor,torch.BoolTensor]:\n",
    "        step_subset = [(o.reward,o.state,o.truncated or o.terminated) for o in steps]\n",
    "        zipped_fields = zip(*step_subset)\n",
    "        return list(map(torch.vstack,zipped_fields))\n",
    "\n",
    "    def delta_calc(self,reward,v,v_next,done):\n",
    "        return reward + (self.gamma * v * done) - v_next\n",
    "\n",
    "    def __iter__(self) -> AdvantageStep:\n",
    "        self.env_advantage_buffer:Dict[Literal['env'],list] = {}\n",
    "        for step in self.source_datapipe:\n",
    "            env_id = self.update_advantage_buffer(step)\n",
    "            done = step.truncated or step.terminated\n",
    "            if done or len(self.env_advantage_buffer[env_id])>self.bs:\n",
    "                steps = self.env_advantage_buffer[env_id]\n",
    "                rewards,states,dones = self.zip_steps(steps)\n",
    "                # We vstack the final next_state so we have a complete picture\n",
    "                # of the state transitions and matching reward/done shapes.\n",
    "                with torch.no_grad():\n",
    "                    with evaluating(self.critic):\n",
    "                        values = self.critic(torch.vstack((states,steps[-1].next_state)))\n",
    "                delta = self.delta_calc(rewards,values[:-1],values[1:],dones)\n",
    "                discounted_cumsum_(delta,self.discount*self.gamma**self.nsteps,reverse=True)\n",
    "\n",
    "                for _step,gae_advantage,v in zip(*(steps,delta,values)):\n",
    "                    yield AdvantageStep(\n",
    "                        advantage=gae_advantage,\n",
    "                        next_advantage=gae_advantage+v,\n",
    "                        **{f:getattr(_step,f) for f in _step._fields}\n",
    "                    )\n",
    "                self.env_advantage_buffer[env_id].clear()\n",
    "\n",
    "add_docs(\n",
    "AdvantageBuffer,\n",
    "\"\"\"Collects an entire episode, calculates the advantage for each step, then\n",
    "yields that episode's `AdvantageStep`s.\n",
    "\n",
    "This is described in the original paper `(Shulman et al., 2016) High-Dimensional \n",
    "Continuous Control Usin Generalized Advantage Estimation`.\n",
    "\n",
    "This algorithm is based on the concept of advantage:\n",
    "\n",
    "$A_{\\pi}(s,a) = Q_{\\pi}(s,a) - V_{\\pi}(s)$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 5 calculates it as:\n",
    "\n",
    "$\\hat{A}_{t}^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}^V$\n",
    "\n",
    "Where (Shulman et al., 2016) pg 4 defines $\\delta$ as:\n",
    "\n",
    "$\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$\n",
    "\"\"\",\n",
    "to=torch.Tensor.to.__doc__,\n",
    "update_advantage_buffer=\"Adds `step` to `env_advantage_buffer` based on the environment id.\",\n",
    "zip_steps=\"\"\"Given `steps`, strip out the `Tuple[reward,state,truncated or terminated]` fields,\n",
    "and `torch.vstack` them.\"\"\",\n",
    "delta_calc=\"\"\"Calculates $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_{t})$ which \n",
    "is the advantage difference between state transitions.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import test_eq\n",
    "import gymnasium as gym\n",
    "import fastrl.pipes.iter.cacheholder\n",
    "from fastrl.layers import Critic\n",
    "from fastrl.envs.gym import GymStepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipe(source,critic,adv_bs=16):\n",
    "    pipe = dp.iter.IterableWrapper(source)\n",
    "    pipe = pipe.map(gym.make)\n",
    "    pipe = pipe.pickleable_in_memory_cache()\n",
    "    pipe = pipe.cycle() \n",
    "    pipe = GymStepper(pipe,agent=None,seed=0)\n",
    "    pipe = AdvantageBuffer(pipe,critic=critic,bs=adv_bs)\n",
    "    pipe = pipe.batch(1)\n",
    "    return pipe.header(400)\n",
    "\n",
    "critic = Critic(3,0)\n",
    "\n",
    "gym_pipe = test_pipe(['Pendulum-v1'],critic=critic)\n",
    "\n",
    "steps = []\n",
    "for chunk in gym_pipe:\n",
    "    for step in chunk:\n",
    "        steps.append(step)\n",
    "        test_eq(type(step),AdvantageStep)\n",
    "        assert step.advantage!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    mean = sum(data) / len(data)\n",
    "    variance = sum([(x - mean) ** 2 for x in data]) / len(data)\n",
    "    stddev = variance ** 0.5\n",
    "    return [(x - mean) / stddev for x in data]\n",
    "\n",
    "\n",
    "def visualize_advantage_steps(steps: [AdvantageStep]):\n",
    "    # Extract relevant data from steps\n",
    "    rewards = [step.reward.item() for step in steps]\n",
    "    advantages = [step.advantage.item() for step in steps]\n",
    "    next_advantages = [step.next_advantage.item() for step in steps]\n",
    "    action = [step.action.item() for step in steps]\n",
    "    critic_values = [na - a for na, a in zip(next_advantages, advantages)]\n",
    "\n",
    "    # Normalize the data\n",
    "    rewards = normalize(rewards)\n",
    "    advantages = normalize(advantages)\n",
    "    action = normalize(action)\n",
    "    next_advantages = normalize(next_advantages)\n",
    "    critic_values = normalize(critic_values)\n",
    "\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # ax.plot(rewards, label=\"Rewards\", color=\"blue\")\n",
    "    ax.plot(advantages, label=\"Advantages\", color=\"green\")\n",
    "    # ax.plot(next_advantages, label=\"Next Advantages\", color=\"red\")\n",
    "    ax.plot(critic_values, label=\"Critic Value Estimates\", color=\"purple\")\n",
    "    # ax.plot(action, label=\"Action Value Estimates\", color=\"orange\")\n",
    "    \n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_title(\"Visualization of Advantage Steps\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb14004",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_advantage_steps(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
