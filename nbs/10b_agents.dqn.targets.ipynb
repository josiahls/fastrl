{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbverbose.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp agents.dqn.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from collections import deque\n",
    "from typing import *\n",
    "# Third party libs\n",
    "import torch\n",
    "from torch.nn import *\n",
    "from torch import optim\n",
    "from fastcore.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.optimizer import OptimWrapper\n",
    "from fastai.callback.all import *\n",
    "# Local modules\n",
    "from fastrl.data.block import *\n",
    "from fastrl.data.gym import *\n",
    "from fastrl.agent import *\n",
    "from fastrl.core import *\n",
    "from fastrl.agents.dqn.core import *\n",
    "from fastrl.memory.experience_replay import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# DQN Targets + N-Step\n",
    "> A Bare-Bones DQN is usually extremely unstable. Target models can eleviate this. We also support First-Last N steps better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b3c7c2-5c0a-43a1-a38d-cbbb96df6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA=0.99\n",
    "def calc_target(net, local_reward, next_state,d):\n",
    "    if next_state is None or d:\n",
    "        return local_reward\n",
    "    state_v = torch.tensor([next_state], dtype=torch.float32).to(default_device())\n",
    "    next_q_v = net(state_v)\n",
    "    best_q = next_q_v.max(dim=1)[0].item()\n",
    "    return local_reward + GAMMA * best_q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246d55c5-3204-442b-b49c-c3cea07b8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for  i in range(1000):\n",
    "\n",
    "# # learn.opt.zero_grad()\n",
    "\n",
    "# learn.state_action_values = learn.model.model(learn.xb['state']).gather(1, learn.xb['action']).squeeze(-1)\n",
    "# learn.next_state_values = learn.target_model(learn.xb['next_state']).max(1)[0]\n",
    "# learn.next_state_values[learn.xb['done'].squeeze(-1)] = 0.0\n",
    "\n",
    "# learn.expected_state_action_values = learn.next_state_values.detach() * (0.99**3) + learn.xb['reward'].squeeze(-1)\n",
    "# learn.loss= nn.MSELoss()(learn.state_action_values,learn.next_state_values)\n",
    "\n",
    "# # learn.loss.backward()\n",
    "# # learn.opt.step()\n",
    "\n",
    "# print(learn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68efe13-b0da-4885-bd93-44e4721088e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e558c085-c99d-49c0-804c-b9935598a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d37e0c-6ec3-47b3-9a20-1c949793e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.xb['reward'].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e821e20-fb0b-41b4-9e37-251462b3a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.next_state_values.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf6eb01-5449-4d5a-afd5-cc422bcac62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DQNTargetTrainer(Callback):\n",
    "    \n",
    "    def __init__(self,n_batch=0,target_sync=300,discount=0.99,n_steps=1):\n",
    "        store_attr()\n",
    "        self._xb=None  \n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.learn.target_model=deepcopy(self.learn.model.model)\n",
    "        self.n_batch=0\n",
    "    \n",
    "    def after_pred(self):\n",
    "        self._xb=self.yb\n",
    "        self.learn.yb=[]\n",
    "        \n",
    "\n",
    "  \n",
    "        self.learn.opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            s=self.learn.xb['state']\n",
    "            a=self.learn.xb['action']\n",
    "            ns=self.xb['next_state']\n",
    "            r=self.xb['reward']\n",
    "            d=self.xb['done']\n",
    "    \n",
    "\n",
    "        self.learn.state_action_values = self.learn.model.model(s).gather(1,a).squeeze(-1)\n",
    "        with torch.no_grad():\n",
    "            self.learn.next_state_values = self.target_model(ns).max(1)[0]\n",
    "            self.learn.next_state_values[d.squeeze(-1)] = 0.0\n",
    "\n",
    "            self.learn.expected_state_action_values = self.learn.next_state_values.detach() * (self.discount**self.n_steps) + r.squeeze(-1)\n",
    "        self.learn.loss= nn.MSELoss()(self.learn.state_action_values,self.learn.expected_state_action_values)\n",
    "        \n",
    "        if (self.n_batch-1)%self.target_sync==0:\n",
    "            print('The loss should be practically zero: ',self.loss)\n",
    "            print(self.learn.state_action_values-self.learn.expected_state_action_values)\n",
    "        \n",
    "        # raise Exception\n",
    "        self.learn.loss.backward()\n",
    "        self.learn.opt.step()\n",
    "#         self.learn.batch_targets = torch.cat([calc_target(self.learn.model.model, r, ns.cpu().numpy(),d)\n",
    "#                          for r,ns,d in zip(self.learn.xb['reward'],self.learn.xb['next_state'],self.learn.xb['done'])])\n",
    "        \n",
    "#         self.learn.opt.zero_grad()\n",
    "#         self.learn.states_v = self.xb['state'].to(default_device()).float()\n",
    "#         self.learn.net_q_v = self.learn.model.model(self.learn.states_v)\n",
    "#         # print(net_q_v)\n",
    "#         self.learn.target_q = self.learn.net_q_v.cpu().data.numpy().copy()\n",
    "        \n",
    "#         # print(batch_targets,target_q)\n",
    "#         self.learn.target_q[range(self.learn.xb.bs()), self.xb['action'].cpu()] = self.learn.batch_targets.cpu()\n",
    "#         self.learn.target_q_v = torch.tensor(self.learn.target_q)\n",
    "#         # print(net_q_v, target_q_v)\n",
    "#         loss_v = self.learn.loss_func(self.learn.net_q_v.cpu(), self.learn.target_q_v.cpu())\n",
    "#         loss_v.backward()\n",
    "#         self.learn.loss=loss_v.cpu()\n",
    "#         # print(loss_v)\n",
    "#         self.learn.opt.step()\n",
    "        \n",
    "#         self.learn.yb=self.xb\n",
    "#         self.learn.xb=self.xb[0]\n",
    "#         self._xb=({k:v.clone() for k,v in self.xb.items()},)\n",
    "#         self.learn.done_mask=self.xb['done'].reshape(-1,)\n",
    "#         self.learn.next_q=self.target_model(self.xb['next_state']).max(dim=1).values.reshape(-1,1)\n",
    "#         self.learn.next_q[self.done_mask]=1\n",
    "#         self.learn.targets=self.xb['reward']+self.learn.next_q*(self.discount**self.n_steps)\n",
    "#         self.learn.pred=self.learn.model.model(self.xb['state'])\n",
    "\n",
    "#         t_q=self.pred.clone()\n",
    "#         t_q.scatter_(1,self.xb['action'],self.targets)\n",
    "#         self.learn.yb=(t_q,)\n",
    "        with torch.no_grad():\n",
    "            self.learn.td_error=(self.learn.state_action_values.cpu()-self.learn.expected_state_action_values.cpu()).reshape(-1,1)**2\n",
    "        \n",
    "    def before_backward(self): self.learn.yb=self._xb\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if self.n_batch%self.target_sync==0:\n",
    "            self.target_model.load_state_dict(self.learn.model.state_dict())\n",
    "            # if self.n_batch>1:raise Exception\n",
    "        self.n_batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2d91aec-d20d-4e28-83b1-5492915350fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it\n"
     ]
    }
   ],
   "source": [
    "dqn=DQN(4,2)\n",
    "agent=Agent(dqn,cbs=[ArgMaxFeed,DiscreteEpsilonRandomSelect(min_epsilon=0.02)])\n",
    "source=Source(cbs=[GymLoop('CartPole-v1',agent,steps_count=1,seed=None,#mode='rgb_array',\n",
    "                           steps_delta=1),FirstLast#,ResReduce(reduce_by=4)\n",
    "                  ])\n",
    "dls=SourceDataBlock().dataloaders([source],n=1000,bs=1,num_workers=0)\n",
    "\n",
    "er_tb=ExperienceReplayTensorboard(comment='_dqn_target',every_epoch=1)\n",
    "# opt=optim.Adam(dqn.parameters(), lr=0.0001)\n",
    "learn=Learner(dls,agent,loss_func=MSELoss(),\n",
    "              opt_func=partial(OptimWrapper,dqn.parameters(),optim.Adam),\n",
    "              cbs=[ExperienceReplayCallback(bs=32,max_sz=1000,warmup_sz=32,freeze_at_max=True),\n",
    "                   DQNTargetTrainer(n_steps=1,target_sync=5000)\n",
    "                   ,er_tb\n",
    "                  ],\n",
    "              metrics=[Reward,Epsilon,NEpisodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcaa1ba9-7e7c-46eb-81a0-68d54e1e039b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0a01500cff14a3f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0a01500cff14a3f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "SHOW_TENSOR_BOARD=True\n",
    "if not os.environ.get(\"IN_TEST\", None) and SHOW_TENSOR_BOARD:\n",
    "    run_tensorboard(samples_per_plugin='images=2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c763d-d293-45b2-bb7e-46f6d00bcaad",
   "metadata": {},
   "source": [
    "A few things, the loss should be <1. If it is not, there is something majorly wrong iwth the training.\n",
    "\n",
    "It is alright for the actual values twe are comparing to be way more than >1 though, but it is expected that the loss should not be all that high at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6864bea5-39d1-498c-ba33-cf4cc756d391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_reward</th>\n",
       "      <th>train_epsilon</th>\n",
       "      <th>train_n_episodes</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_reward</th>\n",
       "      <th>valid_epsilon</th>\n",
       "      <th>valid_n_episodes</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>17.960784</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>51</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>19.730000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>102</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>20.880000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>145</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>20.520000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>191</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>21.570000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>268</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>20.930000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>331</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036043</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>378</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036505</td>\n",
       "      <td>19.940000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>429</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.037018</td>\n",
       "      <td>19.190000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>481</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034429</td>\n",
       "      <td>18.440000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>529</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.083630</td>\n",
       "      <td>19.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>591</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.087661</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>633</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>18.220000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>689</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.066275</td>\n",
       "      <td>18.270000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>736</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.072148</td>\n",
       "      <td>19.090000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>791</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.131040</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>848</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.131858</td>\n",
       "      <td>18.880000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>895</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.111032</td>\n",
       "      <td>18.970000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>939</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.135014</td>\n",
       "      <td>19.550000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>984</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.112437</td>\n",
       "      <td>20.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1039</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.155983</td>\n",
       "      <td>20.380000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1090</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.158334</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1131</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.157344</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1182</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.146119</td>\n",
       "      <td>19.530000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1243</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.140432</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1293</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.196181</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1346</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.195283</td>\n",
       "      <td>19.370000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1406</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.209266</td>\n",
       "      <td>20.890000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1451</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.166778</td>\n",
       "      <td>20.540000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1493</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>18.530000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1549</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.249149</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1594</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.244026</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1653</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.222805</td>\n",
       "      <td>19.440000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1695</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.273831</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1737</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.198564</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1788</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.266650</td>\n",
       "      <td>19.970000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1846</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.284163</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1901</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>21.640000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1953</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.291279</td>\n",
       "      <td>21.660000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2002</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.213886</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2059</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.299499</td>\n",
       "      <td>21.160000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2096</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2147</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.350722</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2193</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.330054</td>\n",
       "      <td>19.380000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2253</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.289513</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2298</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.295217</td>\n",
       "      <td>19.260000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2348</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.377988</td>\n",
       "      <td>18.760000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2401</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastrl_user/fastrl/fastrl/memory/experience_replay.py:134: UserWarning: image is missing from the experience replay. Image section of the replay will not be logged.\n",
      "  warn('image is missing from the experience replay. Image section of the replay will not be logged.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss should be practically zero:  TensorBatch(0.9907, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-1.0440, -1.0236, -1.0498, -0.0246, -1.0358, -1.0149, -1.0102, -0.9955,\n",
      "        -1.0295, -1.0671, -1.0393, -1.0625, -1.0461, -1.0243, -1.0161, -0.0478,\n",
      "        -1.0099, -1.0370, -1.0111, -1.0322, -0.9760, -1.0351, -1.0100, -1.0388,\n",
      "        -0.9724, -1.0122, -1.0463, -1.0477, -1.0482, -1.0032, -1.0263, -1.0659],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(0.9941, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.7403, -1.0462, -1.0342, -1.0266, -1.0569, -1.0326, -1.0578, -1.0104,\n",
      "        -0.8361, -1.0350, -1.0262, -0.9537, -1.0112, -1.0029, -1.0225, -1.0659,\n",
      "        -1.0202, -0.7528, -0.9697, -1.0647, -1.0191, -0.8861, -0.9843, -0.9945,\n",
      "        -1.1034, -0.9715, -0.9514, -0.9955, -1.0290, -1.0312, -1.0427, -1.0244],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(0.9553, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-1.0185, -0.9584,  0.8990,  0.9528, -1.0355, -1.0327, -1.0092, -1.0775,\n",
      "        -0.9754, -0.8838, -0.9632, -1.0346, -1.0088, -1.0477, -0.8991, -1.0187,\n",
      "        -1.0642, -1.0263, -0.9478, -1.0115, -1.0032, -1.0402, -0.9881, -0.7587,\n",
      "        -1.0443, -0.7896, -1.0099, -0.9547, -0.8350, -1.0178, -1.0601, -0.7973],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(0.9588, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.8241, -1.0244, -1.0118, -1.0135,  1.0298, -0.9325, -0.9494, -1.0288,\n",
      "        -1.0205, -0.9891, -0.5950, -0.9397, -0.7888, -0.9894, -0.8533, -1.0553,\n",
      "        -1.0736, -1.0246, -1.0122, -0.9711, -1.0122, -0.8550, -1.0351, -1.0126,\n",
      "        -1.1418, -1.0311, -1.0379, -0.9990, -1.0669, -0.9197, -1.0025, -0.9266],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(1.0159, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.9738, -0.9136, -0.9633, -0.9090, -1.0005, -1.0263, -1.0132, -0.7311,\n",
      "        -1.0020, -0.9532, -0.8858, -0.7669, -1.0376, -1.0519, -1.0595,  0.8889,\n",
      "        -0.9872, -1.0615, -1.1490, -0.7453,  1.7700, -1.0987, -1.0165, -0.9961,\n",
      "        -1.0295, -0.9991, -0.4255, -0.9966, -1.1306, -0.8779, -1.1215, -1.0460],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(0.9412, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.2848, -0.9861, -0.9301, -1.0298, -0.8576, -1.0366, -1.0627, -0.7327,\n",
      "        -1.0125, -0.6575, -1.0085, -1.1769, -0.9276, -1.2366, -1.0257, -0.9779,\n",
      "        -1.0205, -0.9124, -1.0567, -0.8535, -1.0326, -1.0437, -0.1402, -1.0329,\n",
      "        -0.9745, -0.9426, -1.0366, -1.0727, -1.0830, -1.0256, -1.0922, -0.9812],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(0.9559, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.9958, -0.9480, -0.9336, -0.9065, -1.0435,  1.2460, -0.6082, -1.3118,\n",
      "        -1.1426, -1.1379, -0.8994, -1.2348, -1.0665, -0.8708, -0.9839, -0.9996,\n",
      "        -1.0454, -1.0811, -0.8362, -0.9554, -0.3836, -0.9011, -0.8181, -1.0746,\n",
      "         0.1780, -1.0884, -1.2873, -0.6144, -0.8937, -1.0146, -0.9254, -0.9402],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(1.0946, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-1.1649, -1.0036, -1.4222, -0.9720, -1.4222, -1.1738, -1.0411, -1.0109,\n",
      "        -1.2337, -0.8495, -0.6120, -1.0750, -0.8990, -0.9928, -0.4254, -0.9056,\n",
      "        -0.8031, -1.0797, -1.4249, -1.2271, -1.0268, -0.9696, -1.1557,  1.0655,\n",
      "        -0.6010, -1.0378, -1.0100, -0.9253, -1.1409, -0.9776, -1.0320, -1.0832],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n",
      "The loss should be practically zero:  TensorBatch(1.5047, device='cuda:0', grad_fn=<AliasBackward>)\n",
      "TensorBatch([-0.9195, -1.0776, -1.0518, -1.4092, -0.9261, -1.2703, -1.0708, -0.8932,\n",
      "        -0.9700, -0.2185, -1.0065, -0.6596, -0.9664, -0.8329, -0.9280,  4.2690,\n",
      "        -0.9530, -0.9550, -0.9449, -1.0306, -0.5597, -0.8922, -1.1217, -1.1185,\n",
      "        -0.9802, -0.9308, -1.2203, -0.9336, -0.7687, -0.9156, -1.2860, -0.9073],\n",
      "       device='cuda:0', grad_fn=<AliasBackward>)\n"
     ]
    }
   ],
   "source": [
    "slow=True\n",
    "learn.fit(3 if not slow else 47,lr=0.0001,wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbverbose.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()\n",
    "    notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2104d-1cd0-4e35-91c6-c49d7a85454c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
