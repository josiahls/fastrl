{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        # assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        # assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.dataloader2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6969a7b",
   "metadata": {},
   "source": [
    "# DataLoader2\n",
    "\n",
    "> Warning: This is intended to be a revision of the torchdata dataloader2 and so the source code might be \"ugly\". The preference is that\n",
    "we dont need to do any of this, and instead do a PR into torchdata to natively support what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os,typing\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import *\n",
    "from torchdata.dataloader2.graph import find_dps,traverse\n",
    "\n",
    "from torchdata.dataloader2.dataloader2 import *\n",
    "from torchdata.dataloader2.reading_service import *\n",
    "from torchdata.dataloader2.reading_service import _IterateQueueDataPipes\n",
    "from torchdata.dataloader2.communication.protocol import *\n",
    "from torchdata.dataloader2.communication.messages import *\n",
    "from torchdata.dataloader2.communication.iter import EnsureNonBlockingDataPipe,NotAvailable,InvalidStateResetRequired\n",
    "from torch.utils.data import IterDataPipe, MapDataPipe\n",
    "# Local modules\n",
    "from fastrl.core import *\n",
    "from fastrl.agents.core import AgentBase\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.loggers.core import LoggerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f887fb7-6fab-4312-a5b7-0c9c275a1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DataPipeToQueuesLoop(source_datapipe, req_queue, res_queue, call_locally_fn=None, protocol_type=None, pipe_type=None):\n",
    "    if call_locally_fn is not None:\n",
    "        result = call_locally_fn(source_datapipe)\n",
    "        if result is not None: \n",
    "            source_datapipe = result\n",
    "        \n",
    "    if isinstance(source_datapipe, IterDataPipe):\n",
    "        if pipe_type is None:\n",
    "            pipe_type = communication.iter\n",
    "        if protocol_type is None:\n",
    "            protocol_type = communication.protocol.IterDataPipeQueueProtocolServer\n",
    "    elif isinstance(source_datapipe, MapDataPipe):\n",
    "        if pipe_type is None:\n",
    "            pipe_type = communication.map  # type: ignore[misc]\n",
    "        if protocol_type is None:\n",
    "            protocol_type = communication.protocol.MapDataPipeQueueProtocolServer  # type: ignore[assignment]\n",
    "    else:\n",
    "        raise Exception(\"Only supports IterDataPipe or MapDataPipe, got\", source_datapipe)\n",
    "\n",
    "    torch.set_num_threads(1)\n",
    "    for _ in pipe_type.DataPipeBehindQueues(\n",
    "        source_datapipe, protocol_type(req_queue, res_queue), blocking_request_get=True\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "\n",
    "def SpawnProcessForDataPipeline(multiprocessing_ctx, datapipe, call_locally_fn=None, protocol_type=None, pipe_type=None):\n",
    "    req_queue = multiprocessing_ctx.Queue()\n",
    "    res_queue = multiprocessing_ctx.Queue()\n",
    "    process = multiprocessing_ctx.Process(\n",
    "        target=DataPipeToQueuesLoop, args=(datapipe, req_queue, res_queue, call_locally_fn, protocol_type, pipe_type)\n",
    "    )\n",
    "    return process, req_queue, res_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030f151-cf8a-4d21-a96b-a2dd73a6abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# The opposite of the GetItemRequest/GetItemResponse api. We want to input items into the dataloader's processes\n",
    "class GetInputItemResponse(Response):\n",
    "    __slots__ = \"value\"\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    \n",
    "class GetInputItemRequest(Response):\n",
    "    __slots__ = (\"key\", \"value\")\n",
    "\n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee428112-87e0-49df-9375-a1566677a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class InputItemIterDataPipeQueueProtocolClient(communication.protocol.IterDataPipeQueueProtocolClient):\n",
    "    def request_input_item(self,key, value):\n",
    "        if not self.can_take_request():\n",
    "            raise Exception(\"Can not reset while we are still waiting response for previous request\")\n",
    "        request = GetInputItemRequest(key, value)\n",
    "        self.request_queue.put(request)\n",
    "        self.request_sent(request)\n",
    "\n",
    "    def get_response_input_item(self, block=False):\n",
    "        try:\n",
    "            response = self.response_queue.get(block=block)\n",
    "        except Exception:  # TODO(627): Catch only timeout exceptions\n",
    "            raise EmptyQueue(\"queue is empty\")\n",
    "        self.request_served(response)\n",
    "\n",
    "        if not isinstance(response, GetInputItemResponse):\n",
    "            raise Exception(\"Invalid response received\")\n",
    "            \n",
    "class InputItemIterDataPipeQueueProtocolServer(IterDataPipeQueueProtocolServer):\n",
    "    def response_input_item(self, key):\n",
    "        if not self.have_pending_request():\n",
    "            raise Exception(\"Attempting to reply with pending request\")\n",
    "        if not isinstance(self._req_received, GetInputItemRequest):\n",
    "            raise Exception(\"Replaying with reset status to other type of message\")\n",
    "        self.response_queue.put(GetInputItemResponse(\n",
    "            # We need this to make it all the way to the main process datapipe\n",
    "            # Without this, we get a string... which isnt great to build triggers / filters on.\n",
    "            GetInputItemResponse(key) \n",
    "        ))\n",
    "        self._req_received = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6d416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485d3d5-4075-453e-8fc2-2171335ea71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AgentLoggerMerger(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        try:\n",
    "            self.logger_bases = [o for o in find_dp(traverse(self),AgentBase).logger_bases]\n",
    "        except LookupError:\n",
    "            self.logger_bases = []\n",
    "        try:\n",
    "            self.logger_bases.extend([o for o in find_dp(traverse(self),LoggerBase)])\n",
    "        except LookupError:pass\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for value in self.source_datapipe:\n",
    "            for logger_base in self.logger_bases:\n",
    "                # print('iterating through logger bases',self.logger_bases)\n",
    "                for record in logger_base.dequeue():\n",
    "                    # print('Yielding record ',record)\n",
    "                    yield record\n",
    "            yield value\n",
    "add_docs(\n",
    "    AgentLoggerMerger,\n",
    "    \"\"\"Inserts values from `input_jests` into the current pipeline.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cbd80-ef53-4cb4-993f-18bf8ca08771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PrototypeMultiProcessingReadingService(ReadingServiceInterface):\n",
    "    num_workers: int\n",
    "    processes: List\n",
    "    datapipes: List\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_workers: int = 0,\n",
    "        multiprocessing_context=None,\n",
    "        protocol_client_type = None,\n",
    "        protocol_server_type = None,\n",
    "        pipe_type = None,\n",
    "        eventloop = None\n",
    "    ) -> None:\n",
    "        self.num_workers = num_workers\n",
    "        # TODO(613): Should be one of 'fork', 'spawn'\n",
    "        self.multiprocessing_context = multiprocessing_context\n",
    "        self.processes = []\n",
    "        self.datapipes = []\n",
    "        if protocol_client_type is None:\n",
    "            self.protocol_client_type = communication.protocol.IterDataPipeQueueProtocolClient\n",
    "        else:\n",
    "            self.protocol_client_type = protocol_client_type\n",
    "        self.protocol_server_type = protocol_server_type\n",
    "        # pipe_type (should) use self.protocol_server_type in a forever loop\n",
    "        self.pipe_type = pipe_type \n",
    "\n",
    "        if eventloop is None:\n",
    "            self.eventloop = communication.eventloop.SpawnProcessForDataPipeline\n",
    "        else:\n",
    "            self.eventloop = eventloop\n",
    "\n",
    "    @staticmethod\n",
    "    def init_datapipe_process(num_workers, worker_id, datapipe):\n",
    "        # TODO(614): Add distributed support\n",
    "        # TODO(615): Add shuffle determinism support\n",
    "        torch.utils.data.graph_settings.apply_sharding(datapipe, num_workers, worker_id)\n",
    "        return AgentLoggerMerger(datapipe)\n",
    "\n",
    "    def initialize(self, datapipe: DataPipe) -> DataPipe:\n",
    "        if self.num_workers == 0:\n",
    "            # TODO(616): Warn and recommend usage of InProcessReadingService\n",
    "            return datapipe\n",
    "        for worker_id in range(self.num_workers):\n",
    "            # TODO(617): Separate into function, because we also need to apply distributed seed\n",
    "            #            and call it inside process\n",
    "            call_inside_process = functools.partial(self.init_datapipe_process, self.num_workers, worker_id)\n",
    "            ctx = mp.get_context(self.multiprocessing_context)\n",
    "            (process, req_queue, res_queue) = self.eventloop(\n",
    "                ctx, datapipe, call_inside_process, self.protocol_server_type,self.pipe_type\n",
    "            )\n",
    "            process.start()\n",
    "            self.processes.append((process, req_queue, res_queue))  # These queues are independent\n",
    "            local_datapipe = communication.iter.QueueWrapper(\n",
    "                self.protocol_client_type(req_queue, res_queue)\n",
    "            )\n",
    "            self.datapipes.append(local_datapipe)\n",
    "\n",
    "        return IterableWrapper(_IterateQueueDataPipes(self.datapipes), deepcopy=False)  # type: ignore[return-value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e9bec-6978-44b4-bd01-2b4ffdd3d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class InputInjester(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.input_injests = []\n",
    "        \n",
    "    def __iter__(self): \n",
    "        for value in self.source_datapipe:\n",
    "            if self.input_injests:\n",
    "                for input_value in self.input_injests:\n",
    "                    yield input_value\n",
    "            yield value\n",
    "add_docs(\n",
    "    InputInjester,\n",
    "    \"\"\"Inserts values from `input_jests` into the current pipeline.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82314fe-f3af-4ddb-9660-ea23d0ca8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DataPipeBehindQueues(source_datapipe, protocol, full_stop=False, blocking_request_get=False):\n",
    "    \"\"\"\n",
    "    Indefinitely iterates over req_queue and passing values from source_datapipe to res_queue\n",
    "    If raise_stop is true, raises exception when StopIteration received from the source_datapipe\n",
    "    \"\"\"\n",
    "    if not isinstance(protocol, communication.protocol.IterDataPipeQueueProtocolServer):\n",
    "        raise Exception(\"Expecting IterDataPipeQueueProtocolServer, got\", protocol)\n",
    "    source_datapipe = EnsureNonBlockingDataPipe(source_datapipe)\n",
    "    input_injester_pipes = find_dps(traverse(source_datapipe),InputInjester)\n",
    "    forever = True\n",
    "    while forever:\n",
    "        try:\n",
    "            # Non-blocking call is Extremely slow here for python.mp, need to figure out a good workaround\n",
    "            request = protocol.get_new_request(block=blocking_request_get)\n",
    "        except communication.protocol.EmptyQueue:\n",
    "            yield True\n",
    "            continue\n",
    "        # Requires there to be InputInjester pipelines in `source_datapipe`\n",
    "        if isinstance(request, GetInputItemRequest):\n",
    "            for input_dp in input_injester_pipes: input_dp.input_injests.append(request)\n",
    "            protocol.response_input_item(request.key)\n",
    "            \n",
    "        elif isinstance(request, communication.messages.ResetIteratorRequest):\n",
    "            source_datapipe.reset_iterator()\n",
    "            protocol.response_reset_iterator()\n",
    "\n",
    "        elif isinstance(request, communication.messages.TerminateRequest):\n",
    "            forever = False\n",
    "            protocol.response_terminate()\n",
    "\n",
    "        elif isinstance(request, communication.messages.GetNextRequest):\n",
    "            while forever:\n",
    "                try:\n",
    "                    value = source_datapipe.nonblocking_next()\n",
    "                except NotAvailable:\n",
    "                    yield True\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    protocol.response_stop_iteration()\n",
    "                    if full_stop:\n",
    "                        forever = False\n",
    "                    else:\n",
    "                        yield True\n",
    "                    break\n",
    "                except InvalidStateResetRequired:\n",
    "                    protocol.response_invalid_state()\n",
    "                    if full_stop:\n",
    "                        forever = False\n",
    "                    else:\n",
    "                        yield True\n",
    "                    break\n",
    "                protocol.response_next(value)\n",
    "                \n",
    "                yield True  # Returns control\n",
    "                break\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized type of request received\", request)\n",
    "\n",
    "class item_input_pipe_type():\n",
    "    DataPipeBehindQueues = DataPipeBehindQueues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d76a55-09d6-4360-932e-0f5f9a36aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_service=PrototypeMultiProcessingReadingService(\n",
    "    num_workers = 1,\n",
    "    protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "    protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "    pipe_type = item_input_pipe_type,\n",
    "    eventloop = SpawnProcessForDataPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290e1eb-6f1c-48a1-8b04-6ac0a5902d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
