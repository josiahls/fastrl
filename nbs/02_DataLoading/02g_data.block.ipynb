{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from typing import Any,Callable,Generator\n",
    "from inspect import isfunction,ismethod\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torchdata.dataloader2.dataloader2 import DataLoader2\n",
    "from torchdata.dataloader2.graph import find_dps,traverse,DataPipe\n",
    "from fastai.torch_core import *\n",
    "from fastai.data.transforms import *\n",
    "import torchdata.datapipes as dp\n",
    "from collections import deque\n",
    "from fastai.imports import *\n",
    "# Local modules\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.core import *\n",
    "from fastrl.data.dataloader2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "> High level API to quickly get your data in a `DataLoader`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63392058-2c52-4282-94f7-3c01400845d9",
   "metadata": {},
   "source": [
    "## Transform Block\n",
    "> Loosely similar to the fastai==2.* `TransformBlock`, only this time, just like the fastrl **Agent** and **Learner**, is simply a *DataPipe* construction\n",
    "function with augmentation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b684484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# class TransformBlock():\n",
    "#     \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "#     def __init__(self, \n",
    "#         # A function that initializes a datapipeline and returns a datapipe.\n",
    "#         # Minimum must support:\n",
    "#         #\n",
    "#         #     `pipe_fn(source, bs, n)`\n",
    "#         #\n",
    "#         # Where:\n",
    "#         #   - `source` is the data to be input into the datapipes\n",
    "#         #   - `bs` is the batch size of the returned data\n",
    "#         #   - `n` is the number of iterations to make through the datapipes per epoch                 \n",
    "#         pipe_fn:Callable[[Any,int,int],_DataPipeMeta]=None, \n",
    "#         # One or more `Transform`s for converting types. These will be re-called if workers!=0 for the dataloader.\n",
    "#         type_tfms:list=None, \n",
    "#         item_tfms:list=None, # `ItemTransform`s, applied per peice of data (not batch)\n",
    "#         batch_tfms:list=None, # `Transform`s applied over a batch of data\n",
    "#         # `Callback`s for use in dataloaders. These usually augment a preexisting pipeline in some way\n",
    "#         cbs:list=None,\n",
    "#         pipe_fn_kwargs:dict=None, # Additional arguments to be passed to `pipe_fn`\n",
    "#         dl_type:DataLoader2=None, # Task specific `TfmdDL`, defaults to `TfmdDL`\n",
    "#         dls_kwargs:dict=None, # Additional arguments to be passed to `DataLoaders`\n",
    "#     ):\n",
    "#         self.type_tfms                   = L(type_tfms)\n",
    "#         self.item_tfms                   = L(item_tfms)\n",
    "#         self.batch_tfms                  = L(batch_tfms)\n",
    "#         self.pipe_fn,self.pipe_fn_kwargs = pipe_fn,ifnone(pipe_fn_kwargs,{})\n",
    "#         self.cbs                         = L(cbs)\n",
    "#         self.dl_type,self.dls_kwargs     = dl_type,ifnone(dls_kwargs,{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f5942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "DataPipeOrDataLoader = Union[DataPipe,DataLoader2]\n",
    "TransformBlock = Callable[[Union[Iterable,DataPipe]],DataPipeOrDataLoader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52424cc1",
   "metadata": {},
   "source": [
    "`DataBlock` as defined below expects single or tuples of `TransformBlock` callables. These functions need to have the above signature. The simplest example would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b675b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestTransformBlock(\n",
    "    a:int=1,\n",
    "    b:str='test',\n",
    "    dp_augmentation_fns:List[Callable]=None\n",
    ") -> Callable[[Any],DataPipeOrDataLoader]:\n",
    "    def _TestTransformBlock(\n",
    "        source,\n",
    "        num_workers:int=0,\n",
    "        as_dataloader:bool=False\n",
    "    ) -> DataPipeOrDataLoader:\n",
    "        pipe = dp.iter.IterableWrapper(range(10))\n",
    "        pipe = pipe.map(lambda o:o+a)\n",
    "        pipe = pipe.map(lambda o:str(o))\n",
    "        pipe = pipe.map(lambda o:o+b)\n",
    "\n",
    "        for fn in ifnone(dp_augmentation_fns,[]):\n",
    "            result = fn(pipe)\n",
    "            if result is not None: pipe = result\n",
    "        \n",
    "        if as_dataloader:\n",
    "            pipe = DataLoader2(\n",
    "                datapipe=pipe,\n",
    "                reading_service=PrototypeMultiProcessingReadingService(\n",
    "                    num_workers = num_workers,\n",
    "                    protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "                    protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "                    pipe_type = item_input_pipe_type,\n",
    "                    eventloop = SpawnProcessForDataPipeline\n",
    "                ) if num_workers>0 else None\n",
    "            )\n",
    "        return pipe \n",
    "    return _TestTransformBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b69adc7-2512-450d-9dc3-c12c1d5901ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_DataPipeMeta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5100/2023604235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#|export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDataBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__(\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# `TransformBlock`s where a single transform block is treated as a single dataloader / datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5100/2023604235.py\u001b[0m in \u001b[0;36mDataBlock\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mreturn_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     ) -> Generator[Union[Tuple[_DataPipeMeta,TransformBlock],_DataPipeMeta],None,None]:\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_DataPipeMeta' is not defined"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "class DataBlock(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # `TransformBlock`s where a single transform block is treated as a single dataloader / datapipe\n",
    "        # and a Tuple[TransformBlock] is also treated as a single dataloader of combined pipes.\n",
    "        blocks:List[TransformBlock]=None, \n",
    "    ):\n",
    "        store_attr(but='blocks')\n",
    "        self.blocks = L(blocks)\n",
    "\n",
    "    def datapipes(\n",
    "        self,\n",
    "        source:Any,\n",
    "        bs=1,\n",
    "        n=None,\n",
    "        return_blocks:bool=False\n",
    "    ) -> Generator[Union[Tuple[_DataPipeMeta,TransformBlock],_DataPipeMeta],None,None]:\n",
    "        for b in self.blocks:\n",
    "            pipe = b.pipe_fn(source,bs=bs,n=n,**b.pipe_fn_kwargs)\n",
    "            yield (pipe,b) if return_blocks else pipe\n",
    "        \n",
    "    def dataloaders(\n",
    "        self,\n",
    "        source:Any,\n",
    "        bs=1,\n",
    "        n=None,\n",
    "        num_workers=0,\n",
    "        **kwargs\n",
    "    ) -> Generator[DataLoader2,None,None]:\n",
    "        for pipe,block in self.datapipes(source,bs=bs,n=n,return_blocks=True,**kwargs):\n",
    "            yield block.dl_type(pipe,num_workers=num_workers,**merge(kwargs,block.dls_kwargs))\n",
    "\n",
    "add_docs(DataBlock,\n",
    "\"\"\"`DataBlock` is a single object for constructing datapipes and dataloaders from `TransformBlock`s.\"\"\",\n",
    "datapipes=\"\"\"Combines `self.blocks` with `source` where `bs` can be defined. `n=None` means that the datapipes are\n",
    "infinite / lengthless. If `n` is an integer then the datapipes will have an expected max len.\n",
    "\"\"\",\n",
    "dataloaders=\"Returns a dataloader for each respoctive combination of blocks.\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8d8e7-e153-41a3-ac31-22f936720d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
