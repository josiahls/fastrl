{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbverbose.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules\n",
    "from fastrl.fastai.loop import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Basic DataPipes\n",
    "> Basic datapipes for work with fastrl core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b200370-6438-4d8e-84b4-2d7cf82f9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('/home/fastrl_user/.fastai/data/mnist_tiny'),\n",
       " 'https://s3.amazonaws.com/fast-ai-sample/mnist_tiny.tgz')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, so not exported\n",
    "\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *\n",
    "\n",
    "untar_data(URLs.MNIST_TINY),URLs.MNIST_TINY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47fd2-3d79-4a6b-8fcd-473bc59c8416",
   "metadata": {},
   "source": [
    "Load the mnist csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63232a4c-fbc7-44a8-a125-dace28c27d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterToMapConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Data from prior DataPipe are loaded to get length of\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"IterToMapConverter before execution of the pipeline.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Please consider removing len().\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/src/torchdata/torchdata/datapipes/map/util/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp.map.IterToMapConverter.__len__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce704c6e-d420-4e2e-8f60-fd085b3e6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper([str(untar_data(URLs.MNIST_TINY)/'labels.csv')]) # FileOpener really should support Path as well as str\n",
    "pipe = dp.iter.FileOpener(pipe, mode=\"b\")\n",
    "pipe = dp.iter.CSVParser(pipe,skip_lines=1)\n",
    "\n",
    "class AddIdx():\n",
    "    def __init__(self): self.idx=0\n",
    "    def __call__(self,file):\n",
    "        try:     return (self.idx,file)\n",
    "        finally: self.idx+=1\n",
    "\n",
    "base_pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "# pipe[5],len(base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d7432d8-5f18-460c-a2e3-ab849b6d1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastrl_user/src/torchdata/torchdata/datapipes/map/util/utils.py:78: UserWarning: Data from prior DataPipe are loaded to get length ofIterToMapConverter before execution of the pipeline.Please consider removing len().\n",
      "  \"Data from prior DataPipe are loaded to get length of\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "072cb4ba-ffa3-468a-a337-1d46cc25ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [pipe[i] for i in range(len(pipe))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee557318-8dca-4978-9f3e-30b4b81aee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchdata.datapipes.map.util.utils.IterToMapConverterMapDataPipe at 0x7f990e096e10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364743a-2b26-4398-9501-e6ec0cdfbec4",
   "metadata": {},
   "source": [
    "Now that we have the csv converted into a map, we want to split it into a training and validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "655825f2-0e03-4bd1-a7b3-9ad5c1436e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator=iter({1,2,3,4})\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fa47b9ac-6049-414d-acc9-6272cd8afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import warnings\n",
    "\n",
    "from collections import deque\n",
    "from collections.abc import Hashable\n",
    "\n",
    "from typing import Any, Callable, Iterator, List, Optional, Set, Sized, Tuple, TypeVar, Deque\n",
    "\n",
    "from torch.utils.data import IterDataPipe, functional_datapipe\n",
    "from torch.utils.data.datapipes.utils.common import check_lambda_fn\n",
    "from torch.utils.data._utils.serialization import serialize_fn, deserialize_fn\n",
    "\n",
    "\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "\n",
    "\n",
    "class _ChildMapDataPipe(dp.map.MapDataPipe):\n",
    "    def __init__(self, main_datapipe, instance_id: Hashable):\n",
    "        required_attrs = [\"get_next_element_by_instance\", \"is_instance_started\", \"getitem_by_instance\"]\n",
    "        required_ops = [getattr(main_datapipe, attr) for attr in required_attrs]\n",
    "        if any(not callable(op) for op in required_ops):\n",
    "            raise NotImplementedError(f\"Main Datapipe must have methods {required_attrs} implemented.\")\n",
    "        self.main_datapipe = main_datapipe\n",
    "        self.instance_id = instance_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        # These is no concept of exhaustion of the 'main_datapipe'. We only need\n",
    "        # to run through it once, then use the cached indexes for querying.\n",
    "        return self.get_generator_by_instance(self.instance_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.main_datapipe.main_datapipe_exhausted:\n",
    "            warnings.warn(\n",
    "                \"Data from prior DataPipe are loaded to get length of\"\n",
    "                \"_ChildMapDataPipe before execution of the pipeline.\"\n",
    "                \"Please consider removing len().\"\n",
    "            )\n",
    "            return len(list(self.get_generator_by_instance(self.instance_id)))\n",
    "        # Need to be careful here,  the len of `_ChildMapDataPipe` will be <= len(self.main_datapipe)\n",
    "        return len(self.main_datapipe.get_instance_buffer(self.instance_id))\n",
    "\n",
    "    def get_generator_by_instance(self, instance_id: Hashable):\n",
    "        yield from self.main_datapipe.get_next_element_by_instance(self.instance_id)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"Gets an item from `self.main_datapipe` in `self.instance_id`\"\n",
    "        return self.main_datapipe.getitem_by_instance(self.instance_id, index)\n",
    "\n",
    "\n",
    "class _DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    def __init__(self, datapipe: dp.map.MapDataPipe[T_co], \n",
    "                 # num_instances: int,\n",
    "                 instance_keys: Hashable,\n",
    "                 classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool):\n",
    "        self.main_datapipe = datapipe\n",
    "        self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "        # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "        self.instance_keys = instance_keys\n",
    "        # The child buffers will store the indexes separated into their respective\n",
    "        # `_ChildMapDataPipe`'s\n",
    "        self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.drop_none = drop_none\n",
    "        self.main_datapipe_exhausted = False\n",
    "        \n",
    "    def _setup_datapipe_indexer(self) -> Optional[Iterator[Any]]:\n",
    "        # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "        # Instead of _datapipe_iterator we have _datapipe_indexer\n",
    "        # We need to know how to get the index from the main_datapipe. In order\n",
    "        # to do this, we check if it is...\n",
    "        \n",
    "        # NOTE: THIS IS NOT A GOOD SOLUTION SINCE THIS CANT RELY ON A STANDARD\n",
    "        # INTERFACE FOR GETTING INDEXES\n",
    "        \n",
    "        # We cash the indexes because we want to be able to have consistent behavior \n",
    "        # when calling __getitem__ on a child pipe. \n",
    "        # What we don't want is the main_datapipe being indexed by `str` but the\n",
    "        # child pipes indexing by `int`...\n",
    "        if isinstance(self.main_datapipe, dp.map.SequenceWrapper):\n",
    "            return range(len(self.main_datapipe))\n",
    "        elif hasattr(self.main_datapipe, '_map'):\n",
    "            return iter(self.main_datapipe._map)\n",
    "        elif hasattr(self.main_datapipe, 'index_map'):\n",
    "            return iter(self.main_datapipe.index_map)\n",
    "        else:\n",
    "            warnings.warn('data pipe will be indexed by len')\n",
    "            return range(len(self.main_datapipe))\n",
    "        \n",
    "    def get_instance_buffer(self, instance_id: Hashable):\n",
    "        return self.child_index_buffers[instance_id]\n",
    "\n",
    "    def _find_next(self, instance_id: Hashable) -> T_co:\n",
    "        while True:\n",
    "            if self.main_datapipe_exhausted:\n",
    "                raise StopIteration\n",
    "            if self._datapipe_indexer is None:\n",
    "                raise ValueError(\n",
    "                    \"_datapipe_indexer has not been set, likely because this private method is called directly \"\n",
    "                    \"without invoking get_next_element_by_instance() first.\")\n",
    "            index = next(self._datapipe_indexer)\n",
    "            value = self.main_datapipe[index]\n",
    "            classification = self.classifier_fn(value)\n",
    "            if classification is None and self.drop_none:\n",
    "                continue\n",
    "            if classification is None or classification not in self.instance_keys:\n",
    "                raise ValueError(f\"Output of the classification fn should be a key in {self.instance_keys}. \" +\n",
    "                                 f\"{classification} is returned.\")\n",
    "            \n",
    "            if index not in self.child_index_buffers[classification]:\n",
    "                self.child_index_buffers[classification].add(index)\n",
    "\n",
    "            if classification == instance_id:\n",
    "                return value,index\n",
    "            \n",
    "    def getitem_by_instance(self, instance_id: Hashable, index: Hashable):\n",
    "        # We need to handle the situation where the index is not currently cached.\n",
    "        # In this case we still need to build the cache, while still attempting to \n",
    "        # get the value for `index`\n",
    "        \n",
    "        # In this case, `main_datapipe_exhausted` which means we still have some\n",
    "        # of the cache to populate possibly.\n",
    "        # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "        if index in self.child_index_buffers[instance_id]:\n",
    "            return self.main_datapipe[index]\n",
    "        \n",
    "        if not self.main_datapipe_exhausted:\n",
    "            for _ in self.get_next_element_by_instance(instance_id):\n",
    "                if index in self.child_index_buffers[instance_id]:\n",
    "                    return self.main_datapipe[index]\n",
    "        \n",
    "        raise IndexError(f'Index {index} not found in {instance_id}')\n",
    "\n",
    "    def get_next_element_by_instance(self, instance_id: Hashable):\n",
    "        # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "        if self._datapipe_indexer is None and not self.main_datapipe_exhausted:\n",
    "            self._datapipe_indexer = iter(self._setup_datapipe_indexer())\n",
    "        stop = False\n",
    "        self.instance_started[instance_id] = True\n",
    "        instance_next_indexer = None\n",
    "        \n",
    "        while not stop:\n",
    "            # We only want to iterate through the indexes once `self._datapipe_indexer` is clear\n",
    "            # so that we are \"gaurenteed\" to go through all the indexes possible for \n",
    "            # instance_id\n",
    "            if self.child_index_buffers[instance_id] and self.main_datapipe_exhausted:\n",
    "                instance_next_indexer = self.child_index_buffers[instance_id]\n",
    "                yield from (self.main_datapipe[index] for index in instance_next_indexer)\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    value,index = self._find_next(instance_id)\n",
    "                    yield value\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    self.main_datapipe_exhausted = True\n",
    "                    self._datapipe_indexer = None\n",
    "                    \n",
    "    def is_instance_started(self, instance_id: Hashable) -> bool:\n",
    "        return self.instance_started[instance_id]\n",
    "\n",
    "    def reset(self):\n",
    "        self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "        self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.main_datapipe_exhausted = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        if IterDataPipe.getstate_hook is not None:\n",
    "            return IterDataPipe.getstate_hook(self)\n",
    "\n",
    "        serialized_fn_with_method = serialize_fn(self.classifier_fn)\n",
    "        state = (\n",
    "            self.main_datapipe,\n",
    "            self.instance_keys,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        (\n",
    "            self.main_datapipe,\n",
    "            self.num_instances,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        ) = state\n",
    "        self.classifier_fn = deserialize_fn(serialized_fn_with_method)\n",
    "        self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "        self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.main_datapipe_exhausted = False\n",
    "\n",
    "class DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    def __new__(cls, datapipe: dp.map.MapDataPipe, instance_keys: List[Hashable],\n",
    "                classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool = False):\n",
    "        if not isinstance(datapipe, dp.map.MapDataPipe):\n",
    "            raise TypeError(f\"DemultiplexerMapDataPipe can only apply on MapDataPipe, but found {type(datapipe)}\")\n",
    "        if not instance_keys:\n",
    "            raise ValueError(f\"Expected `instance_keys` larger than 0, but {instance_keys} is found\")\n",
    "\n",
    "        check_lambda_fn(classifier_fn)\n",
    "\n",
    "        container = _DemultiplexerMapDataPipe(datapipe, instance_keys, classifier_fn, drop_none)\n",
    "        return [_ChildMapDataPipe(container, k) for k in instance_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7eecf2d5-5103-4480-8d03-11f5d48fc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_splitter(o): \n",
    "    return Path(o[0]).parts[0]\n",
    "\n",
    "dp1, dp2 = DemultiplexerMapDataPipe(pipe,instance_keys=['train','valid'], classifier_fn=train_valid_splitter, drop_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c1ba19ed-c3ee-49e0-9b31-ca425f3dc2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Data from prior DataPipe are loaded to get length of_ChildMapDataPipe before execution of the pipeline.Please consider removing len().\n"
     ]
    }
   ],
   "source": [
    "assert len(dp1)+len(dp2)==len(pipe),f\"The demux'd dp1 and dp2 when added together should be the same len as pipe {len(dp1)} + {len(dp2)} = {len(pipe)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "822421bf-95de-4d02-84e1-2b39c9672e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['train/3/7463.png', '3'], ['valid/3/9614.png', '3'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp1[0],dp2[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f6b1a6c-f36b-40c3-a387-cc9c6a41e8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['train/3/7463.png', '3'],\n",
       "  ['train/3/9829.png', '3'],\n",
       "  ['train/3/7881.png', '3'],\n",
       "  ['train/3/8065.png', '3'],\n",
       "  ['train/3/7046.png', '3']],\n",
       " [['valid/3/8430.png', '3'],\n",
       "  ['valid/3/7946.png', '3'],\n",
       "  ['valid/3/933.png', '3'],\n",
       "  ['valid/3/9308.png', '3'],\n",
       "  ['valid/3/795.png', '3']])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dp1)[:5],list(dp2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24212e4e-88fb-4ce1-8946-b3d77fe16652",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid = MultiplexerMapDataPipe(pipe)\n",
    "\n",
    "assert 0 not in (len(train),len(valid))\n",
    "assert len(train)+len(valid)==len(base_pipe)\n",
    "\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n",
      "Converted 00_core.ipynb.\n",
      "Converted 00_nbdev_extension.ipynb.\n",
      "Converted 02_fastai.exception_test.ipynb.\n",
      "Converted 02a_fastai.loop.ipynb.\n",
      "Converted 02a_fastai.loop_initial.ipynb.\n",
      "Converted 02b_fastai.data.load.ipynb.\n",
      "Converted 02c_fastai.data.block.ipynb.\n",
      "Converted 02c_fastai.data.pipes.ipynb.\n",
      "Converted 03_callback.core.ipynb.\n",
      "Converted 04_agent.ipynb.\n",
      "Converted 05_data.test_async.ipynb.\n",
      "Converted 05a_data.block.ipynb.\n",
      "Converted 05b_data.gym.ipynb.\n",
      "Converted 06a_memory.experience_replay.ipynb.\n",
      "Converted 06f_memory.tensorboard.ipynb.\n",
      "Converted 10a_agents.dqn.core.ipynb.\n",
      "Converted 10b_agents.dqn.targets.ipynb.\n",
      "Converted 10c_agents.dqn.double.ipynb.\n",
      "Converted 10d_agents.dqn.dueling.ipynb.\n",
      "Converted 10e_agents.dqn.categorical.ipynb.\n",
      "Converted 11a_agents.policy_gradient.ppo.ipynb.\n",
      "Converted 20_test_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nbdev_template.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbverbose.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
