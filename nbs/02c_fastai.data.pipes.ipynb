{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbverbose.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules\n",
    "from fastrl.fastai.loop import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Basic DataPipes\n",
    "> Basic datapipes for work with fastrl core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b200370-6438-4d8e-84b4-2d7cf82f9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='344064' class='' max='342207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.54% [344064/342207 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Path('/home/fastrl_user/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, so not exported\n",
    "\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *\n",
    "\n",
    "untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47fd2-3d79-4a6b-8fcd-473bc59c8416",
   "metadata": {},
   "source": [
    "Load the mnist csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce704c6e-d420-4e2e-8f60-fd085b3e6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastrl_user/src/torchdata/torchdata/datapipes/map/util/utils.py:78: UserWarning: Data from prior DataPipe are loaded to get length ofIterToMapConverter before execution of the pipeline.Please consider removing len().\n",
      "  \"Data from prior DataPipe are loaded to get length of\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['train/3/7745.png', '3'], 1408)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = dp.iter.IterableWrapper([str(untar_data(URLs.MNIST_TINY)/'labels.csv')]) # FileOpener really should support Path as well as str\n",
    "pipe = dp.iter.FileOpener(pipe, mode=\"b\")\n",
    "pipe = dp.iter.CSVParser(pipe,skip_lines=1)\n",
    "\n",
    "class AddIdx():\n",
    "    def __init__(self): self.idx=0\n",
    "    def __call__(self,file):\n",
    "        try:     return (self.idx,file)\n",
    "        finally: self.idx+=1\n",
    "\n",
    "base_pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe[5],len(base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "072cb4ba-ffa3-468a-a337-1d46cc25ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [pipe[i] for i in range(len(pipe))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee557318-8dca-4978-9f3e-30b4b81aee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364743a-2b26-4398-9501-e6ec0cdfbec4",
   "metadata": {},
   "source": [
    "Now that we have the csv converted into a map, we want to split it into a training and validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655825f2-0e03-4bd1-a7b3-9ad5c1436e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator=iter({1,2,3,4})\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa47b9ac-6049-414d-acc9-6272cd8afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from collections import deque\n",
    "from typing import Any, Callable, Iterator, List, Optional, Set, Sized, Tuple, TypeVar, Deque\n",
    "\n",
    "from torch.utils.data import IterDataPipe, functional_datapipe\n",
    "from torch.utils.data.datapipes.utils.common import check_lambda_fn\n",
    "from torch.utils.data._utils.serialization import serialize_fn, deserialize_fn\n",
    "\n",
    "\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "from collections import deque\n",
    "from collections.abc import Hashable\n",
    "\n",
    "\n",
    "class _ChildMapDataPipe(dp.map.MapDataPipe):\n",
    "    r\"\"\"\n",
    "    Map Datapipe that is a child of a main DataPipe. The instance of this class\n",
    "    will pass its instance_id to get the next value from its main DataPipe.\n",
    "\n",
    "    Args:\n",
    "        main_datapipe: Main DataPipe with a method 'get_next_element_by_instance(instance_id)'\n",
    "        instance_id: integer identifier of this instance\n",
    "    \"\"\"\n",
    "    def __init__(self, main_datapipe, instance_id: Hashable):\n",
    "        \n",
    "        required_attrs = [\"get_next_element_by_instance\", \"is_instance_started\", \"is_every_instance_exhausted\", \"reset\"]\n",
    "        required_ops = [getattr(main_datapipe, attr) for attr in required_attrs]\n",
    "        if any(not callable(op) for op in required_ops):\n",
    "            raise NotImplementedError(f\"Main Datapipe must have methods {required_attrs} implemented.\")\n",
    "        self.main_datapipe = main_datapipe\n",
    "        self.instance_id = instance_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        # if self.main_datapipe.is_instance_started(self.instance_id):  # Only reset if the DataPipe started to read\n",
    "        #     if not self.main_datapipe.is_every_instance_exhausted():\n",
    "        #         warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
    "        #                       \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n",
    "        #     self.main_datapipe.reset()\n",
    "        \n",
    "        # These is no concept of exhaustion of the 'main_datapipe'. We only need\n",
    "        # to run through it once,, then use the cached indexes for querying.\n",
    "        return self.get_generator_by_instance(self.instance_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Need to be careful here,  the len of `_ChildMapDataPipe` will be <= len(self.main_datapipe)\n",
    "        return len(self.main_datapipe.get_instance_buffer(self.instance_id))\n",
    "\n",
    "    def get_generator_by_instance(self, instance_id: Hashable):\n",
    "        yield from self.main_datapipe.get_next_element_by_instance(self.instance_id)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"Gets an item from `self.main_datapipe` in `self.instance_id`\"\n",
    "        return self.main_datapipe.getitem_by_instance(self.instance_id, index)\n",
    "\n",
    "\n",
    "class _DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    r\"\"\"\n",
    "    Container to hold instance-specific information on behalf of _DemultiplexerMapDataPipe. It tracks\n",
    "    the state of its child DataPipes, maintains the buffer, classifies and yields the next correct value\n",
    "    as requested by the child DataPipes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datapipe: dp.map.MapDataPipe[T_co], \n",
    "                 # num_instances: int,\n",
    "                 instance_keys: Hashable,\n",
    "                 classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool, buffer_size: int):\n",
    "        self.main_datapipe = datapipe\n",
    "        self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "        # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "        self.instance_keys = instance_keys\n",
    "        self.buffer_size = buffer_size\n",
    "        if self.buffer_size < 0:\n",
    "            warnings.warn(\n",
    "                \"Unlimited buffer size is set for `demux`, \"\n",
    "                \"please be aware of OOM at random places\",\n",
    "                UserWarning\n",
    "            )\n",
    "        self.current_buffer_usage = 0\n",
    "        # The child buffers will store the indexes separated into their respective\n",
    "        # `_ChildMapDataPipe`'s\n",
    "        self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.drop_none = drop_none\n",
    "        self.main_datapipe_exhausted = False\n",
    "        \n",
    "    def _setup_datapipe_indexer(self) -> Optional[Iterator[Any]]:\n",
    "        # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "        # Instead of _datapipe_iterator we have _datapipe_indexer\n",
    "        # We need to know how to get the index from the main_datapipe. In order\n",
    "        # to do this, we check if it is...\n",
    "        \n",
    "        # NOTE: THIS IS NOT A GOOD SOLUTION.\n",
    "        \n",
    "        # We cash the indexes because we want to be able to have consistent behavior \n",
    "        # when calling __getitem__ on a child pipe. \n",
    "        # What we don't want is the main_datapipe being indexed by `str` but the\n",
    "        # child pipes indexing by `int`...\n",
    "        if isinstance(self.main_datapipe, dp.map.SequenceWrapper):\n",
    "            return range(len(self.main_datapipe))\n",
    "        elif hasattr(self.main_datapipe, '_map'):\n",
    "            return iter(self.main_datapipe._map)\n",
    "        elif hasattr(self.main_datapipe, 'index_map'):\n",
    "            return iter(self.main_datapipe.index_map)\n",
    "        else:\n",
    "            warnings.warn('data pipe will be indexed by len')\n",
    "            return range(len(self.main_datapipe))\n",
    "        \n",
    "    def get_instance_buffer(self, instance_id: Hashable):\n",
    "        return self.child_index_buffers[instance_id]\n",
    "\n",
    "    def _find_next(self, instance_id: Hashable) -> T_co:\n",
    "        while True:\n",
    "            if self.main_datapipe_exhausted:\n",
    "                raise StopIteration\n",
    "            if self._datapipe_indexer is None:\n",
    "                raise ValueError(\n",
    "                    \"_datapipe_indexer has not been set, likely because this private method is called directly \"\n",
    "                    \"without invoking get_next_element_by_instance() first.\")\n",
    "            index = next(self._datapipe_indexer)\n",
    "            value = self.main_datapipe[index]\n",
    "            classification = self.classifier_fn(value)\n",
    "            if classification is None and self.drop_none:\n",
    "                continue\n",
    "            if classification is None or classification not in self.instance_keys:\n",
    "                raise ValueError(f\"Output of the classification fn should be a key in {self.instance_keys}. \" +\n",
    "                                 f\"{classification} is returned.\")\n",
    "            \n",
    "            if index not in self.child_index_buffers[classification]:\n",
    "                self.child_index_buffers[classification].add(index)\n",
    "                self.current_buffer_usage += 1\n",
    "                if self.buffer_size >= 0 and self.current_buffer_usage > self.buffer_size:\n",
    "                    raise BufferError(\n",
    "                        f\"DemultiplexerIterDataPipe buffer overflow, buffer size {self.buffer_size} is insufficient.\")\n",
    "\n",
    "            if classification == instance_id:\n",
    "                return value,index\n",
    "\n",
    "    def getitem_by_instance(self, instance_id: Hashable, index: Hashable):\n",
    "        # We need to handle the situation where the index is not currently cached.\n",
    "        # In this case we still need to build the cache, while still attempting to \n",
    "        # get the value for `index`\n",
    "        \n",
    "        # In this case, `main_datapipe_exhausted` which means we still have some\n",
    "        # of the cache to populate possibly.\n",
    "        # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "        if index in self.child_index_buffers[instance_id]:\n",
    "            return self.main_datapipe[index]\n",
    "        \n",
    "        if self._datapipe_indexer is None and not self.main_datapipe_exhausted:\n",
    "            self._datapipe_indexer = iter(self._setup_datapipe_indexer())\n",
    "        stop = False\n",
    "        self.instance_started[instance_id] = True\n",
    "\n",
    "        while not stop:\n",
    "            # We only want to iterate through the indexes once `self._datapipe_indexer` is clear\n",
    "            # so that we are \"gaurenteed\" to go through all the indexes possible for \n",
    "            # instance_id\n",
    "            if self.child_index_buffers[instance_id] and self._datapipe_indexer is None:\n",
    "                return self.main_datapipe[index]\n",
    "            else:\n",
    "                try:\n",
    "                    value,_index = self._find_next(instance_id)\n",
    "                    if _index==index: return value\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    self.main_datapipe_exhausted = True\n",
    "                    self._datapipe_indexer = None\n",
    "    \n",
    "                    \n",
    "    def get_next_element_by_instance(self, instance_id: Hashable):\n",
    "        # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "        if self._datapipe_indexer is None and not self.main_datapipe_exhausted:\n",
    "            self._datapipe_indexer = iter(self._setup_datapipe_indexer())\n",
    "        stop = False\n",
    "        self.instance_started[instance_id] = True\n",
    "        instance_next_index = None\n",
    "        \n",
    "        while not stop:\n",
    "            # We only want to iterate through the indexes once `self._datapipe_indexer` is clear\n",
    "            # so that we are \"gaurenteed\" to go through all the indexes possible for \n",
    "            # instance_id\n",
    "            if self.child_index_buffers[instance_id] and self._datapipe_indexer is None:\n",
    "                if instance_next_index is None:\n",
    "                    instance_next_indexer = iter(self.child_index_buffers[instance_id])\n",
    "                index = next(instance_next_indexer)\n",
    "                yield self.main_datapipe[index]\n",
    "            else:\n",
    "                try:\n",
    "                    value,index = self._find_next(instance_id)\n",
    "                    yield value\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    self.main_datapipe_exhausted = True\n",
    "                    self._datapipe_indexer = None\n",
    "    \n",
    "\n",
    "    def is_instance_started(self, instance_id: Hashable) -> bool:\n",
    "        return self.instance_started[instance_id]\n",
    "\n",
    "    def is_every_instance_exhausted(self) -> bool:\n",
    "        return self.main_datapipe_exhausted and all(not child_buffer for child_buffer in self.child_index_buffers)\n",
    "\n",
    "    def reset(self):\n",
    "        self._datapipe_iterator = iter(self.main_datapipe)\n",
    "        self.current_buffer_usage = 0\n",
    "        self.child_index_buffers: Dict[Deque[T_co]] = {k:deque() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.main_datapipe_exhausted = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        if IterDataPipe.getstate_hook is not None:\n",
    "            return IterDataPipe.getstate_hook(self)\n",
    "\n",
    "        serialized_fn_with_method = serialize_fn(self.classifier_fn)\n",
    "        state = (\n",
    "            self.main_datapipe,\n",
    "            self.instance_keys,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        (\n",
    "            self.main_datapipe,\n",
    "            self.num_instances,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        ) = state\n",
    "        self.classifier_fn = deserialize_fn(serialized_fn_with_method)\n",
    "        self._datapipe_iterator = None\n",
    "        self.current_buffer_usage = 0\n",
    "        self.child_index_buffers: Dict[Deque[T_co]] = {k:deque() for k in self.instance_keys}\n",
    "        self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "        self.main_datapipe_exhausted = False\n",
    "        \n",
    "\n",
    "class DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    def __new__(cls, datapipe: dp.map.MapDataPipe, instance_keys: List[Hashable],\n",
    "                classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool = False, buffer_size: int = -1):\n",
    "        if not isinstance(datapipe, dp.map.MapDataPipe):\n",
    "            raise TypeError(f\"DemultiplexerMapDataPipe can only apply on MapDataPipe, but found {type(datapipe)}\")\n",
    "        if not instance_keys:\n",
    "            raise ValueError(f\"Expected `instance_keys` larger than 0, but {instance_keys} is found\")\n",
    "\n",
    "        check_lambda_fn(classifier_fn)\n",
    "\n",
    "        container = _DemultiplexerMapDataPipe(datapipe, instance_keys, classifier_fn, drop_none, buffer_size)\n",
    "        return [_ChildMapDataPipe(container, k) for k in instance_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7eecf2d5-5103-4480-8d03-11f5d48fc351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:78: UserWarning: Unlimited buffer size is set for `demux`, please be aware of OOM at random places\n"
     ]
    }
   ],
   "source": [
    "# It can also filter out any element that gets `None` from the `classifier_fn`\n",
    "\n",
    "def train_valid_splitter(o): \n",
    "    # print(o)\n",
    "    return Path(o[0]).parts[0]\n",
    "\n",
    "dp1, dp2 = DemultiplexerMapDataPipe(pipe,instance_keys=['train','valid'], classifier_fn=train_valid_splitter, drop_none=True)\n",
    "list(dp1);\n",
    "# [2, 4]\n",
    "# list(dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f6b1a6c-f36b-40c3-a387-cc9c6a41e8de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188/3173359871.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_typing.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__iter__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_188/4134906411.py\u001b[0m in \u001b[0;36mget_generator_by_instance\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_generator_by_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_datapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_element_by_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_188/4134906411.py\u001b[0m in \u001b[0;36mget_next_element_by_instance\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0minstance_next_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_index_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_next_indexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_datapipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/torchdata/torchdata/datapipes/map/util/utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list(dp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24212e4e-88fb-4ce1-8946-b3d77fe16652",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiplexerMapDataPipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188/3817575158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiplexerMapDataPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_pipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiplexerMapDataPipe' is not defined"
     ]
    }
   ],
   "source": [
    "train,valid = MultiplexerMapDataPipe(pipe)\n",
    "\n",
    "assert 0 not in (len(train),len(valid))\n",
    "assert len(train)+len(valid)==len(base_pipe)\n",
    "\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbverbose.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
