{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbverbose.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules\n",
    "from fastrl.fastai.loop import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Basic DataPipes\n",
    "> Basic datapipes for work with fastrl core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b200370-6438-4d8e-84b4-2d7cf82f9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/fastrl_user/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, so not exported\n",
    "\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *\n",
    "\n",
    "untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47fd2-3d79-4a6b-8fcd-473bc59c8416",
   "metadata": {},
   "source": [
    "Load the mnist csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ce704c6e-d420-4e2e-8f60-fd085b3e6300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['train/3/7745.png', '3'], 1408)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = dp.iter.IterableWrapper([str(untar_data(URLs.MNIST_TINY)/'labels.csv')]) # FileOpener really should support Path as well as str\n",
    "pipe = dp.iter.FileOpener(pipe, mode=\"b\")\n",
    "pipe = dp.iter.CSVParser(pipe,skip_lines=1)\n",
    "\n",
    "class AddIdx():\n",
    "    def __init__(self): self.idx=0\n",
    "    def __call__(self,file):\n",
    "        try:     return (self.idx,file)\n",
    "        finally: self.idx+=1\n",
    "\n",
    "base_pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe[5],len(base_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364743a-2b26-4398-9501-e6ec0cdfbec4",
   "metadata": {},
   "source": [
    "Now that we have the csv converted into a map, we want to split it into a training and validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fa47b9ac-6049-414d-acc9-6272cd8afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from collections import deque\n",
    "from typing import Any, Callable, Iterator, List, Optional, Set, Sized, Tuple, TypeVar, Deque\n",
    "\n",
    "from torch.utils.data import IterDataPipe, functional_datapipe\n",
    "from torch.utils.data.datapipes.utils.common import check_lambda_fn\n",
    "from torch.utils.data._utils.serialization import serialize_fn, deserialize_fn\n",
    "\n",
    "\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "from collections import deque\n",
    "from collections.abc import Hashable\n",
    "\n",
    "\n",
    "class _ChildDataPipe(dp.map.MapDataPipe):\n",
    "    r\"\"\"\n",
    "    Map Datapipe that is a child of a main DataPipe. The instance of this class\n",
    "    will pass its instance_id to get the next value from its main DataPipe.\n",
    "\n",
    "    Args:\n",
    "        main_datapipe: Main DataPipe with a method 'get_next_element_by_instance(instance_id)'\n",
    "        instance_id: integer identifier of this instance\n",
    "    \"\"\"\n",
    "    # def __init__(self, main_datapipe, instance_id: int):\n",
    "    def __init__(self, main_datapipe, instance_id: Hashable):\n",
    "        \n",
    "        required_attrs = [\"get_next_element_by_instance\", \"is_instance_started\", \"is_every_instance_exhausted\", \"reset\"]\n",
    "        required_ops = [getattr(main_datapipe, attr) for attr in required_attrs]\n",
    "        if any(not callable(op) for op in required_ops):\n",
    "            raise NotImplementedError(f\"Main Datapipe must have methods {required_attrs} implemented.\")\n",
    "        self.main_datapipe = main_datapipe\n",
    "        self.instance_id = instance_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.main_datapipe.is_instance_started(self.instance_id):  # Only reset if the DataPipe started to read\n",
    "            if not self.main_datapipe.is_every_instance_exhausted():\n",
    "                warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
    "                              \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n",
    "            self.main_datapipe.reset()\n",
    "        # We want to separate the code for reset and yield, so that 'reset' exeutes before __next__ is called\n",
    "        return self.get_generator_by_instance(self.instance_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_datapipe)\n",
    "\n",
    "    # def get_generator_by_instance(self, instance_id: int):\n",
    "    def get_generator_by_instance(self, instance_id: Hashable):\n",
    "        yield from self.main_datapipe.get_next_element_by_instance(self.instance_id)\n",
    "\n",
    "\n",
    "class _DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    r\"\"\"\n",
    "    Container to hold instance-specific information on behalf of _DemultiplexerMapDataPipe. It tracks\n",
    "    the state of its child DataPipes, maintains the buffer, classifies and yields the next correct value\n",
    "    as requested by the child DataPipes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datapipe: dp.map.MapDataPipe[T_co], num_instances: int,\n",
    "                 classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool, buffer_size: int):\n",
    "        self.main_datapipe = datapipe\n",
    "        self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "        self.num_instances = num_instances\n",
    "        self.buffer_size = buffer_size\n",
    "        if self.buffer_size < 0:\n",
    "            warnings.warn(\n",
    "                \"Unlimited buffer size is set for `demux`, \"\n",
    "                \"please be aware of OOM at random places\",\n",
    "                UserWarning\n",
    "            )\n",
    "        self.current_buffer_usage = 0\n",
    "        self.child_buffers: List[Deque[T_co]] = [deque() for _ in range(num_instances)]\n",
    "        self.instance_started: List[bool] = [False] * num_instances\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.drop_none = drop_none\n",
    "        self.main_datapipe_exhausted = False\n",
    "\n",
    "    def _find_next(self, instance_id: int) -> T_co:\n",
    "        while True:\n",
    "            if self.main_datapipe_exhausted:\n",
    "                raise StopIteration\n",
    "            if self._datapipe_iterator is None:\n",
    "                raise ValueError(\n",
    "                    \"_datapipe_iterator has not been set, likely because this private method is called directly \"\n",
    "                    \"without invoking get_next_element_by_instance() first.\")\n",
    "            value = next(self._datapipe_iterator)\n",
    "            classification = self.classifier_fn(value)\n",
    "            if classification is None and self.drop_none:\n",
    "                continue\n",
    "            if classification is None or classification >= self.num_instances or classification < 0:\n",
    "                raise ValueError(f\"Output of the classification fn should be between 0 and {self.num_instances - 1}. \" +\n",
    "                                 f\"{classification} is returned.\")\n",
    "            if classification == instance_id:\n",
    "                return value\n",
    "            self.child_buffers[classification].append(value)\n",
    "            self.current_buffer_usage += 1\n",
    "            if self.buffer_size >= 0 and self.current_buffer_usage > self.buffer_size:\n",
    "                raise BufferError(\n",
    "                    f\"DemultiplexerIterDataPipe buffer overflow, buffer size {self.buffer_size} is insufficient.\")\n",
    "\n",
    "    def get_next_element_by_instance(self, instance_id: Hashable):\n",
    "    # def get_next_element_by_instance(self, instance_id: int):\n",
    "        if self._datapipe_iterator is None and not self.main_datapipe_exhausted:\n",
    "            self._datapipe_iterator = iter(self.main_datapipe)\n",
    "        stop = False\n",
    "        self.instance_started[instance_id] = True\n",
    "        while not stop:\n",
    "            if self.child_buffers[instance_id]:\n",
    "                self.current_buffer_usage -= 1\n",
    "                yield self.child_buffers[instance_id].popleft()\n",
    "            else:\n",
    "                try:\n",
    "                    yield self._find_next(instance_id)\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    self.main_datapipe_exhausted = True\n",
    "                    self._datapipe_iterator = None\n",
    "\n",
    "    def is_instance_started(self, instance_id: int) -> bool:\n",
    "        return self.instance_started[instance_id]\n",
    "\n",
    "    def is_every_instance_exhausted(self) -> bool:\n",
    "        return self.main_datapipe_exhausted and all(not child_buffer for child_buffer in self.child_buffers)\n",
    "\n",
    "    def reset(self):\n",
    "        self._datapipe_iterator = iter(self.main_datapipe)\n",
    "        self.current_buffer_usage = 0\n",
    "        self.child_buffers = [deque() for _ in range(self.num_instances)]\n",
    "        self.instance_started = [False] * self.num_instances\n",
    "        self.main_datapipe_exhausted = False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        if IterDataPipe.getstate_hook is not None:\n",
    "            return IterDataPipe.getstate_hook(self)\n",
    "\n",
    "        serialized_fn_with_method = serialize_fn(self.classifier_fn)\n",
    "        state = (\n",
    "            self.main_datapipe,\n",
    "            self.num_instances,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        (\n",
    "            self.main_datapipe,\n",
    "            self.num_instances,\n",
    "            self.buffer_size,\n",
    "            serialized_fn_with_method,\n",
    "            self.drop_none,\n",
    "        ) = state\n",
    "        self.classifier_fn = deserialize_fn(serialized_fn_with_method)\n",
    "        self._datapipe_iterator = None\n",
    "        self.current_buffer_usage = 0\n",
    "        self.child_buffers = [deque() for _ in range(self.num_instances)]\n",
    "        self.instance_started = [False] * self.num_instances\n",
    "        self.main_datapipe_exhausted = False\n",
    "        \n",
    "\n",
    "class DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "    r\"\"\"\n",
    "    Splits the input DataPipe into multiple child DataPipes, using the given\n",
    "    classification function (functional name: ``demux``). A list of the child DataPipes is returned from this operation.\n",
    "\n",
    "    Args:\n",
    "        datapipe: Iterable DataPipe being filtered\n",
    "        num_instances: number of instances of the DataPipe to create\n",
    "        classifier_fn: a function that maps values to an integer within the range ``[0, num_instances - 1]`` or ``None``\n",
    "        drop_none: defaults to ``False``, if ``True``, the function will skip over elements classified as ``None``\n",
    "        \n",
    "        Josiah: buffer_size doesn't look like it should be needed(?)\n",
    "        buffer_size: this defines the maximum number of inputs that the buffer can hold across all child\n",
    "            DataPipes while waiting for their values to be yielded.\n",
    "            Defaults to ``1000``. Use ``-1`` for the unlimited buffer.\n",
    "\n",
    "    Examples:\n",
    "        >>> from torchdata.datapipes.iter import IterableWrapper\n",
    "        >>> def odd_or_even(n):\n",
    "        ...     return n % 2\n",
    "        >>> source_dp = IterableWrapper(range(5))\n",
    "        >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even)\n",
    "        >>> list(dp1)\n",
    "        [0, 2, 4]\n",
    "        >>> list(dp2)\n",
    "        [1, 3]\n",
    "        >>> # It can also filter out any element that gets `None` from the `classifier_fn`\n",
    "        >>> def odd_or_even_no_zero(n):\n",
    "        ...     return n % 2 if n != 0 else None\n",
    "        >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even_no_zero, drop_none=True)\n",
    "        >>> list(dp1)\n",
    "        [2, 4]\n",
    "        >>> list(dp2)\n",
    "        [1, 3]\n",
    "    \"\"\"\n",
    "    def __new__(cls, datapipe: dp.map.MapDataPipe, num_instances: int,\n",
    "                classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool = False, buffer_size: int = 1000):\n",
    "        if num_instances < 1:\n",
    "            raise ValueError(f\"Expected `num_instaces` larger than 0, but {num_instances} is found\")\n",
    "\n",
    "        check_lambda_fn(classifier_fn)\n",
    "\n",
    "        # When num_instances == 1, demux can be replaced by filter,\n",
    "        # but keep it as Demultiplexer for the sake of consistency\n",
    "        # like throwing Error when classification result is out of o range\n",
    "        container = _DemultiplexerIterDataPipe(datapipe, num_instances, classifier_fn, drop_none, buffer_size)\n",
    "        return [_ChildDataPipe(container, i) for i in range(num_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7eecf2d5-5103-4480-8d03-11f5d48fc351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train/3/7463.png', '3']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157/3674935931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDemultiplexerMapDataPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_valid_splitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_typing.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__iter__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_157/3193957576.py\u001b[0m in \u001b[0;36mget_generator_by_instance\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# def get_generator_by_instance(self, instance_id: int):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_generator_by_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_datapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_element_by_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_157/2960625740.py\u001b[0m in \u001b[0;36mget_next_element_by_instance\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_157/2960625740.py\u001b[0m in \u001b[0;36m_find_next\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_instances\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 raise ValueError(f\"Output of the classification fn should be between 0 and {self.num_instances - 1}. \" +\n\u001b[1;32m     43\u001b[0m                                  f\"{classification} is returned.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# It can also filter out any element that gets `None` from the `classifier_fn`\n",
    "\n",
    "def train_valid_splitter(o): \n",
    "    print(o)\n",
    "    return 'train'\n",
    "\n",
    "dp1, dp2 = DemultiplexerMapDataPipe(pipe,num_instances=2, classifier_fn=train_valid_splitter, drop_none=True)\n",
    "list(dp1)\n",
    "[2, 4]\n",
    "list(dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "24212e4e-88fb-4ce1-8946-b3d77fe16652",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157/3817575158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiplexerMapDataPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_pipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train,valid = MultiplexerMapDataPipe(pipe)\n",
    "\n",
    "assert 0 not in (len(train),len(valid))\n",
    "assert len(train)+len(valid)==len(base_pipe)\n",
    "\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n",
      "Converted 00_core.ipynb.\n",
      "Converted 00_nbdev_extension.ipynb.\n",
      "Converted 02_fastai.exception_test.ipynb.\n",
      "Converted 02a_fastai.loop.ipynb.\n",
      "Converted 02a_fastai.loop_initial.ipynb.\n",
      "Converted 02b_fastai.data.load.ipynb.\n",
      "Converted 02c_fastai.data.block.ipynb.\n",
      "Converted 02c_fastai.data.pipes.ipynb.\n",
      "Converted 03_callback.core.ipynb.\n",
      "Converted 04_agent.ipynb.\n",
      "Converted 05_data.test_async.ipynb.\n",
      "Converted 05a_data.block.ipynb.\n",
      "Converted 05b_data.gym.ipynb.\n",
      "Converted 06a_memory.experience_replay.ipynb.\n",
      "Converted 06f_memory.tensorboard.ipynb.\n",
      "Converted 10a_agents.dqn.core.ipynb.\n",
      "Converted 10b_agents.dqn.targets.ipynb.\n",
      "Converted 10c_agents.dqn.double.ipynb.\n",
      "Converted 10d_agents.dqn.dueling.ipynb.\n",
      "Converted 10e_agents.dqn.categorical.ipynb.\n",
      "Converted 11a_agents.policy_gradient.ppo.ipynb.\n",
      "Converted 20_test_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nbdev_template.ipynb.\n",
      "converting: /home/fastrl_user/fastrl/nbs/02a_fastai.loop.ipynb\n",
      "converting: /home/fastrl_user/fastrl/nbs/02c_fastai.data.block.ipynb\n",
      "converting: /home/fastrl_user/fastrl/nbs/02c_fastai.data.pipes.ipynb\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "from nbverbose.showdoc import show_doc\n",
      "from fastrl.fastai.loop import *\n",
      "------------------\n",
      "\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_233/2412008996.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n",
      "\u001b[0;31m    from fastrl.fastai.loop import *\u001b[0m\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/fastrl_user/fastrl/fastrl/fastai/loop.py\"\u001b[0;36m, line \u001b[0;32m149\u001b[0m\n",
      "\u001b[0;31m    print(section)\u001b[0m\n",
      "\u001b[0m        ^\u001b[0m\n",
      "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n",
      "\n",
      "IndentationError: expected an indented block (loop.py, line 149)\n",
      "\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "# export\n",
      "# Python native modules\n",
      "import os\n",
      "from inspect import isfunction,ismethod\n",
      "from typing import *\n",
      "# Third party libs\n",
      "from fastcore.all import *\n",
      "from fastai.torch_basics import *\n",
      "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
      "import torchdata.datapipes as dp\n",
      "from torch.utils.data.dataloader_experimental import DataLoader2\n",
      "from fastai.data.transforms import *\n",
      "# Local modules\n",
      "from fastrl.fastai.loop import *\n",
      "from fastrl.fastai.data.load import *\n",
      "------------------\n",
      "\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_245/786550491.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n",
      "\u001b[0;31m    from fastrl.fastai.loop import *\u001b[0m\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/fastrl_user/fastrl/fastrl/fastai/loop.py\"\u001b[0;36m, line \u001b[0;32m149\u001b[0m\n",
      "\u001b[0;31m    print(section)\u001b[0m\n",
      "\u001b[0m        ^\u001b[0m\n",
      "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n",
      "\n",
      "IndentationError: expected an indented block (loop.py, line 149)\n",
      "\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "from nbverbose.showdoc import show_doc\n",
      "from fastrl.fastai.data.pipes import *\n",
      "------------------\n",
      "\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_255/2042821812.py\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n",
      "    from fastrl.fastai.data.pipes import *\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/fastrl_user/fastrl/fastrl/fastai/data/pipes.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n",
      "\u001b[0;31m    from ..loop import *\u001b[0m\n",
      "\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/fastrl_user/fastrl/fastrl/fastai/loop.py\"\u001b[0;36m, line \u001b[0;32m149\u001b[0m\n",
      "\u001b[0;31m    print(section)\u001b[0m\n",
      "\u001b[0m        ^\u001b[0m\n",
      "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n",
      "\n",
      "IndentationError: expected an indented block (loop.py, line 149)\n",
      "\n",
      "Conversion failed on the following:\n",
      "02c_fastai.data.block.ipynb\n",
      "02a_fastai.loop.ipynb\n",
      "02c_fastai.data.pipes.ipynb\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbverbose.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()\n",
    "    notebook2html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
