---

title: TRPO


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/19_policy_gradient.trpo.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/19_policy_gradient.trpo.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModelActor" class="doc_header"><code>class</code> <code>ModelActor</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L427" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModelActor</code>(<strong><code>obs_size</code></strong>, <strong><code>act_size</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModelCritic" class="doc_header"><code>class</code> <code>ModelCritic</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L445" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModelCritic</code>(<strong><code>obs_size</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AgentA2C" class="doc_header"><code>class</code> <code>AgentA2C</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L461" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AgentA2C</code>(<strong><code>model</code></strong>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>) :: <a href="/fast-reinforcement-learning-2/basic_agents.html#BaseAgent"><code>BaseAgent</code></a></p>
</blockquote>
<p>BaseAgent(model: torch.nn.modules.module.Module = None)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="calc_logprob" class="doc_header"><code>calc_logprob</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L702" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>calc_logprob</code>(<strong><code>mu_v</code></strong>, <strong><code>logstd_v</code></strong>, <strong><code>actions_v</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="calc_adv_ref" class="doc_header"><code>calc_adv_ref</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L708" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>calc_adv_ref</code>(<strong><code>trajectory</code></strong>, <strong><code>net_crt</code></strong>, <strong><code>states_v</code></strong>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>)</p>
</blockquote>
<p>By trajectory calculate advantage and 1-step ref value
:param trajectory: trajectory list
:param net_crt: critic network
:param states_v: states tensor
:return: tuple with advantage numpy array and reference values</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_flat_params_from" class="doc_header"><code>get_flat_params_from</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L742" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_flat_params_from</code>(<strong><code>model</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="set_flat_params_to" class="doc_header"><code>set_flat_params_to</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L751" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>set_flat_params_to</code>(<strong><code>model</code></strong>, <strong><code>flat_params</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="conjugate_gradients" class="doc_header"><code>conjugate_gradients</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L760" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>conjugate_gradients</code>(<strong><code>Avp</code></strong>, <strong><code>b</code></strong>, <strong><code>nsteps</code></strong>, <strong><code>residual_tol</code></strong>=<em><code>1e-10</code></em>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="linesearch" class="doc_header"><code>linesearch</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L779" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>linesearch</code>(<strong><code>model</code></strong>, <strong><code>f</code></strong>, <strong><code>x</code></strong>, <strong><code>fullstep</code></strong>, <strong><code>expected_improve_rate</code></strong>, <strong><code>max_backtracks</code></strong>=<em><code>10</code></em>, <strong><code>accept_ratio</code></strong>=<em><code>0.1</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="trpo_step" class="doc_header"><code>trpo_step</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L799" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>trpo_step</code>(<strong><code>model</code></strong>, <strong><code>get_loss</code></strong>, <strong><code>get_kl</code></strong>, <strong><code>max_kl</code></strong>, <strong><code>damping</code></strong>, <strong><code>device</code></strong>=<em><code>'cpu'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_func" class="doc_header"><code>loss_func</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L834" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_func</code>(<strong>*<code>yb</code></strong>, <strong><code>learn</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TRPOTrainer" class="doc_header"><code>class</code> <code>TRPOTrainer</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L901" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TRPOTrainer</code>(<strong><code>after_create</code></strong>=<em><code>None</code></em>, <strong><code>before_fit</code></strong>=<em><code>None</code></em>, <strong><code>before_epoch</code></strong>=<em><code>None</code></em>, <strong><code>before_train</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_pred</code></strong>=<em><code>None</code></em>, <strong><code>after_loss</code></strong>=<em><code>None</code></em>, <strong><code>before_backward</code></strong>=<em><code>None</code></em>, <strong><code>before_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_step</code></strong>=<em><code>None</code></em>, <strong><code>after_step</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_train</code></strong>=<em><code>None</code></em>, <strong><code>after_train</code></strong>=<em><code>None</code></em>, <strong><code>before_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_validate</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_epoch</code></strong>=<em><code>None</code></em>, <strong><code>after_cancel_fit</code></strong>=<em><code>None</code></em>, <strong><code>after_fit</code></strong>=<em><code>None</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TRPOLearner" class="doc_header"><code>class</code> <code>TRPOLearner</code><a href="https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/fastrl/actorcritic/sac.py#L905" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TRPOLearner</code>(<strong><code>dls</code></strong>, <strong><code>agent</code></strong>=<em><code>None</code></em>, <strong><code>model</code></strong>=<em><code>None</code></em>, <strong><code>use_train_mets</code></strong>=<em><code>True</code></em>, <strong><code>loss_func</code></strong>=<em><code>None</code></em>, <strong><code>opt_func</code></strong>=<em><code>Adam</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>splitter</code></strong>=<em><code>trainable_params</code></em>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>metrics</code></strong>=<em><code>None</code></em>, <strong><code>path</code></strong>=<em><code>None</code></em>, <strong><code>model_dir</code></strong>=<em><code>'models'</code></em>, <strong><code>wd</code></strong>=<em><code>None</code></em>, <strong><code>wd_bn_bias</code></strong>=<em><code>False</code></em>, <strong><code>train_bn</code></strong>=<em><code>True</code></em>, <strong><code>moms</code></strong>=<em><code>(0.95, 0.85, 0.95)</code></em>) :: <a href="/fast-reinforcement-learning-2/learner.html#AgentLearner"><code>AgentLearner</code></a></p>
</blockquote>
<p>Base Learner for all reinforcement learning agents</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">pybulletgym</span>
<span class="n">env</span><span class="o">=</span><span class="s1">&#39;HalfCheetahPyBulletEnv-v0&#39;</span>
<span class="n">agent</span><span class="o">=</span><span class="n">AgentA2C</span><span class="p">(</span><span class="n">ModelActor</span><span class="p">(</span><span class="mi">26</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">()),</span> <span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">block</span><span class="o">=</span><span class="n">ExperienceBlock</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">dls_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;bs&#39;</span><span class="p">:</span><span class="mi">2049</span><span class="p">,</span><span class="s1">&#39;num_workers&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;verbose&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">,</span><span class="s1">&#39;indexed&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span><span class="s1">&#39;shuffle_train&#39;</span><span class="p">:</span><span class="kc">False</span><span class="p">})</span>
<span class="n">blk</span><span class="o">=</span><span class="n">IterableDataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">block</span><span class="p">),</span><span class="n">splitter</span><span class="o">=</span><span class="n">FuncSplitter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="kc">False</span><span class="p">))</span>
<span class="n">dls</span><span class="o">=</span><span class="n">blk</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">([</span><span class="n">env</span><span class="p">]</span><span class="o">*</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">2049</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">default_device</span><span class="p">())</span>

<span class="n">learner</span><span class="o">=</span><span class="n">TRPOLearner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">TRPOTrainer</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">AvgEpisodeRewardMetric</span><span class="p">(</span><span class="n">Experience</span><span class="p">)],</span><span class="n">max_step</span><span class="o">=</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env</span><span class="p">)</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">wd</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>WalkerBase::__init__
WalkerBase::__init__
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/envs/fastrl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: <span class="ansi-yellow-fg">WARN: Box bound precision lowered by casting to float32</span>
  warnings.warn(colorize(&#39;%s: %s&#39;%(&#39;WARN&#39;, msg % args), &#39;yellow&#39;))
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>train_avg_episode_r</th>
      <th>valid_loss</th>
      <th>valid_avg_episode_r</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.000000</td>
      <td>1.867225</td>
      <td>None</td>
      <td>1.867225</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.000000</td>
      <td>4.581237</td>
      <td>None</td>
      <td>4.581237</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.000000</td>
      <td>6.013871</td>
      <td>None</td>
      <td>6.013871</td>
      <td>00:40</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.000000</td>
      <td>6.744021</td>
      <td>None</td>
      <td>6.744021</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.000000</td>
      <td>7.103516</td>
      <td>None</td>
      <td>7.103516</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.000000</td>
      <td>7.459056</td>
      <td>None</td>
      <td>7.459056</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.000000</td>
      <td>7.711311</td>
      <td>None</td>
      <td>7.711311</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.000000</td>
      <td>8.075411</td>
      <td>None</td>
      <td>8.075411</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.000000</td>
      <td>8.610297</td>
      <td>None</td>
      <td>8.610297</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.000000</td>
      <td>9.437673</td>
      <td>None</td>
      <td>9.437673</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.000000</td>
      <td>11.331005</td>
      <td>None</td>
      <td>11.331005</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.000000</td>
      <td>12.907000</td>
      <td>None</td>
      <td>12.907000</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.000000</td>
      <td>14.910796</td>
      <td>None</td>
      <td>14.910796</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.000000</td>
      <td>17.457663</td>
      <td>None</td>
      <td>17.457663</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.000000</td>
      <td>20.343821</td>
      <td>None</td>
      <td>20.343821</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.000000</td>
      <td>23.993520</td>
      <td>None</td>
      <td>23.993520</td>
      <td>00:39</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.000000</td>
      <td>28.274211</td>
      <td>None</td>
      <td>28.274211</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.000000</td>
      <td>34.292387</td>
      <td>None</td>
      <td>34.292387</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.000000</td>
      <td>43.755539</td>
      <td>None</td>
      <td>43.755539</td>
      <td>00:38</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.000000</td>
      <td>66.928651</td>
      <td>None</td>
      <td>66.928651</td>
      <td>00:38</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/envs/fastrl/lib/python3.7/site-packages/ipykernel_launcher.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/opt/conda/envs/fastrl/lib/python3.7/site-packages/ipykernel_launcher.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
/opt/conda/envs/fastrl/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.
  warn(&#34;Your generator is empty.&#34;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

