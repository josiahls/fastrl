{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loggers.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os,typing\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torch.multiprocessing import Queue\n",
    "import torchdata.datapipes as dp\n",
    "from fastprogress.fastprogress import *\n",
    "# Local modules\n",
    "from fastrl.pipes.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Loggers Core\n",
    "> Core Utilties for logging in fastrl using Callbacks and DataPipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af94be53-536c-446d-8449-b415586a25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LoggerBase(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe=None):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = Queue()\n",
    "        \n",
    "    def connect_source_datapipe(self,pipe):\n",
    "        self.source_datapipe = pipe\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1713a8-0c6d-4c72-953d-8edede20684b",
   "metadata": {},
   "source": [
    "The `LoggerBase` class outlines simply the `main_queue`. It works in combo with `LogCollector` datapipe which will add to the `main_queue` even across processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea17189-d5bf-487a-a348-6cb9797ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LogCollector(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase] # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queues = [o.main_queue for o in logger_bases]\n",
    "        \n",
    "    def __iter__(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac79f4c-7e2b-45bf-bba5-99f3abdf050c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "User can init multiple different logger bases if they want\n",
    "\n",
    "We then can manually add Collectors, custom for certain pipes such as for collecting rewards. This can be put in \n",
    "full on pipelines even across workers.\n",
    "\n",
    "The manual process might not be desireable though, so I'm thinking of revising the `add_cbs_to_pipes` where\n",
    "the pipe itself literally says if it should go before, after, exclude from.\n",
    "\n",
    "This seems... easier than defining a callback, and keeps a level of flatness to the api.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e185243-5c23-4927-8896-c6a60cb5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def is_pipe_instance(pipe,cls): return isinstance(pipe,cls) \n",
    "def find_pipe_instance(main_pipe,pipe_cls):\n",
    "    return find_pipes(main_pipe,partial(is_pipe_instance,cls=pipe_cls))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "317033b3-e2e5-43a3-89e8-fd474fbb5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Record(typing.NamedTuple):\n",
    "    name:str\n",
    "    value:typing.Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562244df-e4e1-410e-bcb3-4487698effa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EpocherCollector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_507/3095165972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#|export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mProgressBarLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoggerBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__(self,\n\u001b[1;32m      4\u001b[0m                  \u001b[0;31m# This does not need to be immediately set since we need the `LogCollectors` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0;31m# first be able to reference its queues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/3095165972.py\u001b[0m in \u001b[0;36mProgressBarLogger\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                  \u001b[0;31m# For automatic pipe attaching, we can designate which pipe this should be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                  \u001b[0;31m# referneced for information on which epoch we are on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                  \u001b[0mepoch_on_pipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterDataPipe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEpocherCollector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                  \u001b[0;31m# For automatic pipe attaching, we can designate which pipe this should be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0;31m# referneced for information on which batch we are on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EpocherCollector' is not defined"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "class ProgressBarLogger(LoggerBase):\n",
    "    def __init__(self,\n",
    "                 # This does not need to be immediately set since we need the `LogCollectors` to \n",
    "                 # first be able to reference its queues.\n",
    "                 source_datapipe=None, \n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which epoch we are on\n",
    "                 epoch_on_pipe:dp.iter.IterDataPipe=EpocherCollector,\n",
    "                 # For automatic pipe attaching, we can designate which pipe this should be\n",
    "                 # referneced for information on which batch we are on\n",
    "                 batch_on_pipe:dp.iter.IterDataPipe=BatchCollector\n",
    "                ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_queue = Queue()\n",
    "        self.epoch_on_pipe = epoch_on_pipe\n",
    "        self.batch_on_pipe = batch_on_pipe\n",
    "        \n",
    "        self.collector_keys = None\n",
    "        self.attached_collectors = None\n",
    "    \n",
    "    def dequeue(self): \n",
    "        while not self.main_queue.empty(): yield self.main_queue.get()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        epocher = find_pipe_instance(self,self.epoch_on_pipe)\n",
    "        batcher = find_pipe_instance(self,self.batch_on_pipe)\n",
    "        mbar = master_bar(range(epocher.epochs)) \n",
    "        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)\n",
    "\n",
    "        mbar.update(0)\n",
    "        for i,record in enumerate(self.source_datapipe):\n",
    "            if i==0:\n",
    "                self.attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "                mbar.write(self.attached_collectors, table=True)\n",
    "                self.collector_keys = list(self.attached_collectors)\n",
    "                    \n",
    "            attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "            \n",
    "            if attached_collectors:\n",
    "                self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "            \n",
    "            if 'batch' in attached_collectors:\n",
    "                pbar.update(attached_collectors['batch'])\n",
    "                \n",
    "            if 'epoch' in attached_collectors:\n",
    "                mbar.update(attached_collectors['epoch'])\n",
    "                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "            \n",
    "\n",
    "            yield record\n",
    "\n",
    "        attached_collectors = {o.name:o.value for o in self.dequeue()}\n",
    "        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)\n",
    "\n",
    "        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}\n",
    "        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)\n",
    "\n",
    "        pbar.on_iter_end()\n",
    "        mbar.on_iter_end()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49764c20-370d-431d-88ab-6ecabdaadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RewardCollector(LogCollector):\n",
    "    def __iter__(self):\n",
    "        for q in self.main_queues: q.put(Record('reward',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_queues: q.put(Record('reward',step.reward.detach().numpy()))\n",
    "            else:\n",
    "                for q in self.main_queues: q.put(Record('reward',steps.reward.detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455bda-ec57-4beb-9485-4295537ab7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.envs.gym import *\n",
    "\n",
    "envs = ['CartPole-v1']*10\n",
    "\n",
    "logger_base = ProgressBarLogger(batches=18*len(envs),epoch_on_pipe=Epocher)\n",
    "\n",
    "pipe = dp.map.Mapper(envs)\n",
    "pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "pipe = dp.iter.MapToIterConverter(pipe)\n",
    "pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "pipe = pipe.cycle(count=(18*len(envs))) \n",
    "pipe = Epocher(pipe,epochs=5)\n",
    "# Turn off the seed so that some envs end before others...\n",
    "pipe = GymStepper(pipe,synchronized_reset=True)\n",
    "\n",
    "pipe = RewardCollector(pipe,[logger_base])\n",
    "\n",
    "# pipe = logger_base.connect_source_datapipe(pipe)\n",
    "# pipe = add_cbs_to_pipes(pipe,L(cb))\n",
    "# steps = list(pipe)\n",
    "steps = list(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8dae91-6f28-4256-975d-48783866322e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
