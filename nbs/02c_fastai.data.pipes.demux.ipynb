{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.pipes.demux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules\n",
    "from fastrl.fastai.loop import *\n",
    "from fastrl.fastai.data.load import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Basic DataPipes\n",
    "> Basic datapipes for work with fastrl core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b200370-6438-4d8e-84b4-2d7cf82f9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='344064' class='' max='342207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.54% [344064/342207 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Path('/home/fastrl_user/.fastai/data/mnist_tiny'),\n",
       " 'https://s3.amazonaws.com/fast-ai-sample/mnist_tiny.tgz')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, so not exported\n",
    "\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *\n",
    "\n",
    "untar_data(URLs.MNIST_TINY),URLs.MNIST_TINY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47fd2-3d79-4a6b-8fcd-473bc59c8416",
   "metadata": {},
   "source": [
    "Load the mnist csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce704c6e-d420-4e2e-8f60-fd085b3e6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper([str(untar_data(URLs.MNIST_TINY)/'labels.csv')]) # FileOpener really should support Path as well as str\n",
    "pipe = dp.iter.FileOpener(pipe, mode=\"b\")\n",
    "pipe = dp.iter.CSVParser(pipe,skip_lines=1)\n",
    "\n",
    "class AddIdx():\n",
    "    def __init__(self): self.idx=0\n",
    "    def __call__(self,file):\n",
    "        try:     return (self.idx,file)\n",
    "        finally: self.idx+=1\n",
    "\n",
    "base_pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "# pipe[5],len(base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7432d8-5f18-460c-a2e3-ab849b6d1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastrl_user/src/torchdata/torchdata/datapipes/map/util/utils.py:78: UserWarning: Data from prior DataPipe are loaded to get length ofIterToMapConverter before execution of the pipeline.Please consider removing len().\n",
      "  \"Data from prior DataPipe are loaded to get length of\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364743a-2b26-4398-9501-e6ec0cdfbec4",
   "metadata": {},
   "source": [
    "Now that we have the csv converted into a map, we want to split it into a training and validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655825f2-0e03-4bd1-a7b3-9ad5c1436e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator=iter({1,2,3,4})\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa47b9ac-6049-414d-acc9-6272cd8afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from typing import Callable, Dict, Iterable, Optional, TypeVar\n",
    "\n",
    "from torch.utils.data.datapipes._decorator import functional_datapipe\n",
    "from torch.utils.data.datapipes.datapipe import MapDataPipe\n",
    "\n",
    "from torch.utils.data.datapipes.utils.common import check_lambda_fn\n",
    "\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "\n",
    "\n",
    "@functional_datapipe(\"demux\")\n",
    "class DemultiplexerMapDataPipe:\n",
    "    def __new__(cls, datapipe: MapDataPipe, num_instances: int, classifier_fn: Callable, drop_none: bool = False,\n",
    "                source_index: Optional[Iterable] = None):\n",
    "        if num_instances < 1:\n",
    "            raise ValueError(f\"Expected `num_instances` larger than 0, but {num_instances} is found\")\n",
    "        check_lambda_fn(classifier_fn)\n",
    "        container = _DemultiplexerMapDataPipe(datapipe, num_instances, classifier_fn, drop_none, source_index)\n",
    "        return [_DemultiplexerChildMapDataPipe(container, i) for i in range(num_instances)]\n",
    "\n",
    "\n",
    "class _DemultiplexerMapDataPipe:\n",
    "    def __init__(\n",
    "        self,\n",
    "        datapipe: MapDataPipe[T_co],\n",
    "        num_instances: int,\n",
    "        classifier_fn: Callable[[T_co], Optional[int]],\n",
    "        drop_none: bool,\n",
    "        source_index: Optional[Iterable],\n",
    "    ):\n",
    "        self.main_datapipe = datapipe\n",
    "        self.num_instances = num_instances\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.drop_none = drop_none\n",
    "        self.iterator = None\n",
    "        self.exhausted = False  # Once we iterate through `main_datapipe` once, we know all the index mapping\n",
    "        self.index_mapping = [[] for _ in range(num_instances)]\n",
    "        self.source_index = source_index  # if None, assume `main_datapipe` 0-index\n",
    "\n",
    "    def _classify_next(self):\n",
    "        if self.source_index is None:\n",
    "            self.source_index = range(len(self.main_datapipe))\n",
    "        if self.iterator is None:\n",
    "            self.iterator = iter(self.source_index)\n",
    "        try:\n",
    "            next_source_idx = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.exhausted = True\n",
    "            return\n",
    "        value = self.main_datapipe[next_source_idx]\n",
    "        classification = self.classifier_fn(value)\n",
    "        if classification is None and self.drop_none:\n",
    "            self._classify_next()\n",
    "        else:\n",
    "            self.index_mapping[classification].append(value)\n",
    "\n",
    "    def classify_all(self):\n",
    "        while not self.exhausted:\n",
    "            self._classify_next()\n",
    "\n",
    "    def get_value(self, instance_id: int, index: int) -> T_co:\n",
    "        while not self.exhausted and len(self.index_mapping[instance_id]) <= index:\n",
    "            self._classify_next()\n",
    "        if len(self.index_mapping[instance_id]) > index:\n",
    "            return self.index_mapping[instance_id][index]\n",
    "        raise RuntimeError(\"Index is out of bound.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_datapipe)\n",
    "\n",
    "\n",
    "class _DemultiplexerChildMapDataPipe(MapDataPipe):\n",
    "    def __init__(self, main_datapipe: _DemultiplexerMapDataPipe, instance_id: int):\n",
    "        self.main_datapipe: _DemultiplexerMapDataPipe = main_datapipe\n",
    "        self.instance_id = instance_id\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.main_datapipe.get_value(self.instance_id, index)\n",
    "\n",
    "    def __len__(self):\n",
    "        self.main_datapipe.classify_all()  # You have to read through the entirety of main_datapipe to know `len`\n",
    "        return len(self.main_datapipe.index_mapping[self.instance_id])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eecf2d5-5103-4480-8d03-11f5d48fc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_splitter(o): \n",
    "    int_mapping={'train':0,'valid':1}\n",
    "    return int_mapping[Path(o[0]).parts[0]]\n",
    "\n",
    "dp1, dp2 = DemultiplexerMapDataPipe(pipe,num_instances=2, classifier_fn=train_valid_splitter, drop_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1ba19ed-c3ee-49e0-9b31-ca425f3dc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dp1)+len(dp2)==len(pipe),f\"The demux'd dp1 and dp2 when added together should be the same len as pipe {len(dp1)} + {len(dp2)} = {len(pipe)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "822421bf-95de-4d02-84e1-2b39c9672e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['train/3/9680.png', '3'], ['valid/3/9219.png', '3'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp1[60],dp2[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6b1a6c-f36b-40c3-a387-cc9c6a41e8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['train/3/7463.png', '3'],\n",
       "  ['train/3/9829.png', '3'],\n",
       "  ['train/3/7881.png', '3'],\n",
       "  ['train/3/8065.png', '3'],\n",
       "  ['train/3/7046.png', '3']],\n",
       " [['valid/3/8430.png', '3'],\n",
       "  ['valid/3/7946.png', '3'],\n",
       "  ['valid/3/933.png', '3'],\n",
       "  ['valid/3/9308.png', '3'],\n",
       "  ['valid/3/795.png', '3']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dp1)[:5],list(dp2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "177ad2be-919f-404b-8a01-08352051b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSplitter():\n",
    "    k=0\n",
    "    def __init__(self,k_splits=2): self.k_splits=k_splits\n",
    "    def __call__(self,*args):\n",
    "        try: \n",
    "            return self.k\n",
    "        finally: \n",
    "            self.k+=1\n",
    "            if self.k==self.k_splits: self.k=0\n",
    "\n",
    "k1,k2,k3 = DemultiplexerMapDataPipe(dp1,num_instances=3, classifier_fn=KSplitter(k_splits=3), drop_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb514d77-7623-47d4-ae48-cbc1f903252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 236, 236)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k1),len(k2),len(k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting /home/fastrl_user/fastrl/nbs/index.ipynb to README.md\n",
      "Converted 00_core.ipynb.\n",
      "Converted 00_nbdev_extension.ipynb.\n",
      "Converted 02_fastai.exception_test.ipynb.\n",
      "Converted 02a_fastai.loop.ipynb.\n",
      "Converted 02a_fastai.loop_initial.ipynb.\n",
      "Converted 02b_fastai.data.load.ipynb.\n",
      "Converted 02c_fastai.data.block.ipynb.\n",
      "Converted 02c_fastai.data.pipes.demux.ipynb.\n",
      "Converted 02c_fastai.data.pipes.mux.ipynb.\n",
      "Converted 03_callback.core.ipynb.\n",
      "Converted 04_agent.ipynb.\n",
      "Converted 05_data.test_async.ipynb.\n",
      "Converted 05a_data.block.ipynb.\n",
      "Converted 05b_data.gym.ipynb.\n",
      "Converted 06a_memory.experience_replay.ipynb.\n",
      "Converted 06f_memory.tensorboard.ipynb.\n",
      "Converted 10a_agents.dqn.core.ipynb.\n",
      "Converted 10b_agents.dqn.targets.ipynb.\n",
      "Converted 10c_agents.dqn.double.ipynb.\n",
      "Converted 10d_agents.dqn.dueling.ipynb.\n",
      "Converted 10e_agents.dqn.categorical.ipynb.\n",
      "Converted 11a_agents.policy_gradient.ppo.ipynb.\n",
      "Converted 20_test_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nbdev_template.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbdev.cli import *\n",
    "    make_readme()\n",
    "    notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import warnings\n",
    "\n",
    "# from collections import deque\n",
    "# from collections.abc import Hashable\n",
    "\n",
    "# from typing import Any, Callable, Iterator, List, Optional, Set, Sized, Tuple, TypeVar, Deque\n",
    "\n",
    "# from torch.utils.data import IterDataPipe, functional_datapipe\n",
    "# from torch.utils.data.datapipes.utils.common import check_lambda_fn\n",
    "# from torch.utils.data._utils.serialization import serialize_fn, deserialize_fn\n",
    "\n",
    "\n",
    "# T_co = TypeVar(\"T_co\", covariant=True)\n",
    "\n",
    "\n",
    "\n",
    "# class _ChildMapDataPipe(dp.map.MapDataPipe):\n",
    "#     def __init__(self, main_datapipe, instance_id: Hashable):\n",
    "#         required_attrs = [\"get_next_element_by_instance\", \"is_instance_started\", \"getitem_by_instance\"]\n",
    "#         required_ops = [getattr(main_datapipe, attr) for attr in required_attrs]\n",
    "#         if any(not callable(op) for op in required_ops):\n",
    "#             raise NotImplementedError(f\"Main Datapipe must have methods {required_attrs} implemented.\")\n",
    "#         self.main_datapipe = main_datapipe\n",
    "#         self.instance_id = instance_id\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         # These is no concept of exhaustion of the 'main_datapipe'. We only need\n",
    "#         # to run through it once, then use the cached indexes for querying.\n",
    "#         return self.get_generator_by_instance(self.instance_id)\n",
    "    \n",
    "#     @property\n",
    "#     def _map(self): \n",
    "#         return self.main_datapipe.child_index_buffers[self.instance_id]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         if not self.main_datapipe.main_datapipe_exhausted:\n",
    "#             warnings.warn(\n",
    "#                 \"Data from prior DataPipe are loaded to get length of\"\n",
    "#                 \"_ChildMapDataPipe before execution of the pipeline.\"\n",
    "#                 \"Please consider removing len().\"\n",
    "#             )\n",
    "#             return len(list(self.get_generator_by_instance(self.instance_id)))\n",
    "#         # Need to be careful here,  the len of `_ChildMapDataPipe` will be <= len(self.main_datapipe)\n",
    "#         return len(self.main_datapipe.get_instance_buffer(self.instance_id))\n",
    "\n",
    "#     def get_generator_by_instance(self, instance_id: Hashable):\n",
    "#         yield from self.main_datapipe.get_next_element_by_instance(self.instance_id)\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         \"Gets an item from `self.main_datapipe` in `self.instance_id`\"\n",
    "#         return self.main_datapipe.getitem_by_instance(self.instance_id, index)\n",
    "\n",
    "\n",
    "# class _DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "#     def __init__(self, datapipe: dp.map.MapDataPipe[T_co], \n",
    "#                  # num_instances: int,\n",
    "#                  instance_keys: Hashable,\n",
    "#                  classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool):\n",
    "#         self.main_datapipe = datapipe\n",
    "#         self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "#         # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "#         self.instance_keys = instance_keys\n",
    "#         # The child buffers will store the indexes separated into their respective\n",
    "#         # `_ChildMapDataPipe`'s\n",
    "#         self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "#         self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "#         self.classifier_fn = classifier_fn\n",
    "#         self.drop_none = drop_none\n",
    "#         self.main_datapipe_exhausted = False\n",
    "        \n",
    "#     def _setup_datapipe_indexer(self) -> Optional[Iterator[Any]]:\n",
    "#         # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "#         # Instead of _datapipe_iterator we have _datapipe_indexer\n",
    "#         # We need to know how to get the index from the main_datapipe. In order\n",
    "#         # to do this, we check if it is...\n",
    "        \n",
    "#         # NOTE: THIS IS NOT A GOOD SOLUTION SINCE THIS CANT RELY ON A STANDARD\n",
    "#         # INTERFACE FOR GETTING INDEXES\n",
    "        \n",
    "#         # We cash the indexes because we want to be able to have consistent behavior \n",
    "#         # when calling __getitem__ on a child pipe. \n",
    "#         # What we don't want is the main_datapipe being indexed by `str` but the\n",
    "#         # child pipes indexing by `int`...\n",
    "#         if isinstance(self.main_datapipe, dp.map.SequenceWrapper):\n",
    "#             return iter(range(len(self.main_datapipe)))\n",
    "#         elif hasattr(self.main_datapipe, '_map'):\n",
    "#             return iter(self.main_datapipe._map)\n",
    "#         elif hasattr(self.main_datapipe, 'index_map'):\n",
    "#             return iter(self.main_datapipe.index_map)\n",
    "#         else:\n",
    "#             warnings.warn('data pipe will be indexed by len')\n",
    "#             return iter(range(len(self.main_datapipe)))\n",
    "        \n",
    "#     def get_instance_buffer(self, instance_id: Hashable):\n",
    "#         return self.child_index_buffers[instance_id]\n",
    "\n",
    "#     def _find_next(self, instance_id: Hashable) -> T_co:\n",
    "#         while True:\n",
    "#             if self.main_datapipe_exhausted:\n",
    "#                 raise StopIteration\n",
    "#             if self._datapipe_indexer is None:\n",
    "#                 raise ValueError(\n",
    "#                     \"_datapipe_indexer has not been set, likely because this private method is called directly \"\n",
    "#                     \"without invoking get_next_element_by_instance() first.\")\n",
    "#             index = next(self._datapipe_indexer)\n",
    "#             value = self.main_datapipe[index]\n",
    "#             classification = self.classifier_fn(value)\n",
    "#             if classification is None and self.drop_none:\n",
    "#                 continue\n",
    "#             if classification is None or classification not in self.instance_keys:\n",
    "#                 raise ValueError(f\"Output of the classification fn should be a key in {self.instance_keys}. \" +\n",
    "#                                  f\"{classification} is returned.\")\n",
    "            \n",
    "#             if index not in self.child_index_buffers[classification]:\n",
    "#                 self.child_index_buffers[classification].add(index)\n",
    "\n",
    "#             if classification == instance_id:\n",
    "#                 return value,index\n",
    "            \n",
    "#     def getitem_by_instance(self, instance_id: Hashable, index: Hashable):\n",
    "#         # We need to handle the situation where the index is not currently cached.\n",
    "#         # In this case we still need to build the cache, while still attempting to \n",
    "#         # get the value for `index`\n",
    "        \n",
    "#         # In this case, `main_datapipe_exhausted` which means we still have some\n",
    "#         # of the cache to populate possibly.\n",
    "#         # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "#         if index in self.child_index_buffers[instance_id]:\n",
    "#             return self.main_datapipe[index]\n",
    "        \n",
    "#         if not self.main_datapipe_exhausted:\n",
    "#             for _ in self.get_next_element_by_instance(instance_id):\n",
    "#                 if index in self.child_index_buffers[instance_id]:\n",
    "#                     return self.main_datapipe[index]\n",
    "        \n",
    "#         raise IndexError(f'Index {index} not found in {instance_id}')\n",
    "\n",
    "#     def get_next_element_by_instance(self, instance_id: Hashable):\n",
    "#         # Josiah: The main_datapipe_exhausted doesnt make sense in this context.\n",
    "#         if self._datapipe_indexer is None and not self.main_datapipe_exhausted:\n",
    "#             self._datapipe_indexer = iter(self._setup_datapipe_indexer())\n",
    "#         stop = False\n",
    "#         self.instance_started[instance_id] = True\n",
    "#         instance_next_indexer = None\n",
    "        \n",
    "#         while not stop:\n",
    "#             # We only want to iterate through the indexes once `self._datapipe_indexer` is clear\n",
    "#             # so that we are \"gaurenteed\" to go through all the indexes possible for \n",
    "#             # instance_id\n",
    "#             if self.child_index_buffers[instance_id] and self.main_datapipe_exhausted:\n",
    "#                 instance_next_indexer = self.child_index_buffers[instance_id]\n",
    "#                 yield from (self.main_datapipe[index] for index in instance_next_indexer)\n",
    "#                 break\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     value,index = self._find_next(instance_id)\n",
    "#                     yield value\n",
    "#                 except StopIteration:\n",
    "#                     stop = True\n",
    "#                     self.main_datapipe_exhausted = True\n",
    "#                     self._datapipe_indexer = None\n",
    "                    \n",
    "#     def is_instance_started(self, instance_id: Hashable) -> bool:\n",
    "#         return self.instance_started[instance_id]\n",
    "\n",
    "#     def reset(self):\n",
    "#         self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "#         self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "#         self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "#         self.main_datapipe_exhausted = False\n",
    "\n",
    "#     def __getstate__(self):\n",
    "#         if IterDataPipe.getstate_hook is not None:\n",
    "#             return IterDataPipe.getstate_hook(self)\n",
    "\n",
    "#         serialized_fn_with_method = serialize_fn(self.classifier_fn)\n",
    "#         state = (\n",
    "#             self.main_datapipe,\n",
    "#             self.instance_keys,\n",
    "#             self.buffer_size,\n",
    "#             serialized_fn_with_method,\n",
    "#             self.drop_none,\n",
    "#         )\n",
    "#         return state\n",
    "\n",
    "#     def __setstate__(self, state):\n",
    "#         (\n",
    "#             self.main_datapipe,\n",
    "#             self.num_instances,\n",
    "#             self.buffer_size,\n",
    "#             serialized_fn_with_method,\n",
    "#             self.drop_none,\n",
    "#         ) = state\n",
    "#         self.classifier_fn = deserialize_fn(serialized_fn_with_method)\n",
    "#         self._datapipe_indexer: Optional[Iterator[Any]] = None\n",
    "#         self.child_index_buffers: Dict[set[T_co]] = {k:set() for k in self.instance_keys}\n",
    "#         self.instance_started: Dict[Hashable,bool] = {k:False for k in instance_keys}\n",
    "#         self.main_datapipe_exhausted = False\n",
    "\n",
    "# class DemultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "#     def __new__(cls, datapipe: dp.map.MapDataPipe, instance_keys: List[Hashable],\n",
    "#                 classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool = False):\n",
    "#         if not isinstance(datapipe, dp.map.MapDataPipe):\n",
    "#             raise TypeError(f\"DemultiplexerMapDataPipe can only apply on MapDataPipe, but found {type(datapipe)}\")\n",
    "#         if not instance_keys:\n",
    "#             raise ValueError(f\"Expected `instance_keys` larger than 0, but {instance_keys} is found\")\n",
    "\n",
    "#         check_lambda_fn(classifier_fn)\n",
    "\n",
    "#         container = _DemultiplexerMapDataPipe(datapipe, instance_keys, classifier_fn, drop_none)\n",
    "#         return [_ChildMapDataPipe(container, k) for k in instance_keys]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
