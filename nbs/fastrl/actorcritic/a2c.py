# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/16_actorcritic.a2c.ipynb (unless otherwise specified).

__all__ = ['LinearA2C', 'unbatch', 'A2CTrainer', 'loss_func', 'A2CLearner']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from ..ptan_extension import *

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class LinearA2C(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(LinearA2C, self).__init__()

        self.policy = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, n_actions)
        )

        self.value = nn.Sequential(
            nn.Linear(input_shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self,x):
        fx=x.float()
        return self.policy(fx),self.value(fx)

# Cell
def unbatch(batch, net, val_gamma,device='cpu'):
    states = []
    actions = []
    rewards = []
    not_done_idx = []
    last_states = []
    for idx, exp in enumerate(batch):
#         print(exp.state.numpy().shape,int(exp.action),float(exp.reward),exp.last_state.numpy().shape if not bool(exp.done) else None,exp.done)
        states.append(np.array(exp.state.cpu().detach().numpy(), copy=False))
        actions.append(int(exp.action.cpu().detach()))
        rewards.append(float(exp.reward.cpu().detach()))
        if not exp.done:
            not_done_idx.append(idx)
            last_states.append(np.array(exp.last_state.cpu().detach().numpy(), copy=False))
    states_v = torch.FloatTensor(states).to(device)
    actions_t = torch.LongTensor(actions).to(device)
    # handle rewards
    rewards_np = np.array(rewards, dtype=np.float32)
    if not_done_idx:
        last_states_v = torch.FloatTensor(last_states).to(device)
        last_vals_v = net(last_states_v)[1]
        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]
        rewards_np[not_done_idx] += val_gamma * last_vals_np

    ref_vals_v = torch.FloatTensor(rewards_np).to(device)
    return states_v, actions_t, ref_vals_v

# Cell
class A2CTrainer(Callback):

    def after_backward(self):
#         print('clipping',self.learn.clip_grad,np.mean([o.detach().numpy().mean() for o in self.learn.model.parameters()]))
        nn_utils.clip_grad_norm_(self.learn.model.parameters(),self.learn.clip_grad)

    def after_step(self):
        self.learn.loss+=self.learn.loss_policy_v

# Cell
def loss_func(pred,a,r,sp,d,episode_rewards,learn=None):
    if type(learn.yb[0][0])!=ExperienceFirstLast:
        bs=len(learn.xb[0])
        yb=[]
        for i in range(bs):
            yb.append(ExperienceFirstLast(state=learn.xb[0][i],action=a[i],reward=r[i],last_state=sp[i],done=d[i],episode_reward=0))
    else:
        bs=len(learn.yb)
        yb=learn.yb[0]

    s_t,a_t,r_est=unbatch(yb,learn.model,learn.discount**learn.reward_steps,default_device())

    learn.opt.zero_grad()
    logits_v,value_v=learn.model(s_t)

    loss_value_v=F.mse_loss(value_v.squeeze(-1),r_est)
#     loss_value_v=F.mse_loss(value_v,r_est)

    log_prob_v=F.log_softmax(logits_v,dim=1)
    adv_v=r_est-value_v.squeeze(-1).detach()

    log_prob_actions_v=adv_v*log_prob_v[range(bs),a_t]
    loss_policy_v=-log_prob_actions_v.mean()

    prob_v=F.softmax(logits_v,dim=1)
    entropy_loss_v=learn.entropy_beta*(prob_v*log_prob_v).sum(dim=1).mean()

    # calculate the polocy gradients only
    loss_policy_v.backward(retain_graph=True)


    loss_v=entropy_loss_v+loss_value_v

    setattr(learn,'loss_policy_v',loss_policy_v)
    return loss_v

class A2CLearner(AgentLearner):
    def __init__(self,dls,discount=0.99,entropy_beta=0.01,clip_grad=0.1,reward_steps=1,**kwargs):
        super().__init__(dls,loss_func=partial(loss_func,learn=self),**kwargs)
        self.opt=OptimWrapper(AdamW(self.model.parameters(),eps=1e-3))
        self.discount=discount
        self.entropy_beta=entropy_beta
        self.reward_steps=reward_steps
        self.clip_grad=clip_grad