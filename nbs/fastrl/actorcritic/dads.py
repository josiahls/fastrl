# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/14c_actorcritic.dads.ipynb (unless otherwise specified).

__all__ = ['OptionalClampLinear', 'MultiCompGMM', 'SimpleGMM', 'GMM', 'SkillDynamics', 'discrete_uniform',
           'cont_gaussian', 'cont_uniform', 'DADS', 'ExperienceReplay', 'SACCriticTrainer', 'DADSTrainer']

# Cell
import torch.nn.utils as nn_utils
from fastai.torch_basics import *
import torch.nn.functional as F
from fastai.data.all import *
from fastai.basics import *
from dataclasses import field,asdict
from typing import List,Any,Dict,Callable
from collections import deque
import gym
import torch.multiprocessing as mp
from torch.optim import *
from dataclasses import dataclass

from ..data import *
from ..async_data import *
from ..basic_agents import *
from ..learner import *
from ..metrics import *
from fastai.callback.progress import *
from ..ptan_extension import *
from .sac import *
from .diayn import *
import gym
from torch.distributions import *

import matplotlib.pyplot as plt

if IN_NOTEBOOK:
    from IPython import display
    import PIL.Image

# Cell
class OptionalClampLinear(Module):
    def __init__(self,num_inputs,state_dims,fix_variance:bool=False,
                 clip_min=0.3,clip_max=10.0):
        "Linear layer or constant block used for std."
        store_attr()
        if not self.fix_variance: self.fc=nn.Linear(self.num_inputs,self.state_dims)

    def forward(self,x):
        if self.fix_variance: return torch.full((x.shape[0],self.state_dims),1.0)
        else:                 return torch.clamp(nn.Softplus()(self.fc(x)),self.clip_min,self.clip_max)

class MultiCompGMM(Module):
    def __init__(self,num_inputs,state_dims,n_components,fix_variance:bool=False):
        "Multi-component GMM parameterized by a fully connected layer with optional std layer."
        store_attr()
        self.logit_fc=nn.Linear(self.num_inputs,self.n_components)
        self.mean_fcs=nn.ModuleList([nn.Linear(self.num_inputs,self.state_dims)
                                     for _ in range(self.n_components)])
        self.std_fcs=nn.ModuleList([OptionalClampLinear(self.num_inputs,self.state_dims,fix_variance)
                                    for _ in range(self.n_components)])
        self.means,self.logits,self.stds=[],[],[]

    def forward(self,x):
        self.means=torch.stack([o(x) for o in self.mean_fcs],dim=1)
        self.stds=torch.stack([o(x) for o in self.std_fcs],dim=1)
        self.logits=self.logit_fc(x)
        return MixtureSameFamily(
            mixture_distribution=Categorical(self.logits),
            component_distribution=Independent(Normal(self.means,self.stds),1)
        )

class SimpleGMM(Module):
    def __init__(self,num_inputs,state_dims,fix_variance:bool=False):
        "Single-component GMM parameterized by a fully connected layer with optional std layer."
        store_attr()
        self.mean_fc=nn.Linear(self.num_inputs,self.state_dims)
        self.std_fc=OptionalClampLinear(self.num_inputs,self.state_dims,fix_variance)

    def forward(self,x): return Independent(Normal(self.mean_fc(x),self.std_fc(x)),1)

class GMM(Module):
    def __init__(self,num_inputs,state_dims,n_components,fix_variance:bool=False):
        "N-component GMM parameterized by fully connected layers with optional std layers."
        store_attr()
        if self.n_components>1: self.distribution=MultiCompGMM(num_inputs,state_dims,n_components,fix_variance)
        else:                   self.distribution=SimpleGMM(num_inputs,state_dims,fix_variance)

    def forward(self,x): return self.distribution(x)

# Cell
class SkillDynamics(Module):
    def __init__(self,s_dim,a_dim,n_components,fix_variance:bool=False,
                 use_model_mean:bool=None,use_batch_norm:bool=True,fc_params:tuple=None):
        store_attr(but='fc_params,use_model_mean')
        self.fc_params=ifnone(fc_params,(256,256))
        self.use_model_mean=ifnone(use_model_mean,n_components>1)
        if self.use_batch_norm:
            self.s_bn,self.sp_bn=nn.BatchNorm1d(s_dim),nn.BatchNorm1d(s_dim)
        self.fcs=nn.Sequential(*[nn.Linear((s_dim+a_dim) if i==0 else self.fc_params[i-1],p)
                                for i,p in enumerate(self.fc_params)])

        self.gmm=GMM(self.fc_params[-1],s_dim,n_components,fix_variance)

    def forward(self,s,a,sp=None,training=True):
        "Returns the `GMM` distribution of `s` and `a`, mean, and **optionally** "\
        "log(p) of the state transition between `s` and `sp` if `sp` is not None."
        if sp is not None: sp=sp-s
        if not training: self.eval()

        if self.use_batch_norm:
            s=self.s_bn(s)
            if sp is not None: sp=self.sp_bn(sp)

        sa=torch.hstack([s,a])

        x=self.fcs(sa)

        dist=self.gmm(x)
        self.train()
        return dist,(sp if sp is None else dist.log_prob(sp))

    def log_prob(self,s,a,sp): return self(s,a,sp,training=False)[1]

    def predict_state(self,s,a):
        "Returns the predicted state that `s` and `a` will result in."
        dist,_=self(s,a,training=False)
        if self.use_model_mean:
            means,idx=dist.component_distribution.mean,torch.argmax(dist.mixture_distribution.logits,dim=1)
            pred_s=means[[torch.arange(means.shape[0]),idx]]
        else:
            pred_s=dist.mean

        if self.use_batch_norm:
            pred_s=pred_s*(self.sp_bn.running_var+1e-3).sqrt()+self.sp_bn.running_mean

        pred_s+=s
        return pred_s

# Cell
def discrete_uniform(current_skill,latent_sz:int=2,alt_s:Tensor=None,deterministic:bool=False):
    "Returns a uniform discrete distribution."
    if deterministic:
        return torch.cat([torch.roll(current_skill,i,dims=1) for i in range(1,alt_s.shape[0])])
    return Multinomial(1,probs=Tensor([1./latent_sz]*latent_sz)).sample_n(alt_s.shape[0])

# Cell
def cont_gaussian(sz:int,alt_s:Tensor):
    "Returns a continuous guassian distribution of size `sz`"
    return MultivariateNormal(torch.zeros(sz),torch.eye(sz)).sample_n(alt_s.shape[0])

# Cell
def cont_uniform(sz,alt_s:Tensor,low=-1.0, high=1.0):
    return torch.zeros((alt_s.shape[0],sz)).uniform_(low,high)

# Cell
@delegates(SAC)
class DADS(SAC):
    def __init__(self,num_inputs,action_space,num_skills:int=20,include_actions:bool=False,hidden_size=100,lr=0.003,
                 n_components:int=4,prior_samples:int=100,latent_sz:int=2,latent_prior_method='',
                 skill_lr=3e-4,episide_horizon:int=1,planning_horizon:int=1,
                 primitive_horizon:int=1,train_dyn=True,**kwargs):
        store_attr()
        self.num_inputs=num_inputs+self.num_skills
        self.original_num_inputs=num_inputs
        self.skill_dyn=SkillDynamics(self.original_num_inputs,self.num_skills,self.n_components)
        super().__init__(self.num_inputs,self.action_space,hidden_size=hidden_size,lr=lr,**kwargs)
        self.skill_opt=Adam(self.skill_dyn.parameters(),lr=3e-4)

        self.p_z=np.full(self.num_skills,1.0/self.num_skills)
        self.log_p_z_episode=[]
        self.z=0
        self.reset_z()

    @torch.no_grad()
    def intrinsic_reward(self,s,skill:Tensor,sp,gpu_limit=20*4000):
        "Given a batch of `s` and `sp` what is the reward for using skill `z`?"
        n_repetitions=self.prior_samples if self.prior_samples>0 else self.num_skills-1
        alt_s=torch.cat([s]*n_repetitions,axis=0)
        alt_sp=torch.cat([sp]*n_repetitions,axis=0)

        if self.latent_prior_method=='discrete_uniform' and not self.prior_samples:
            alt_skill=discrete_uniform(skill,self.num_skills,alt_s,True)
        elif self.latent_prior_method=='discrete_uniform':
            alt_skill=discrete_uniform(skill,self.num_skills,alt_s,False)
        elif self.latent_prior_method=='cont_gaussian':
            alt_skill=cont_gaussian(self.num_skills,alt_s)
        else:
            alt_skill=cont_uniform(self.num_skills,alt_s)

        logp=self.skill_dyn.log_prob(s,skill,sp)

        if alt_s.shape[0]<=gpu_limit:
            logp_altz=self.skill_dyn.log_prob(alt_s,alt_skill,alt_sp)
        else:
            # TODO JL: Does this chunking code have to be so complex? Does fastcore have something
            logp_altz=[]
            for idx in range(alt_s.shape[0]//gpu_limit):
                start_idx=idx*gpu_limit
                end_idx=(idx+1)*gpu_limit
                logp_altz.append(
                    self.skill_dyn.log_prob(alt_s[start_idx:end_idx],
                                            alt_skill[start_idx:end_idx],
                                            alt_sp[start_idx:end_idx])
                )
            # TODO: JL: ok what is the scenario where this if statement is needed?
            if alt_s.shape[0]%gpu_limit:
                start_idx=alt_s.shape[0]%gpu_limit
                logp_altz.append(
                    self.skill_dyn.log_prob(alt_s[-start_idx:],
                                            alt_skill[-start_idx:],
                                            alt_sp[-start_idx:]))
            logp_altz=torch.cat(logp_altz)
        logp_altz=torch.stack(torch.chunk(logp_altz,n_repetitions))

        intrinsic_reward=np.log(n_repetitions+1)-torch.log(
            1+torch.exp(torch.clamp(logp_altz-logp.reshape(1,-1),-50,50)).sum(dim=0)
        )

        return intrinsic_reward.reshape(-1,1),logp,logp_altz

    # TODO: They have batch weighting here. Maybe look into
    def increase_skill_opt(self,log_prob:Tensor):
        self.skill_opt.zero_grad()
        loss=-torch.mean(log_prob)
        loss.backward()
        self.skill_opt.step()

    def decrease_skill_opt(self,log_prob:Tensor):
        self.skill_opt.zero_grad()
        loss=torch.mean(log_prob)
        loss.backward()
        self.skill_opt.step()

    def sample_z(self):
        """Samples z from p(z), using probabilities in self._p_z."""
        return np.random.choice(self.num_skills,p=self.p_z)

    def reset_z(self): self.z=self.sample_z()
    def __call__(self,s,asl):
        aug_s=self.concat_obs_z(s,self.z)
        return super().__call__(aug_s,asl)

    def concat_obs_z(self,obs,z):
        """Concatenates the observation to a one-hot encoding of Z."""
        assert np.isscalar(z)
        if type(obs)==list and len(obs)==1: obs=obs[0]
        if len(obs.shape)==2 and obs.shape[0]==1: obs=obs[0]

        z_one_hot=np.zeros(self.num_skills)
        z_one_hot[z]=1
        if type(obs)==Tensor: obs=obs.cpu()
        return torch.FloatTensor(np.hstack([obs,z_one_hot])).reshape(1,-1)

    def update_parameters(self, *yb, learn):
        batch=learn.sample_yb

        state_batch=torch.stack([o.state.to(device=default_device()) for o in batch]).float()
        next_state_batch=torch.stack([o.last_state.to(device=default_device()) for o in batch]).float()
        action_batch=torch.stack([o.action.to(device=default_device()) for o in batch]).float()
        reward_batch=torch.stack([o.reward.to(device=default_device()) for o in batch]).float()
        mask_batch=torch.stack([o.done.to(device=default_device()) for o in batch]).float().unsqueeze(1)

        if self.train_dyn:
            self.increase_skill_opt(self.skill_dyn.log_prob(
                state_batch[:,:self.num_skills],
                state_batch[:,self.num_skills:],
                next_state_batch[:,:self.num_skills]
            ))

            reward_batch,_,_=self.intrinsic_reward(
                state_batch[:,:self.num_skills],
                state_batch[:,self.num_skills:],
                next_state_batch[:,:self.num_skills])
#             print(reward_batch)


        with torch.no_grad():
            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)
            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)
            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi
            next_q_value = reward_batch + (1-mask_batch) * self.gamma * (min_qf_next_target)
        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step
        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]
        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]
        qf_loss = qf1_loss + qf2_loss

        self.critic_optim.zero_grad()
        qf_loss.backward()
        self.critic_optim.step()

        pi, log_pi, _ = self.policy.sample(state_batch)

        qf1_pi, qf2_pi = self.critic(state_batch, pi)
        min_qf_pi = torch.min(qf1_pi, qf2_pi)

        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]

        self.policy_optim.zero_grad()
        policy_loss.backward()
        self.policy_optim.step()

        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()

            self.alpha_optim.zero_grad()
            alpha_loss.backward()
            self.alpha_optim.step()

            self.alpha = self.log_alpha.exp()
            alpha_tlogs = self.alpha.clone() # For TensorboardX logs
        else:
            alpha_loss = torch.tensor(0.).to(self.device)
            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs


        if self.updates % self.target_update_interval == 0:
            soft_update(self.critic_target, self.critic, self.tau)
        self.updates+=1
#         print(self.updates)
#         print('complete')
        return qf1_loss+ qf2_loss+ policy_loss+ alpha_loss+ alpha_tlogs

# Cell
class ExperienceReplay(Callback):
    def __init__(self,sz=100,bs=128,starting_els=1,max_steps=1):
        store_attr()
        self.queue=deque(maxlen=int(sz))
        self.max_steps=max_steps

    def before_fit(self):
        self.learn.agent.warming_up=True
        while len(self.queue)<self.starting_els:
            for i,o in enumerate(self.dls.train):
                batch=[ExperienceFirstLast(state=o[0][i],action=o[1][i],reward=o[2][i],
                                    last_state=o[3][i], done=(o[4][i] and self.max_steps!=o[6][i]),episode_reward=o[5][i],steps=o[6][i])
                                    for i in range(len(o[0]))]
#                 print(self.max_steps,max([o.steps for o in batch]))
                for _b in batch: self.queue.append(_b)
                if len(self.queue)>self.starting_els:break
        self.learn.agent.warming_up=False

#     def after_epoch(self):
#         print(len(self.queue))
    def before_batch(self):
#         print(len(self.queue))
        b=list(self.learn.xb)+list(self.learn.yb)
        batch=[ExperienceFirstLast(state=b[0][i],action=b[1][i],reward=b[2][i],
                                last_state=b[3][i], done=(b[4][i] and self.max_steps!=b[6][i]),episode_reward=b[5][i],
                                steps=b[6][i])
                                for i in range(len(b[0]))]
        for _b in batch: self.queue.append(_b)
        idxs=np.random.randint(0,len(self.queue), self.bs)
        self.learn.sample_yb=[self.queue[i] for i in idxs]

# Cell
class SACCriticTrainer(Callback):
    def after_batch(self):
        self.learn.dls.bs=1
        for d in self.learn.dls.loaders: d.bs=1

    def after_loss(self):raise CancelBatchException

# Cell
class DADSTrainer(ExperienceReplay):

    def __init__(self,*args,**kwargs):
        self.log_p_z_episode=[]
        super().__init__(*args,**kwargs)

    def before_fit(self):
        self.learn.agent.warming_up=True
        while len(self.queue)<self.starting_els:
            for i,o in enumerate(self.dls.train):
                z=self.learn.agent.z
                batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(o[0][i],z)[0],
                                           action=o[1][i],
                                           reward=o[2][i],
                                           last_state=self.learn.agent.concat_obs_z(o[3][i],z)[0],
                                           done=(o[4][i] and self.max_steps!=o[6][i]),
                                           episode_reward=o[5][i],steps=o[6][i])
                                    for i in range(len(o[0]))]
#                 print(self.max_steps,max([o.steps for o in batch]))
#                 print(batch[0])
#                 for k in range(len(batch)):
#                     intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))
#                     self.learn.agent.increase_skill_opt(self.agent.z,disc_out)
#                     batch[k]=ExperienceFirstLast(
#                         state=batch[k].state.to(device=default_device()),
#                         action=batch[k].action,
#                         reward=batch[k].reward,
#                         last_state=batch[k].last_state.to(device=default_device()),
#                         done=batch[k].done,
#                         episode_reward=batch[k].episode_reward,
#                         steps=batch[k].steps
#                     )


#                 print(batch[0])
                for _b in batch:self.queue.append(_b)
                if any([_b.done for _b in batch]): self.learn.agent.reset_z()
                if len(self.queue)>self.starting_els:break
        self.learn.agent.warming_up=False

# #     def after_epoch(self):
# #         print(len(self.queue))
    def before_batch(self):
#         print(len(self.queue))
        b=list(self.learn.xb)+list(self.learn.yb)
        z=self.learn.agent.z
        batch=[ExperienceFirstLast(state=self.learn.agent.concat_obs_z(b[0][i],z)[0],
                                   action=b[1][i],
                                   reward=b[2][i],
                                   last_state=self.learn.agent.concat_obs_z(b[3][i],z)[0],
                                   done=(b[4][i] and self.max_steps!=b[6][i]),
                                   episode_reward=b[5][i],steps=b[6][i])
              for i in range(len(b[0]))]

#         for k in range(len(batch)):
#             intrinsic_reward,disc_out=self.learn.agent.intrinsic_reward(Tensor(batch[k].last_state))
#             self.learn.agent.discriminator_learn(self.agent.z,disc_out)
#             batch[k]=ExperienceFirstLast(
#                 state=batch[k].state.to(device=default_device()),
#                 action=batch[k].action,
#                 reward=intrinsic_reward,
#                 last_state=batch[k].last_state.to(device=default_device()),
#                 done=batch[k].done,
#                 episode_reward=batch[k].episode_reward,
#                 steps=batch[k].steps
#             )

#         print(self.learn.xb)
        self.learn.xb=(torch.stack([e.state for e in batch]),)
#         print(self.learn.yb)
        self.learn.yb=(torch.stack([o.action for o in batch]),
                       torch.stack([o.reward for o in batch]),
                       torch.stack([o.last_state for o in batch]),
                       torch.stack([o.done for o in batch]),
                       torch.stack([o.episode_reward for o in batch]),
                       torch.stack([o.steps for o in batch]))
#         print(self.learn.yb)

        for _b in batch: self.queue.append(_b)
        idxs=np.random.randint(0,len(self.queue), self.bs)
        self.learn.sample_yb=[self.queue[i] for i in idxs]