# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"PixelObservationWrapper": "01_wrappers.ipynb",
         "STATE_KEY": "01_wrappers.ipynb",
         "ActionSelector": "03_basic_agents.ipynb",
         "ArgmaxActionSelector": "03_basic_agents.ipynb",
         "EpsilonGreedyActionSelector": "03_basic_agents.ipynb",
         "ProbabilityActionSelector": "03_basic_agents.ipynb",
         "default_states_preprocessor": "03_basic_agents.ipynb",
         "float32_preprocessor": "03_basic_agents.ipynb",
         "BaseAgent": "03_basic_agents.ipynb",
         "TestAgent": "05b_data.ipynb",
         "DiscreteAgent": "03_basic_agents.ipynb",
         "DQNAgent": "03_basic_agents.ipynb",
         "TargetNet": "03_basic_agents.ipynb",
         "PolicyAgent": "03_basic_agents.ipynb",
         "ActorCriticAgent": "03_basic_agents.ipynb",
         "AgentLearner": "04_learner.ipynb",
         "Experience": "05a_ptan_extend.ipynb",
         "ExperienceSource": "05a_ptan_extend.ipynb",
         "ExperienceFirstLast": "05a_ptan_extend.ipynb",
         "ExperienceSourceFirstLast": "05a_ptan_extend.ipynb",
         "is_single_nested_tuple": "05b_data.ipynb",
         "TfmdSourceDL": "05b_data.ipynb",
         "TfmdSource": "05b_data.ipynb",
         "IterableDataBlock": "05b_data.ipynb",
         "SeedZeroWrapper": "05b_data.ipynb",
         "MakeTfm": "05b_data.ipynb",
         "env_display": "05b_data.ipynb",
         "envlen": "05b_data.ipynb",
         "ResetAndStepTfm": "05b_data.ipynb",
         "ExperienceBlock": "05b_data.ipynb",
         "FirstLastTfm": "05b_data.ipynb",
         "FirstLastExperienceBlock": "05b_data.ipynb",
         "noopo": "05c_async_data.ipynb",
         "template_data_fit": "05c_async_data.ipynb",
         "DataFitProcess": "05c_async_data.ipynb",
         "safe_get": "05c_async_data.ipynb",
         "MultiProcessTfm": "05c_async_data.ipynb",
         "TotalReward": "05c_async_data.ipynb",
         "AsyncExperienceBlock": "05c_async_data.ipynb",
         "AvgEpisodeRewardMetric": "13_metrics.ipynb",
         "weights_init_": "14a_actorcritic.sac.ipynb",
         "ValueNetwork": "14a_actorcritic.sac.ipynb",
         "QNetwork": "14a_actorcritic.sac.ipynb",
         "GaussianPolicy": "14a_actorcritic.sac.ipynb",
         "DeterministicPolicy": "14a_actorcritic.sac.ipynb",
         "create_log_gaussian": "14a_actorcritic.sac.ipynb",
         "logsumexp": "14a_actorcritic.sac.ipynb",
         "soft_update": "14a_actorcritic.sac.ipynb",
         "hard_update": "14a_actorcritic.sac.ipynb",
         "SAC": "14a_actorcritic.sac.ipynb",
         "ExperienceReplay": "20a_qlearning.dqn.ipynb",
         "SACCriticTrainer": "14c_actorcritic.dads.ipynb",
         "SACLearner": "14a_actorcritic.sac.ipynb",
         "LOG_SIG_MAX": "14a_actorcritic.sac.ipynb",
         "LOG_SIG_MIN": "14a_actorcritic.sac.ipynb",
         "epsilon": "14a_actorcritic.sac.ipynb",
         "Discriminator": "14b_actorcritic.diayn.ipynb",
         "DIAYN": "14b_actorcritic.diayn.ipynb",
         "DIAYN.__doc__": "14b_actorcritic.diayn.ipynb",
         "DiscriminatorTrainer": "14b_actorcritic.diayn.ipynb",
         "OptionalClampLinear": "14c_actorcritic.dads.ipynb",
         "MultiCompGMM": "14c_actorcritic.dads.ipynb",
         "SimpleGMM": "14c_actorcritic.dads.ipynb",
         "GMM": "14c_actorcritic.dads.ipynb",
         "SkillDynamics": "14c_actorcritic.dads.ipynb",
         "discrete_uniform": "14c_actorcritic.dads.ipynb",
         "cont_gaussian": "14c_actorcritic.dads.ipynb",
         "cont_uniform": "14c_actorcritic.dads.ipynb",
         "DADS": "14c_actorcritic.dads.ipynb",
         "DADSTrainer": "14c_actorcritic.dads.ipynb",
         "LinearA2C": "16_actorcritic.a2c.ipynb",
         "unbatch": "16_actorcritic.a2c.ipynb",
         "loss_func": "19_policy_gradient.trpo.ipynb",
         "A3CLearner": "15_actorcritic.a3c_data.ipynb",
         "A3CTrainer": "15_actorcritic.a3c_data.ipynb",
         "data_fit": "15_actorcritic.a3c_data.ipynb",
         "A2CTrainer": "16_actorcritic.a2c.ipynb",
         "A2CLearner": "16_actorcritic.a2c.ipynb",
         "ModelActor": "19_policy_gradient.trpo.ipynb",
         "ModelCritic": "19_policy_gradient.trpo.ipynb",
         "AgentA2C": "19_policy_gradient.trpo.ipynb",
         "HID_SIZE": "19_policy_gradient.trpo.ipynb",
         "calc_logprob": "19_policy_gradient.trpo.ipynb",
         "calc_adv_ref": "19_policy_gradient.trpo.ipynb",
         "GAMMA": "19_policy_gradient.trpo.ipynb",
         "GAE_LAMBDA": "19_policy_gradient.trpo.ipynb",
         "LEARNING_RATE_ACTOR": "19_policy_gradient.trpo.ipynb",
         "LEARNING_RATE_CRITIC": "19_policy_gradient.trpo.ipynb",
         "PPO_EPS": "19_policy_gradient.trpo.ipynb",
         "PPO_EPOCHES": "19_policy_gradient.trpo.ipynb",
         "PPO_BATCH_SIZE": "19_policy_gradient.trpo.ipynb",
         "PPOTrainer": "18_policy_gradient.ppo.ipynb",
         "PPOLearner": "18_policy_gradient.ppo.ipynb",
         "get_flat_params_from": "19_policy_gradient.trpo.ipynb",
         "set_flat_params_to": "19_policy_gradient.trpo.ipynb",
         "conjugate_gradients": "19_policy_gradient.trpo.ipynb",
         "linesearch": "19_policy_gradient.trpo.ipynb",
         "trpo_step": "19_policy_gradient.trpo.ipynb",
         "TRPO_MAX_KL": "19_policy_gradient.trpo.ipynb",
         "TRPO_DAMPING": "19_policy_gradient.trpo.ipynb",
         "TRPOTrainer": "19_policy_gradient.trpo.ipynb",
         "TRPOLearner": "19_policy_gradient.trpo.ipynb",
         "LinearDQN": "20a_qlearning.dqn.ipynb",
         "EpsilonTracker": "20a_qlearning.dqn.ipynb",
         "calc_target": "20a_qlearning.dqn.ipynb",
         "DQNTrainer": "20b_qlearning.dqn_n_step.ipynb",
         "DQNLearner": "20a_qlearning.dqn.ipynb",
         "NStepDQNLearner": "20b_qlearning.dqn_n_step.ipynb",
         "calc_target_batch": "20c_qlearning.dqn_target.ipynb",
         "TargetDQNTrainer": "20c_qlearning.dqn_target.ipynb",
         "TargetDQNLearner": "20e_qlearning.dqn_noisy.ipynb",
         "DoubleDQNTrainer": "20d_qlearning.dqn_double.ipynb",
         "NoisyLinear": "20e_qlearning.dqn_noisy.ipynb",
         "NoisyFactorizedLinear": "20e_qlearning.dqn_noisy.ipynb",
         "NoisyDQN": "20e_qlearning.dqn_noisy.ipynb",
         "DuelingBlock": "20f_qlearning.dqn_dueling.ipynb",
         "DuelingDQN": "20f_qlearning.dqn_dueling.ipynb",
         "DistributionalDQN": "20h_qlearning.dist_dqn.ipynb",
         "Vmax": "20h_qlearning.dist_dqn.ipynb",
         "Vmin": "20h_qlearning.dist_dqn.ipynb",
         "N_ATOMS": "20h_qlearning.dist_dqn.ipynb",
         "DELTA_Z": "20h_qlearning.dist_dqn.ipynb",
         "distr_projection": "20h_qlearning.dist_dqn.ipynb",
         "loss_fn": "20h_qlearning.dist_dqn.ipynb",
         "calc_dist_target_batch": "20h_qlearning.dist_dqn.ipynb"}

modules = ["wrappers.py",
           "basic_agents.py",
           "learner.py",
           "ptan_extension.py",
           "data.py",
           "async_data.py",
           "metrics.py",
           "actorcritic/sac.py",
           "actorcritic/diayn.py",
           "actorcritic/dads.py",
           "actorcritic/a3c_data.py",
           "actorcritic/a2c.py",
           "qlearning/dqn.py",
           "qlearning/dqn_n_step.py",
           "qlearning/dqn_target.py",
           "qlearning/dqn_double.py",
           "qlearning/dqn_noisy.py",
           "qlearning/dqn_dueling.py",
           "qlearning/dddqn.py",
           "qlearning/dist_dqn.py"]

doc_url = "https://josiahls.github.io/fast-reinforcement-learning-2/"

git_url = "https://github.com/josiahls/fast-reinforcement-learning-2/tree/master/"

def custom_doc_links(name): return None
