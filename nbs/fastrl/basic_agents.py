# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_basic_agents.ipynb (unless otherwise specified).

__all__ = ['ActionSelector', 'ArgmaxActionSelector', 'EpsilonGreedyActionSelector', 'ProbabilityActionSelector',
           'default_states_preprocessor', 'float32_preprocessor', 'BaseAgent', 'TestAgent', 'DiscreteAgent', 'DQNAgent',
           'TargetNet', 'PolicyAgent', 'ActorCriticAgent']

# Cell
import torch, torch.nn.functional as F
from torch import ByteTensor, DoubleTensor, FloatTensor, HalfTensor, LongTensor, ShortTensor, Tensor
from torch import nn, optim, as_tensor
from torch.utils.data import BatchSampler, DataLoader, Dataset, Sampler, TensorDataset
from torch.nn.utils import weight_norm, spectral_norm
from dataclasses import asdict,dataclass
from typing import Callable,Tuple,Union
# from fastai.torch_core import *
# from fastai.basic_data import *
# from fastai.basic_train import *
from fastai.basics import *
import textwrap
import numpy as np
import logging

"Note these are modified versions of 'Shmuma/Ptan'. Github, 2020, https://github.com/Shmuma/ptan/blob/master/ptan/agent.py. Accessed 13 June 2020."

# Cell
class ActionSelector:
    "Abstract class which converts scores to the actions."
    def __call__(self,scores):raise NotImplementedError

class ArgmaxActionSelector(ActionSelector):
    "Selects actions using argmax."
    def __call__(self,scores):
        assert isinstance(scores,np.ndarray)
        return np.argmax(scores,axis=1)

@dataclass
class EpsilonGreedyActionSelector(ActionSelector):
    epsilon:float=0.05
    selector:ActionSelector=ArgmaxActionSelector()

    def __call__(self,scores):
        assert isinstance(scores,np.ndarray)
        bs,n_a=scores.shape
        a=self.selector(scores)
        mask=np.random.random(size=bs)<self.epsilon
        rand_a=np.random.choice(n_a, sum(mask))
        a[mask]=rand_a
        return a

class ProbabilityActionSelector(ActionSelector):
    "Converts probabilities of actions into action by sampling them."
    def __call__(self,probs):
        assert isinstance(probs,np.ndarray)
        actions=[np.random.choice(len(prob),p=prob) for prob in probs]
        return np.array(actions)

# Cell
def default_states_preprocessor(s,dtype=np.float32):
    "Convert list of states into the form suitable for model. By default we assume Variable."
    np_s=np.expand_dims(s,0) if len(np.array(s).shape)==1 else np.array(s, copy=False)
    return torch.tensor(np_s.astype(dtype))

def float32_preprocessor(s):
    np_s=np.array(s, dtype=np.float32)
    return torch.tensor(np_s)

# Cell
import ptan

@dataclass
class BaseAgent(ptan.agent.BaseAgent):
    model:nn.Module=None # If None, learner will set
    def initial_state(self):return None
    def __call__(self,sl,asl,include_batch_dim=False):
        assert isinstance(sl,(list,np.ndarray))
        assert isinstance(asl,(list,np.ndarray))
        assert len(asl)==len(sl)
        raise NotImplementedError()


@dataclass
class TestAgent(BaseAgent):
    env:object=None
    def initial_state(self):return None
    def __call__(self,sl,asl=None,include_batch_dim=False):
        if type(self.env)!=list:return self.env.action_space.sample(),None
        return self.env[0].action_space.sample(),None

# Cell
@dataclass
class DiscreteAgent(BaseAgent):
    "DiscreteAgent a simple discrete action selector."
    a_selector:ActionSelector=None
    device:str=None
    preprocessor:Callable=default_states_preprocessor
    apply_softmax:bool=False

    def safe_unbatch(self,o:np.array)->np.array:return o[0] if o.shape[0]==1 and len(o.shape)>1 else o
    def split_v(self,v,asl): return v,asl

    @torch.no_grad()
    def __call__(self,x,asl=None,include_batch_dim=True):
        x=self.preprocessor(x) if self.preprocessor is not None else s
        asl= np.zeros(x.shape) if asl is None or len(asl)==0 else asl
        if torch.is_tensor(x):
            x=x.to(self.device)
        v=self.model(x)
        if type(v)==tuple:v,asl=self.split_v(v,asl)
        if self.apply_softmax:
            v=F.softmax(v,dim=1)
        q=v.data.cpu().numpy()
        al=self.a_selector(q)
        if not include_batch_dim:al=self.safe_unbatch(al).tolist()

#         print(al)
#         if not isinstance(al,list): al=[al]
        if include_batch_dim:
            al=np.array(al)
            asl=np.array(asl)
            if len(al.shape)==0: al=al.reshape(1,)
            if len(asl.shape)==0: asl=asl.reshape(1,)
            return al,asl

        return (al[0],asl[0])

@dataclass
class DQNAgent(DiscreteAgent):
    "DQNAgent is a memoryless DQN agent which calculates Q values from the observations and  converts them into the actions using a_selector."
    def __post_init__(self):
        self.a_selector=ifnone(self.a_selector,ArgmaxActionSelector())

# Cell
class TargetNet:
    "Wrapper around model which provides copy of it instead of trained weights."
    def __init__(self, model):
        self.model = model
        self.target_model = copy.deepcopy(model)

    def sync(self):self.target_model.load_state_dict(self.model.state_dict())
    def alpha_sync(self,alpha):
        "Blend params of target net with params from the model."
        assert isinstance(alpha,float)
        assert 0.0<alpha<=1.0
        state=self.model.state_dict()
        tgt_state=self.target_model.state_dict()
        for k, v in state.items():
            tgt_state[k]=tgt_state[k]*alpha+(1-alpha)*v
        self.target_model.load_state_dict(tgt_state)

@dataclass
class PolicyAgent(DiscreteAgent):
    "Policy agent gets action probabilities from the model and samples actions from it."
    def __post_init__(self):
        self.a_selector=ifnone(self.a_selector,ProbabilityActionSelector())
        self.apply_softmax=True

class ActorCriticAgent(PolicyAgent):
    "Policy agent which returns policy and value tensors from observations. Value are stored in agent's state \
     and could be reused for rollouts calculations by ExperienceSource."
    def split_v(self,v,asl):
#         v=v
        return v[0],v[1].cpu().detach().squeeze().numpy()