{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "# upgrade fastrl on colab\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastai.data.pipes.map.mux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Python native modules\n",
    "import os\n",
    "from inspect import isfunction,ismethod\n",
    "from typing import *\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from fastai.torch_basics import *\n",
    "# from torch.utils.data.dataloader import DataLoader as OrgDataLoader\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "from fastai.data.transforms import *\n",
    "# Local modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Basic DataPipes - Multiplexer\n",
    "> Basic datapipes for work with fastrl core API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b200370-6438-4d8e-84b4-2d7cf82f9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='344064' class='' max='342207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.54% [344064/342207 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Path('/home/fastrl_user/.fastai/data/mnist_tiny'),\n",
       " 'https://s3.amazonaws.com/fast-ai-sample/mnist_tiny.tgz')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, so not exported\n",
    "\n",
    "from fastai.vision.core import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.data.external import *\n",
    "\n",
    "untar_data(URLs.MNIST_TINY),URLs.MNIST_TINY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc47fd2-3d79-4a6b-8fcd-473bc59c8416",
   "metadata": {},
   "source": [
    "Load the mnist csv..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce704c6e-d420-4e2e-8f60-fd085b3e6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dp.iter.IterableWrapper([str(untar_data(URLs.MNIST_TINY)/'labels.csv')]) # FileOpener really should support Path as well as str\n",
    "pipe = dp.iter.FileOpener(pipe, mode=\"b\")\n",
    "pipe = dp.iter.CSVParser(pipe,skip_lines=1)\n",
    "\n",
    "class AddIdx():\n",
    "    def __init__(self): self.idx=0\n",
    "    def __call__(self,file):\n",
    "        try:     return (self.idx,file)\n",
    "        finally: self.idx+=1\n",
    "\n",
    "base_pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "pipe = dp.map.IterToMapConverter(pipe,key_value_fn=AddIdx())\n",
    "# pipe[5],len(base_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7432d8-5f18-460c-a2e3-ab849b6d1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchdata/datapipes/iter/util/converter.py:97: UserWarning: Data from prior DataPipe are loaded to get length ofIterToMapConverter before execution of the pipeline.Please consider removing len().\n",
      "  \"Data from prior DataPipe are loaded to get length of\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364743a-2b26-4398-9501-e6ec0cdfbec4",
   "metadata": {},
   "source": [
    "Now that we have the csv converted into a map, we want to split it into a training and validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa47b9ac-6049-414d-acc9-6272cd8afe2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/utils/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163/4038001988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemux\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastrl/fastrl/fastai/data/pipes/map/demux.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapDataPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_lambda_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mT_co\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T_co\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/utils/common.py)"
     ]
    }
   ],
   "source": [
    "from fastrl.fastai.data.pipes.map.demux import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecf2d5-5103-4480-8d03-11f5d48fc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_splitter(o): \n",
    "    \n",
    "    int_mapping={'train':0,'valid':1}\n",
    "    \n",
    "    return int_mapping[Path(o[0]).parts[0]]\n",
    "\n",
    "\n",
    "dp1, dp2 = DemultiplexerMapDataPipe(pipe,num_instances=2, classifier_fn=train_valid_splitter, drop_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba19ed-c3ee-49e0-9b31-ca425f3dc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dp1)+len(dp2)==len(pipe),f\"The demux'd dp1 and dp2 when added together should be the same len as pipe {len(dp1)} + {len(dp2)} = {len(pipe)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822421bf-95de-4d02-84e1-2b39c9672e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1[0],dp2[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b1a6c-f36b-40c3-a387-cc9c6a41e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dp1)[:5],list(dp2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c9d7e-10bc-4e5e-9efb-beb54b962324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSplitter():\n",
    "    k=0\n",
    "    def __init__(self,k_splits=2): self.k_splits=k_splits\n",
    "    def __call__(self,*args):\n",
    "        try: \n",
    "            return self.k\n",
    "        finally: \n",
    "            self.k+=1\n",
    "            if self.k==self.k_splits: self.k=0\n",
    "\n",
    "k1,k2,k3 = DemultiplexerMapDataPipe(dp1,num_instances=3,  classifier_fn=KSplitter(k_splits=3), drop_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f05319-516e-4b0c-acef-aac36879379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from itertools import chain, zip_longest\n",
    "\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "from torch.utils.data.datapipes._decorator import functional_datapipe\n",
    "from torch.utils.data.datapipes.datapipe import MapDataPipe\n",
    "\n",
    "\n",
    "@functional_datapipe(\"mux\")\n",
    "class MultiplexerMapDataPipe(MapDataPipe):\n",
    "    def __init__(self, *datapipes, dp_index_map: Optional[Dict[MapDataPipe, Iterable]] = None):\n",
    "        self.datapipes = datapipes\n",
    "        self.dp_index_map = dp_index_map if dp_index_map else {}\n",
    "        self.length: Optional[int] = None\n",
    "        self.index_map = {}\n",
    "        # Create a generator that yields (index, (dp_num, old_index)) in sequentially order.\n",
    "        indices = (self._add_dp_num(i, dp) for i, dp in enumerate(datapipes))\n",
    "        dp_id_and_key_tuples = chain.from_iterable(zip_longest(*indices))\n",
    "        self.key_gen = enumerate(e for e in dp_id_and_key_tuples if e is not None)\n",
    "\n",
    "    def _add_dp_num(self, dp_num: int, dp: MapDataPipe):\n",
    "        # Assume 0-index for all DataPipes unless alternate indices are defined in `self.dp_index_map`\n",
    "        dp_indices = self.dp_index_map[dp] if dp in self.dp_index_map else range(len(dp))\n",
    "        for idx in dp_indices:\n",
    "            yield dp_num, idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if 0 <= index < len(self):\n",
    "            if index in self.index_map:\n",
    "                dp_num, old_key = self.index_map[index]\n",
    "            else:\n",
    "                curr_key = -1\n",
    "                while curr_key < index:\n",
    "                    curr_key, dp_num_key_tuple = next(self.key_gen)\n",
    "                    dp_num, old_key = dp_num_key_tuple\n",
    "                self.index_map[index] = dp_num, old_key\n",
    "            try:\n",
    "                return self.datapipes[dp_num][old_key]\n",
    "            except KeyError:\n",
    "                raise RuntimeError(\n",
    "                    f\"Incorrect key is given to MapDataPipe {dp_num} in Multiplexer, likely because\"\n",
    "                    f\"that DataPipe is not 0-index but alternate indices are not given.\"\n",
    "                )\n",
    "        raise RuntimeError(f\"Index {index} is out of bound for Multiplexer.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for dp in self.datapipes:\n",
    "                self.length += len(dp)\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2c364-ea9d-42b6-822c-f82eaae8c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dp.map.SequenceWrapper(range(10))\n",
    "b = dp.map.SequenceWrapper({'a': 100, 'b': 200, 'c': 300, 'd': 400})\n",
    "datapipe = a.mux(b, dp_index_map={b: ['a', 'b', 'c', 'd']})\n",
    "list(datapipe)  # Returns [0, 100, 1, 200, 2, 300, 3, 400, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b135512-db2d-4f8f-bcd9-fdbfab09df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k1),len(k2),len(k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed4eba-db91-4cbe-ad0a-014d7f213795",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pipes=MultiplexerMapDataPipe(k1,k2,k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622a963-0ffa-4a2d-82ad-9d65eac58cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(combined_pipes)[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f36a1-dc7c-44a4-bc32-3aabc25b6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408f740-ec7d-40fa-9a4a-235842579931",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pipes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cc284-a4df-4151-a2dc-bb982e1e7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(combined_pipes)[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.export import *\n",
    "    from nbdev.export2html import *\n",
    "    from nbdev.cli import *\n",
    "    make_readme()\n",
    "    notebook2script(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e5a77-8683-42a3-b941-8c8a1adf9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "# dp.functional_datapipe('mux')\n",
    "# class MultiplexerMapDataPipe(dp.map.MapDataPipe):\n",
    "#     def __init__(self, *datapipes):\n",
    "#         self.datapipes = datapipes\n",
    "#         # datapipe import standard is import torchdata as dp. We need to make these\n",
    "#         # private\n",
    "#         for _dp in self.datapipes:\n",
    "#             if not isinstance(_dp,dp.map.MapDataPipe):\n",
    "#                 dp_types=[type(o) for o in self.datapipes]\n",
    "#                 raise ValueError(f'Passed in datapipes need to be MapDataPipes, got {dp_types}')\n",
    "        \n",
    "#         self.length: Optional[int] = None\n",
    "#         self._map = {}\n",
    "\n",
    "#     def _setup_datapipe_indexer(self, datapipe) -> Optional[Iterator[Any]]:\n",
    "#         # self._datapipe_iterator: Optional[Iterator[Any]] = None\n",
    "#         # Instead of _datapipe_iterator we have _datapipe_indexer\n",
    "#         # We need to know how to get the index from the main_datapipe. In order\n",
    "#         # to do this, we check if it is...\n",
    "        \n",
    "#         # NOTE: THIS IS NOT A GOOD SOLUTION SINCE THIS CANT RELY ON A STANDARD\n",
    "#         # INTERFACE FOR GETTING INDEXES\n",
    "        \n",
    "#         # We cash the indexes because we want to be able to have consistent behavior \n",
    "#         # when calling __getitem__ on a child pipe. \n",
    "#         # What we don't want is the main_datapipe being indexed by `str` but the\n",
    "#         # child pipes indexing by `int`...\n",
    "#         if isinstance(datapipe, dp.map.SequenceWrapper):\n",
    "#             return iter(range(len(datapipe)))\n",
    "#         elif hasattr(datapipe, '_map'):\n",
    "#             return iter(datapipe._map)\n",
    "#         elif hasattr(datapipe, 'index_map'):\n",
    "#             return iter(datapipe.index_map)\n",
    "#         else:\n",
    "#             warnings.warn('data pipe will be indexed by len')\n",
    "#             return iter(range(len(datapipe)))\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         iterators = [self._setup_datapipe_indexer(x) for x in self.datapipes]\n",
    "#         finished: Set[int] = set()\n",
    "#         while len(finished) < len(iterators):\n",
    "#             for i in range(len(iterators)):\n",
    "#                 if i not in finished:\n",
    "#                     try:\n",
    "#                         index = next(iterators[i])\n",
    "#                         # print(i,index)\n",
    "#                         value = self.datapipes[i][index]\n",
    "#                         # self._map will track which index is associated with \n",
    "#                         # which datapipe...\n",
    "#                         self._map[index] = i\n",
    "#                         yield value\n",
    "#                     except StopIteration:\n",
    "#                         finished.add(i)\n",
    "\n",
    "#     def __getitem__(self, index) -> T_co:\n",
    "#         if index in self._map:\n",
    "#             # self._map[index] -> the datapipe to getitem at, then pass index to \n",
    "#             # get the value \n",
    "#             return self.datapipes[self._map[index]][index]\n",
    "        \n",
    "#         # Remember that iter(self) adds index to self._map. So as we iter,\n",
    "#         # we can check if index has be found in a datapipe, and once found, return \n",
    "#         # that value.\n",
    "#         for value in self:\n",
    "#             if index in self._map: return value\n",
    "        \n",
    "#         raise IndexError(f'Unable to find {index} in the datapipes')\n",
    "                        \n",
    "#     def __len__(self):\n",
    "#         if self.length is not None:\n",
    "#             if self.length == -1:\n",
    "#                 raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n",
    "#             return self.length\n",
    "#         if all(isinstance(_dp, Sized) for _dp in self.datapipes):\n",
    "#             self.length = sum(len(_dp) for _dp in self.datapipes)\n",
    "#         else:\n",
    "#             self.length = -1\n",
    "#         return len(self)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
