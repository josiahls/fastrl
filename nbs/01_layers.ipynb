{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastrl.test_utils import initialize_notebook\n",
    "initialize_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "from copy import deepcopy\n",
    "from typing import Tuple,Callable,Optional\n",
    "from functools import partial\n",
    "# Third party libs\n",
    "from fastrl.torch_core import to_detach,Module\n",
    "from torch import nn\n",
    "import torch\n",
    "from fastcore.all import add_docs\n",
    "import numpy as np\n",
    "# Local modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258abcf",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> Functions and Modules for RL that are pure pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# 3dHistogram rendering code taken from:\n",
    "# https://stackoverflow.com/questions/60432713/filled-3d-histogram-from-2d-histogram-with-plotly\n",
    "# and\n",
    "# https://community.plotly.com/t/adding-a-shape-to-a-3d-plot/1441/10\n",
    "def triangulate_histogram(x, y, z):\n",
    "   \n",
    "    if len(x) != len(y) != len(z) :\n",
    "        raise ValueError(\"The  lists x, y, z, must have the same length\")\n",
    "    n = len(x)\n",
    "    if n % 2 :\n",
    "        raise ValueError(\"The length of lists x, y, z must be an even number\") \n",
    "    pts3d = np.vstack((x, y, z)).T\n",
    "    pts3dp = np.array([[x[2*k+1], y[2*k+1], 0] for k in range(1, n//2-1)])\n",
    "    pts3d = np.vstack((pts3d, pts3dp))\n",
    "    #triangulate the histogram bars:\n",
    "    tri = [[0,1,2], [0,2,n]]\n",
    "    for k, i  in zip(list(range(n, n-3+n//2)), list(range(3, n-4, 2))):\n",
    "        tri.extend([[k, i, i+1], [k, i+1, k+1]])\n",
    "    tri.extend([[n-3+n//2, n-3, n-2], [n-3+n//2, n-2, n-1]])      \n",
    "    return pts3d, np.array(tri)\n",
    "\n",
    "def _create_3d_mesh(layer:str,weights:torch.Tensor):\n",
    "    import plotly.graph_objects as go\n",
    "    a0=weights.tolist()\n",
    "    a0=np.repeat(a0,2).tolist()\n",
    "    a0.insert(0,0)\n",
    "    a0.pop()\n",
    "    a0[-1]=0\n",
    "    a1=np.arange(weights.shape[0]).tolist() \n",
    "    a1=np.repeat(a1,2)\n",
    "\n",
    "    verts, tri = triangulate_histogram([layer]*len(a0), a1, a0)\n",
    "    x, y, z = verts.T\n",
    "    I, J, K = tri.T\n",
    "    z = np.round(z.astype(float),4).astype(str)\n",
    "    return go.Mesh3d(x=x, y=y, z=z, i=I, j=J, k=K, opacity=0.7)\n",
    "\n",
    "def show_sequential_layer_weights(seq:nn.Sequential,title='Layer weights'):\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "    weights = {}\n",
    "    counter = {}\n",
    "    def append_weight_dict(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            counter['ln'] = counter.get('ln',0)+1\n",
    "            weights[f\"ln_{counter['ln']}\"] = to_detach(m.weight.view(-1,)).numpy()\n",
    "        elif type(m) == nn.Conv2d:\n",
    "            counter['conv'] = counter.get('conv',0)+1\n",
    "            weights[f\"conv_{counter['conv']}\"] = to_detach(m.weight.view(-1,)).numpy()\n",
    "\n",
    "    seq.apply(append_weight_dict)\n",
    "\n",
    "    max_len = max([a.shape[0] for a in weights.values()])\n",
    "\n",
    "    for k,v in weights.items():\n",
    "        pre_shape = v.shape[0]\n",
    "        pad = (max_len-pre_shape)//2\n",
    "        weights[k] = np.pad(v,pad)\n",
    "        diff = max_len-weights[k].shape[0]\n",
    "        if diff!=0: \n",
    "            weights[k] = np.hstack((weights[k],np.zeros(diff)))\n",
    "            \n",
    "    fig=go.Figure()\n",
    "    for layer,weights in weights.items():\n",
    "        fig.add_traces(_create_3d_mesh(layer,weights))\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Layer',\n",
    "            yaxis_title='Neuron',\n",
    "            zaxis_title='Weight Value',\n",
    "        ),\n",
    "        width=700,\n",
    "        height=600,\n",
    "        autosize=False,\n",
    "        margin=dict(l=30, r=30, b=50, t=10),\n",
    "        scene_camera_eye_z=0.8,\n",
    "    )\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ab3b6",
   "metadata": {},
   "source": [
    "Given a `nn.Sequential`, we can display the weights for `nn.Linear` and `nn.Conv2d` modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(2,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6,1)\n",
    ")\n",
    "show_sequential_layer_weights(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_xavier_uniform_weights(m:Module,bias=0.01):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.xavier_uniform_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5e85a",
   "metadata": {},
   "source": [
    "Show how `init_xavier_uniform_weights` affects the weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(2,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6,1)\n",
    ")\n",
    "layers.apply(init_xavier_uniform_weights)\n",
    "show_sequential_layer_weights(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_uniform_weights(m:Module,bound:float):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.uniform_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight,-bound,bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5e85a",
   "metadata": {},
   "source": [
    "Show how `init_uniform_weights` affects the weights and that they are\n",
    "randomly initialized between the bounds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(2,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6,1)\n",
    ")\n",
    "layers.apply(partial(init_uniform_weights,bound=0.002))\n",
    "show_sequential_layer_weights(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dff3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_kaiming_normal_weights(m:Module,bias=0.01):\n",
    "    \"Initializes weights for linear layers using `torch.nn.init.kaiming_normal_`\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5e85a",
   "metadata": {},
   "source": [
    "Show how `init_kaiming_normal_weights` affects the weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(2,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6,1)\n",
    ")\n",
    "layers.apply(init_kaiming_normal_weights)\n",
    "show_sequential_layer_weights(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2328bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def simple_conv2d_block(\n",
    "        # A tuple of state sizes generally representing an image of format: \n",
    "        # [channel,width,height]\n",
    "        state_sz:Tuple[int,int,int],\n",
    "        # Number of filters to use for each conv layer\n",
    "        filters:int=32,\n",
    "        # Activation function between each layer.\n",
    "        activation_fn=nn.ReLU,\n",
    "        # We assume the channels dim should be size 3 max. If it is more\n",
    "        # we assume the width/height are in the location of channel and need to\n",
    "        # be transposed.\n",
    "        ignore_warning:bool=False\n",
    "    ) -> Tuple[nn.Sequential,int]: # (Convolutional block,n_features_out)\n",
    "    \"Creates a 3 layer conv block from `state_sz` along with expected n_feature output shape.\"\n",
    "    channels = state_sz[0]\n",
    "    if channels>3 and not ignore_warning:\n",
    "        warn(f'Channels is {channels}>3 in state_sz {state_sz}')\n",
    "    layers = nn.Sequential(\n",
    "        nn.BatchNorm2d(channels),\n",
    "        nn.Conv2d(channels,channels,filters),\n",
    "        activation_fn(),\n",
    "        nn.Conv2d(channels,channels,filters),\n",
    "        activation_fn(),\n",
    "        nn.Conv2d(channels,channels,filters),   \n",
    "        nn.Flatten()\n",
    "    )\n",
    "    m_layers = deepcopy(layers).to(device='meta')\n",
    "    out_sz = m_layers(torch.ones((1,*state_sz),device='meta')).shape[-1]\n",
    "    return layers.to(device='cpu'),out_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_conv2d_block((3,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135bfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Critic(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_sz:int,  # The input dim of the state / flattened conv output\n",
    "            action_sz:int=0, # The input dim of the actions\n",
    "            hidden1:int=400,    # Number of neurons connected between the 2 input/output layers\n",
    "            hidden2:int=300,    # Number of neurons connected between the 2 input/output layers\n",
    "            head_layer:Module=nn.Linear, # Output layer\n",
    "            activation_fn:Module=nn.ReLU, # The activation function\n",
    "            weight_init_fn:Callable=init_kaiming_normal_weights, # The weight initialization strategy\n",
    "            # Final layer initialization strategy\n",
    "            final_layer_init_fn:Callable=partial(init_uniform_weights,bound=1e-4),\n",
    "            # For pixel inputs, we can plug in a `nn.Sequential` block from `ddpg_conv2d_block`.\n",
    "            # This means that actions will be feed into the second linear layer instead of the \n",
    "            # first.\n",
    "            conv_block:Optional[nn.Sequential]=None,\n",
    "            # Whether to do batch norm. \n",
    "            batch_norm:bool=False\n",
    "        ):\n",
    "        self.action_sz = action_sz\n",
    "        self.state_sz = state_sz\n",
    "        self.conv_block = conv_block\n",
    "        if conv_block is None:\n",
    "            if batch_norm:\n",
    "                ln_bn = nn.Sequential(\n",
    "                    nn.BatchNorm1d(state_sz+action_sz),\n",
    "                    nn.Linear(state_sz+action_sz,hidden1)\n",
    "                )\n",
    "            else:\n",
    "                ln_bn = nn.Linear(state_sz+action_sz,hidden1)\n",
    "            self.layers = nn.Sequential(\n",
    "                ln_bn,\n",
    "                activation_fn(),\n",
    "                nn.Linear(hidden1,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,1),\n",
    "            )\n",
    "        else:\n",
    "            self.conv_block = nn.Sequential(\n",
    "                self.conv_block,\n",
    "                nn.Linear(state_sz,hidden1),\n",
    "                activation_fn(),\n",
    "            )\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(hidden1+action_sz,hidden2),\n",
    "                activation_fn(),\n",
    "                head_layer(hidden2,1),\n",
    "            )\n",
    "        self.layers.apply(weight_init_fn)\n",
    "        if final_layer_init_fn is not None:\n",
    "            final_layer_init_fn(self.layers[-1])\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            s:torch.Tensor, # A single tensor of shape [Batch,`state_sz`]\n",
    "            a:torch.Tensor=None # A single tensor of shape [Batch,`action_sz`]\n",
    "            # A single tensor of shape [B,1] representing the cumulative value estimate of state+action combinations  \n",
    "        ) -> torch.Tensor: \n",
    "            if self.conv_block:\n",
    "                s = self.conv_block(s)\n",
    "            if a is None:\n",
    "                if self.action_sz!=0:\n",
    "                    raise RuntimeError(f'`action_sz` is not 0, but no action was provided.')\n",
    "                return self.layers(s)\n",
    "            return self.layers(torch.hstack((s,a)))\n",
    "\n",
    "add_docs(\n",
    "Critic,\n",
    "\"\"\"Takes a either:\n",
    " - 2 tensors of size [B,`state_sz`], [B,`action_sz`] \n",
    " - 1 tensor of size [B,`state_sz`] \n",
    " \n",
    " Returning -> [B,1] outputs a 1d tensor representing the Q value\"\"\",\n",
    "forward=\"\"\"Takes in either:\n",
    "- 2 tensors of a state tensor and action tensor\n",
    "or\n",
    "- a single state tensor  \n",
    "and outputs the Q value estimates of that state,action combination\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875954b",
   "metadata": {},
   "source": [
    "The `Critic` is used by `DDPG`,`TRPO` to estimate the Q value of state-action pairs.\n",
    "\n",
    "Check that low dim input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682dd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import test_eq\n",
    "from fastrl.torch_core import evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "critic = Critic(4,2)\n",
    "\n",
    "state = torch.randn(1,4)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad(),evaluating(critic):\n",
    "    test_eq(\n",
    "        str(critic(state,action)),\n",
    "        str(torch.tensor([[0.0083]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018856d",
   "metadata": {},
   "source": [
    "Check that image input works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "image_shape = (3,100,100)\n",
    "\n",
    "conv_block,feature_out = simple_conv2d_block(image_shape)\n",
    "critic = Critic(feature_out,2,conv_block=conv_block)\n",
    "\n",
    "state = torch.randn(1,*image_shape)\n",
    "action = torch.randn(1,2)\n",
    "\n",
    "with torch.no_grad(),evaluating(critic):\n",
    "    test_eq(\n",
    "        str(critic(state,action)),\n",
    "        str(torch.tensor([[0.0102]]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f17d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71a089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
