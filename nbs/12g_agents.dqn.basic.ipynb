{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.dqn.basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from collections import deque\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "import torchdata.datapipes as dp\n",
    "from torchdata.dataloader2 import DataLoader2\n",
    "from torch.utils.data.datapipes._typing import _DataPipeMeta, _IterDataPipeMeta\n",
    "from torchdata.dataloader2.graph import find_dps,traverse,DataPipe\n",
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.torch_core import *\n",
    "# Local modules\n",
    "\n",
    "from fastrl.core import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.fastai.data.block import *\n",
    "from fastrl.memory.experience_replay import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.agents.discrete import *\n",
    "from fastrl.loggers.core import *\n",
    "from fastrl.loggers.jupyter_visualizers import *\n",
    "from fastrl.learner.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# DQN Basic\n",
    "> Core DQN modules, pipes, and tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f79cf-4763-451d-92a2-817b532ba9d8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6241ea68-611d-4cdf-abdb-4e2a58af23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DQN(Module):\n",
    "    def __init__(self,\n",
    "                 state_sz:int,  # The input dim of the state\n",
    "                 action_sz:int, # The output dim of the actions\n",
    "                 hidden=512,    # Number of neurons connected between the 2 input/output layers\n",
    "                 head_layer:Module=Linear, # DQN extensions such as Dueling DQNs have custom heads\n",
    "                 activition_fn:Module=ReLU # The activiation fn used by `DQN`\n",
    "                ):\n",
    "        self.layers=Sequential(\n",
    "            Linear(state_sz,hidden),\n",
    "            activition_fn(),\n",
    "            head_layer(hidden,action_sz),\n",
    "        )\n",
    "    def forward(self,x): return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfe338-de27-4c61-a18d-d74ea33fde37",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "480529a0-a23f-425a-a165-d4915c96c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "DataPipeAugmentationFn = Callable[[DataPipe],Optional[DataPipe]]\n",
    "\n",
    "def DQNAgent(\n",
    "    model,\n",
    "    logger_bases=None,\n",
    "    min_epsilon=0.02,\n",
    "    max_epsilon=1,\n",
    "    max_steps=1000,\n",
    "    device='cpu',\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None\n",
    ")->AgentHead:\n",
    "    agent_base = AgentBase(model,logger_bases=logger_bases)\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = SimpleModelRunner(agent,device=device)\n",
    "    agent = ArgMaxer(agent)\n",
    "    agent = EpsilonSelector(agent,min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps,device=device)\n",
    "    if agent_base.logger_bases is not None: \n",
    "        agent = EpsilonCollector(agent,agent_base.logger_bases)\n",
    "    agent = ArgMaxer(agent,only_idx=True)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = PyPrimativeConverter(agent)\n",
    "    agent = AgentHead(agent)\n",
    "    \n",
    "    for fn in ifnone(dp_augmentation_fns,[]):\n",
    "        result = fn(agent)\n",
    "        if result is not None: agent = result\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "437f871a-64bb-42ac-868c-d37b0282c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "\n",
    "agent = DQNAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "441c6eac-7851-4539-aa91-295479d05cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tensor([1,2,3,4]).float()\n",
    "step = SimpleStep(state=input_tensor)\n",
    "\n",
    "for action in agent([step]):\n",
    "    print(action)\n",
    "    \n",
    "test_eq(input_tensor,tensor([1., 2., 3., 4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47810afd-4430-4ba6-a840-3d8e5878aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.envs.gym import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ccbec06-fcc5-4be3-8b46-cd76c2a56b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentHead.debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61711f0-7a1d-4b0e-bd81-46ff7547179a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SimpleStep(state=tensor([ 0.0441,  0.0067, -0.0204,  0.0302]), action=tensor(1.), next_state=tensor([ 0.0443,  0.2021, -0.0198, -0.2689]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(1.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(1), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0443,  0.2021, -0.0198, -0.2689]), action=tensor(0.), next_state=tensor([ 0.0483,  0.0073, -0.0252,  0.0175]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(2.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(2), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0483,  0.0073, -0.0252,  0.0175]), action=tensor(1.), next_state=tensor([ 0.0484,  0.2028, -0.0248, -0.2830]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(3.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(3), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0484,  0.2028, -0.0248, -0.2830]), action=tensor(0.), next_state=tensor([ 0.0525,  0.0080, -0.0305,  0.0018]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(4.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(4), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0525,  0.0080, -0.0305,  0.0018]), action=tensor(1.), next_state=tensor([ 0.0527,  0.2036, -0.0304, -0.3004]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(5.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(5), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0527,  0.2036, -0.0304, -0.3004]), action=tensor(0.), next_state=tensor([ 0.0567,  0.0089, -0.0364, -0.0174]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(6.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(6), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0567,  0.0089, -0.0364, -0.0174]), action=tensor(1.), next_state=tensor([ 0.0569,  0.2045, -0.0368, -0.3214]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(7.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(7), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0569,  0.2045, -0.0368, -0.3214]), action=tensor(0.), next_state=tensor([ 0.0610,  0.0099, -0.0432, -0.0405]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(8.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(8), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0610,  0.0099, -0.0432, -0.0405]), action=tensor(0.), next_state=tensor([ 0.0612, -0.1845, -0.0440,  0.2382]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(9.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(9), episode_n=tensor(1), image=tensor([0.]))],\n",
       " [SimpleStep(state=tensor([ 0.0612, -0.1845, -0.0440,  0.2382]), action=tensor(0.), next_state=tensor([ 0.0575, -0.3790, -0.0393,  0.5167]), terminated=tensor(False), truncated=tensor(False), reward=tensor(1.), total_reward=tensor(10.), env_id=tensor(140464213137360), proc_id=tensor(378), step_n=tensor(10), episode_n=tensor(1), image=tensor([0.]))]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Logger\n",
    "logger_base = ProgressBarLogger()\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "\n",
    "agent = DQNAgent(model,[logger_base])\n",
    "\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock(agent)\n",
    ")\n",
    "# dls = L(block.dataloaders(['CartPole-v1']*1,n=10,bs=1))\n",
    "pipes = L(block.datapipes(['CartPole-v1']*1,n=10))\n",
    "\n",
    "# list(dls[0])\n",
    "list(pipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c98be0-6288-443a-b4ab-9390fbe3081c",
   "metadata": {},
   "source": [
    "## Training DataPipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1578d4b1-fa02-49b4-a02a-6bb881a45d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.learner = find_dp(traverse(self),LearnerBase)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.done_mask = batch.terminated.reshape(-1,)\n",
    "            self.learner.next_q = self.learner.model(batch.next_state)\n",
    "            self.learner.next_q = self.learner.next_q.max(dim=1).values.reshape(-1,1)\n",
    "            self.learner.next_q[self.learner.done_mask] = 0 \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9eea139c-52c0-4005-9101-a45c25c2d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TargetCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,discount=0.99,nsteps=1):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.learner = None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.learner = find_dp(traverse(self),LearnerBase)\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.targets = batch.reward+self.learner.next_q*(self.discount**self.nsteps)\n",
    "            self.learner.pred = self.learner.model(batch.state)\n",
    "            self.learner.target_qs = self.learner.pred.clone()\n",
    "            self.learner.target_qs.scatter_(1,batch.action.long(),self.learner.targets)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6c1f7c0-8ae0-4bf9-b193-6f5ce0300606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe,discount=0.99,nsteps=1):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.discount = discount\n",
    "        self.nsteps = nsteps\n",
    "        self.learner = find_dp(traverse(self),LearnerBase)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.loss_grad = self.learner.loss_func(self.learner.pred, self.learner.target_qs)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42ca2d01-a3c4-4f9e-8fef-186c9e1f1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ModelLearnCalc(dp.iter.IterDataPipe):\n",
    "    def __init__(self,source_datapipe):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.learner = find_dp(traverse(self),LearnerBase)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            self.learner.loss_grad.backward()\n",
    "            self.learner.opt.step()\n",
    "            self.learner.opt.zero_grad()\n",
    "            self.learner.loss = self.learner.loss_grad.clone()\n",
    "            yield self.learner.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdb1161a-26de-4f73-b052-dfefbb8bc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class StepBatcher(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            device=None\n",
    "        ):\n",
    "        \"Converts multiple `StepType` into a single `StepType` with the fields concated.\"\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.device = device\n",
    "        \n",
    "    def vstack_by_fld(self,batch,fld):\n",
    "        try:\n",
    "            if self.device is None: return torch.vstack(tuple(getattr(step,fld) for step in batch))\n",
    "            return torch.vstack(tuple(getattr(step,fld) for step in batch)).to(torch.device(self.device))\n",
    "        except RuntimeError as e:\n",
    "            print(f'Failed to stack {fld} given batch: {batch}')\n",
    "            raise\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.source_datapipe:\n",
    "            cls = batch[0].__class__\n",
    "            yield cls(**{fld:self.vstack_by_fld(batch,fld) for fld in cls._fields})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a793dc72-c14a-48aa-bc7a-9bd8e865458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpisodeCollector(LogCollector):\n",
    "    \n",
    "    def episode_detach(self,step): \n",
    "        try:\n",
    "            v = step.episode_n.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return int(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting episode_n which is unexpected: \\n{step}')\n",
    "            raise\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for q in self.main_buffers: q.append(Record('episode',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    for q in self.main_buffers: q.append(Record('episode',self.episode_detach(step)))\n",
    "            else:\n",
    "                for q in self.main_buffers: q.append(Record('episode',self.episode_detach(steps)))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28aa721c-22fc-49e6-8bc4-fc01b20e6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LossCollector(LogCollector):\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase] # `LoggerBase`s that we want to send metrics to\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = [o.buffer for o in logger_bases]\n",
    "        self.learner = find_dp(traverse(self),LearnerBase)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for q in self.main_buffers: q.append(Record('loss',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            for q in self.main_buffers: q.append(Record('loss',self.learner.loss.cpu().detach().numpy()))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "576be92f-0a44-459b-a3a4-3db191cca95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RollingTerminatedRewardCollector(LogCollector):\n",
    "    debug=False\n",
    "    def __init__(self,\n",
    "         source_datapipe, # The parent datapipe, likely the one to collect metrics from\n",
    "         logger_bases:List[LoggerBase], # `LoggerBase`s that we want to send metrics to\n",
    "         rolling_length:int=100\n",
    "        ):\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.main_buffers = [o.buffer for o in logger_bases]\n",
    "        self.rolling_rewards = deque([],maxlen=rolling_length)\n",
    "        \n",
    "    def step2terminated(self,step): return bool(step.terminated)\n",
    "\n",
    "    def reward_detach(self,step): \n",
    "        try:\n",
    "            v = step.total_reward.cpu().detach().numpy()\n",
    "            if len(v.shape)==0: return float(v)\n",
    "            return v[0]\n",
    "        except IndexError:\n",
    "            print(f'Got IndexError getting reward which is unexpected: \\n{step}')\n",
    "            raise\n",
    "\n",
    "    def __iter__(self):\n",
    "        for q in self.main_buffers: q.append(Record('rolling_reward',None))\n",
    "        for steps in self.source_datapipe:\n",
    "            if self.debug: print(f'RollingTerminatedRewardCollector: ',steps)\n",
    "            if isinstance(steps,dp.DataChunk):\n",
    "                for step in steps:\n",
    "                    if self.step2terminated(step):\n",
    "                        self.rolling_rewards.append(self.reward_detach(step))\n",
    "                        for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))\n",
    "            elif self.step2terminated(steps):\n",
    "                self.rolling_rewards.append(self.reward_detach(steps))\n",
    "                for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))\n",
    "            yield steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c50c9dd-89fc-4a89-a6f0-ccd6126135d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DQNLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    logger_bases=None,\n",
    "    loss_func=MSELoss(),\n",
    "    opt=AdamW,\n",
    "    lr=0.005,\n",
    "    bs=128,\n",
    "    max_sz=10000,\n",
    "    nsteps=1,\n",
    "    device=None,\n",
    "    batches=None,\n",
    "    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(model,dls,batches=batches,loss_func=loss_func,opt=opt(model.parameters(),lr=lr))\n",
    "    learner = BatchCollector(learner,logger_bases=logger_bases,batch_on_pipe=LearnerBase)\n",
    "    learner = EpocherCollector(learner,logger_bases=logger_bases)\n",
    "    for logger_base in L(logger_bases): learner = logger_base.connect_source_datapipe(learner)\n",
    "    if logger_bases: \n",
    "        learner = RollingTerminatedRewardCollector(learner,logger_bases)\n",
    "        learner = EpisodeCollector(learner,logger_bases)\n",
    "    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz)\n",
    "    learner = StepBatcher(learner,device=device)\n",
    "    learner = QCalc(learner)\n",
    "    learner = TargetCalc(learner,nsteps=nsteps)\n",
    "    learner = LossCalc(learner)\n",
    "    learner = ModelLearnCalc(learner)\n",
    "    if logger_bases: \n",
    "        learner = LossCollector(learner,logger_bases)\n",
    "    learner = LearnerHead(learner)\n",
    "    \n",
    "    for fn in ifnone(dp_augmentation_fns,[]):\n",
    "        result = fn(learner)\n",
    "        if result is not None: learner = result\n",
    "    \n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f9ed8-fb05-40a1-ac0d-d4cafee8fa07",
   "metadata": {},
   "source": [
    "Try training with basic defaults..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2df20853-a286-4695-b3a4-8cf44f5bf70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>loss</th>\n",
       "      <th>episode</th>\n",
       "      <th>rolling_reward</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>epsilon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.25889987</td>\n",
       "      <td>37</td>\n",
       "      <td>27.194444</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.30326733</td>\n",
       "      <td>59</td>\n",
       "      <td>33.310345</td>\n",
       "      <td>2</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.499250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.28242528</td>\n",
       "      <td>74</td>\n",
       "      <td>40.493151</td>\n",
       "      <td>3</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.35705975</td>\n",
       "      <td>83</td>\n",
       "      <td>47.536585</td>\n",
       "      <td>4</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.2906451</td>\n",
       "      <td>90</td>\n",
       "      <td>55.235955</td>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.27346796</td>\n",
       "      <td>97</td>\n",
       "      <td>62.145833</td>\n",
       "      <td>6</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.31238076</td>\n",
       "      <td>103</td>\n",
       "      <td>65.950000</td>\n",
       "      <td>7</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.4066385</td>\n",
       "      <td>109</td>\n",
       "      <td>75.770000</td>\n",
       "      <td>8</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.2874854</td>\n",
       "      <td>116</td>\n",
       "      <td>86.540000</td>\n",
       "      <td>9</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.29815528</td>\n",
       "      <td>121</td>\n",
       "      <td>93.960000</td>\n",
       "      <td>10</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.26490104</td>\n",
       "      <td>127</td>\n",
       "      <td>101.330000</td>\n",
       "      <td>11</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.33486408</td>\n",
       "      <td>134</td>\n",
       "      <td>110.670000</td>\n",
       "      <td>12</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.26537025</td>\n",
       "      <td>141</td>\n",
       "      <td>118.800000</td>\n",
       "      <td>13</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.23182783</td>\n",
       "      <td>147</td>\n",
       "      <td>125.900000</td>\n",
       "      <td>14</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.24613154</td>\n",
       "      <td>151</td>\n",
       "      <td>134.500000</td>\n",
       "      <td>15</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5285428</td>\n",
       "      <td>156</td>\n",
       "      <td>140.820000</td>\n",
       "      <td>16</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.2444759</td>\n",
       "      <td>161</td>\n",
       "      <td>147.620000</td>\n",
       "      <td>17</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.36190802</td>\n",
       "      <td>165</td>\n",
       "      <td>154.110000</td>\n",
       "      <td>18</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.31972152</td>\n",
       "      <td>170</td>\n",
       "      <td>162.610000</td>\n",
       "      <td>19</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.26733634</td>\n",
       "      <td>172</td>\n",
       "      <td>170.170000</td>\n",
       "      <td>19</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup Loggers\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2).cuda()\n",
    "# Setup the Agent\n",
    "agent = DQNAgent(model,[logger_base],max_steps=4000,device='cuda')\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock(agent=agent,nsteps=1,nskips=1,firstlast=False)\n",
    ")\n",
    "\n",
    "dls = L(block.dataloaders(['CartPole-v1']*1,bs=1,num_workers=0))\n",
    "\n",
    "# Setup the Learner\n",
    "learner = DQNLearner(model,dls,batches=1000,logger_bases=[logger_base],bs=128,max_sz=100_000,device='cuda')\n",
    "# learner.fit(3)\n",
    "learner.fit(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0ed73-7bbf-415b-9ee7-9a95de31d638",
   "metadata": {},
   "source": [
    "If we try a regular DQN with nsteps/nskips it doesnt really converge after 130. We cant expect stability at all, and im pretty sure that nsteps (correctly) tries to reduce to number of duplicated states so that the agent can sample more unique state transitions. The problem with this is that the base dqn is not stable, so giving it lots of \"new\" stuff, im not sure helps. In otherwords, its going to forget the old stuff very quickly, and having duplicate states helps \"remind it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d510e-38c1-458c-9830-df5a68e6a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setfastrl.dataloader2_ext\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2)\n",
    "# Setup the Agent\n",
    "agent = DQNAgent(model,[logger_base],max_steps=10000)\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock(agent=agent,nsteps=2,nskips=2,firstlast=True) # We basically merge 2 steps into 1 and skip. \n",
    ")\n",
    "# pipes = L(block.datapipes(['CartPole-v1']*1,n=10))\n",
    "dls = L(block.dataloaders(['CartPole-v1']*1))\n",
    "# Setup the Learner\n",
    "learner = DQNLearner(model,dls,batches=1000,logger_bases=[logger_base],bs=128,max_sz=20_000,nsteps=2,lr=0.001)\n",
    "# learner.fit(3)\n",
    "learner.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0f41e-2eda-4227-9fa8-f2e0b920754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrl.loggers.jupyter_visualizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15937f36-8efa-4163-bc67-20019d18c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482490a-475e-4c46-b1fa-193bf68b7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.envs.gym import GymTypeTransform,GymStepper\n",
    "\n",
    "def gym_pipe_base(envs,total_steps,seed=0):\n",
    "    pipe = dp.map.Mapper(envs)\n",
    "    pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "    pipe = dp.iter.MapToIterConverter(pipe)\n",
    "    pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "    pipe = pipe.cycle(count=total_steps)\n",
    "    pipe = GymStepper(pipe,agent=agent,seed=seed,include_images=True)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc10e61-4f65-4a17-a5a9-489bb466af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8784650-6f5c-42b7-9a72-68a0d37d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_logger = SimpleJupyterVideoPlayer()\n",
    "\n",
    "pipe = gym_pipe_base(['CartPole-v1'],100,seed=None)\n",
    "pipe = ImageCollector(pipe,[video_logger])\n",
    "\n",
    "pipe = video_logger.connect_source_datapipe(pipe)\n",
    "\n",
    "L(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaa785-605b-4e75-bff7-bae8c5603817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Loggers\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(8,4)\n",
    "# Setup the Agent\n",
    "agent = DQNAgent(model,[logger_base])\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock(agent=agent)\n",
    ")\n",
    "dls = L(block.dataloaders(['LunarLander-v2']*1,n=1000,bs=1))\n",
    "# Setup the Learner\n",
    "learner = DQNLearner(model,dls,logger_bases=[logger_base])\n",
    "learner.fit(3)\n",
    "# learner.fit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e84a7d-9583-485d-8e16-3958c72b526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.envs.gym import GymTypeTransform,GymStepper\n",
    "\n",
    "def gym_pipe_base(envs,total_steps,seed=0):\n",
    "    pipe = dp.map.Mapper(envs)\n",
    "    pipe = TypeTransformLoop(pipe,[GymTypeTransform])\n",
    "    pipe = dp.iter.MapToIterConverter(pipe)\n",
    "    pipe = dp.iter.InMemoryCacheHolder(pipe)\n",
    "    pipe = pipe.cycle(count=total_steps)\n",
    "    pipe = GymStepper(pipe,agent=agent,seed=seed,include_images=True)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a8a98-5091-4e31-9cca-9220c64ecdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_logger = SimpleJupyterVideoPlayer()\n",
    "\n",
    "pipe = gym_pipe_base(['LunarLander-v2'],1000,seed=None)\n",
    "pipe = ImageCollector(pipe,[video_logger])\n",
    "\n",
    "pipe = video_logger.connect_source_datapipe(pipe)\n",
    "\n",
    "L(pipe);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d3626-1702-4f22-ae15-bb93a75bec68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
