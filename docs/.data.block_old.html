---

title: Data Block


keywords: fastai
sidebar: home_sidebar

summary: "Fastrl transforms for iterating through environments"
description: "Fastrl transforms for iterating through environments"
nb_path: "nbs/.data.block_old.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/.data.block_old.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DQN" class="doc_header"><code>class</code> <code>DQN</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/data/block.py#L25" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DQN</code>() :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TestDatasetNoModule" class="doc_header"><code>class</code> <code>TestDatasetNoModule</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/data/block.py#L38" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TestDatasetNoModule</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterableDataset</code></p>
</blockquote>
<p>An iterable Dataset.</p>
<p>All datasets that represent an iterable of data samples should subclass it.
Such form of datasets is particularly useful when data come from a stream.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this dataset.</p>
<p>When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
item in the dataset will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
different copy of the dataset object, so it is often desired to configure
each copy independently to avoid having duplicate data returned from the
workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
process, returns information about the worker. It can be used in either the
dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
:attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example 1: splitting workload across all workers in :meth:<code>__iter__</code>::</p>

<pre><code>&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         worker_info = torch.utils.data.get_worker_info()
...         if worker_info is None:  # single-process data loading, return the full iterator
...             iter_start = self.start
...             iter_end = self.end
...         else:  # in a worker process
...             # split workload
...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
...             worker_id = worker_info.id
...             iter_start = self.start + worker_id * per_worker
...             iter_end = min(iter_start + per_worker, self.end)
...         return iter(range(iter_start, iter_end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]

&gt;&gt;&gt; # Mult-process loading with two worker processes
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 5, 4, 6]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20)))
[3, 4, 5, 6]

</code></pre>
<p>Example 2: splitting workload across all workers using :attr:<code>worker_init_fn</code>::</p>

<pre><code>&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         return iter(range(self.start, self.end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]
&gt;&gt;&gt;
&gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 3, 4, 4, 5, 5, 6, 6]

&gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently
&gt;&gt;&gt; def worker_init_fn(worker_id):
...     worker_info = torch.utils.data.get_worker_info()
...     dataset = worker_info.dataset  # the dataset copy in this worker process
...     overall_start = dataset.start
...     overall_end = dataset.end
...     # configure the dataset to only process the split workload
...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))
...     worker_id = worker_info.id
...     dataset.start = overall_start + worker_id * per_worker
...     dataset.end = min(dataset.start + per_worker, overall_end)
...

&gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn`
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))
[3, 5, 4, 6]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))
[3, 4, 5, 6]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">TestDatasetNoModule</span><span class="p">(),</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>

<span class="ansi-red-fg">NotImplementedError</span>Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-251-901318d53911&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">for</span> x <span class="ansi-green-fg">in</span> DataLoader<span class="ansi-blue-fg">(</span>TestDatasetNoModule<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>num_workers<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>     print<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/fastai/data/load.py</span> in <span class="ansi-cyan-fg">__iter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    107</span>         self<span class="ansi-blue-fg">.</span>before_iter<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    108</span>         self<span class="ansi-blue-fg">.</span>__idxs<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>get_idxs<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># called in context of main process (not workers/subprocesses)</span>
<span class="ansi-green-fg">--&gt; 109</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">for</span> b <span class="ansi-green-fg">in</span> _loaders<span class="ansi-blue-fg">[</span>self<span class="ansi-blue-fg">.</span>fake_l<span class="ansi-blue-fg">.</span>num_workers<span class="ansi-blue-fg">==</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>fake_l<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    110</span>             <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>device <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> b <span class="ansi-blue-fg">=</span> to_device<span class="ansi-blue-fg">(</span>b<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>device<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    111</span>             <span class="ansi-green-fg">yield</span> self<span class="ansi-blue-fg">.</span>after_batch<span class="ansi-blue-fg">(</span>b<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    433</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_sampler_iter <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    434</span>             self<span class="ansi-blue-fg">.</span>_reset<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 435</span><span class="ansi-red-fg">         </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_data<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    436</span>         self<span class="ansi-blue-fg">.</span>_num_yielded <span class="ansi-blue-fg">+=</span> <span class="ansi-cyan-fg">1</span>
<span class="ansi-green-intense-fg ansi-bold">    437</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_dataset_kind <span class="ansi-blue-fg">==</span> _DatasetKind<span class="ansi-blue-fg">.</span>Iterable <span class="ansi-green-fg">and</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">_next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    473</span>     <span class="ansi-green-fg">def</span> _next_data<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         index <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_index<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 475</span><span class="ansi-red-fg">         </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_dataset_fetcher<span class="ansi-blue-fg">.</span>fetch<span class="ansi-blue-fg">(</span>index<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-intense-fg ansi-bold">    476</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_pin_memory<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             data <span class="ansi-blue-fg">=</span> _utils<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py</span> in <span class="ansi-cyan-fg">fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-intense-fg ansi-bold">     32</span>                 <span class="ansi-green-fg">raise</span> StopIteration
<span class="ansi-green-intense-fg ansi-bold">     33</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 34</span><span class="ansi-red-fg">             </span>data <span class="ansi-blue-fg">=</span> next<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dataset_iter<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     35</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>collate_fn<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     36</span> 

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/fastai/data/load.py</span> in <span class="ansi-cyan-fg">create_batches</span><span class="ansi-blue-fg">(self, samps)</span>
<span class="ansi-green-intense-fg ansi-bold">    114</span> 
<span class="ansi-green-intense-fg ansi-bold">    115</span>     <span class="ansi-green-fg">def</span> create_batches<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> samps<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 116</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>dataset <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>it <span class="ansi-blue-fg">=</span> iter<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dataset<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    117</span>         res <span class="ansi-blue-fg">=</span> filter<span class="ansi-blue-fg">(</span><span class="ansi-green-fg">lambda</span> o<span class="ansi-blue-fg">:</span>o <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> map<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>do_item<span class="ansi-blue-fg">,</span> samps<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    118</span>         <span class="ansi-green-fg">yield</span> <span class="ansi-green-fg">from</span> map<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>do_batch<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>chunkify<span class="ansi-blue-fg">(</span>res<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py</span> in <span class="ansi-cyan-fg">__iter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    144</span> 
<span class="ansi-green-intense-fg ansi-bold">    145</span>     <span class="ansi-green-fg">def</span> __iter__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> Iterator<span class="ansi-blue-fg">[</span>T_co<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 146</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">raise</span> NotImplementedError
<span class="ansi-green-intense-fg ansi-bold">    147</span> 
<span class="ansi-green-intense-fg ansi-bold">    148</span>     <span class="ansi-green-fg">def</span> __add__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> other<span class="ansi-blue-fg">:</span> Dataset<span class="ansi-blue-fg">[</span>T_co<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">NotImplementedError</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TestDataset" class="doc_header"><code>class</code> <code>TestDataset</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/data/block.py#L60" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TestDataset</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterableDataset</code></p>
</blockquote>
<p>An iterable Dataset.</p>
<p>All datasets that represent an iterable of data samples should subclass it.
Such form of datasets is particularly useful when data come from a stream.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this dataset.</p>
<p>When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
item in the dataset will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
different copy of the dataset object, so it is often desired to configure
each copy independently to avoid having duplicate data returned from the
workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
process, returns information about the worker. It can be used in either the
dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
:attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example 1: splitting workload across all workers in :meth:<code>__iter__</code>::</p>

<pre><code>&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         worker_info = torch.utils.data.get_worker_info()
...         if worker_info is None:  # single-process data loading, return the full iterator
...             iter_start = self.start
...             iter_end = self.end
...         else:  # in a worker process
...             # split workload
...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
...             worker_id = worker_info.id
...             iter_start = self.start + worker_id * per_worker
...             iter_end = min(iter_start + per_worker, self.end)
...         return iter(range(iter_start, iter_end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]

&gt;&gt;&gt; # Mult-process loading with two worker processes
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 5, 4, 6]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20)))
[3, 4, 5, 6]

</code></pre>
<p>Example 2: splitting workload across all workers using :attr:<code>worker_init_fn</code>::</p>

<pre><code>&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         return iter(range(self.start, self.end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]
&gt;&gt;&gt;
&gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 3, 4, 4, 5, 5, 6, 6]

&gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently
&gt;&gt;&gt; def worker_init_fn(worker_id):
...     worker_info = torch.utils.data.get_worker_info()
...     dataset = worker_info.dataset  # the dataset copy in this worker process
...     overall_start = dataset.start
...     overall_end = dataset.end
...     # configure the dataset to only process the split workload
...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))
...     worker_id = worker_info.id
...     dataset.start = overall_start + worker_id * per_worker
...     dataset.end = min(dataset.start + per_worker, overall_end)
...

&gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn`
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))
[3, 5, 4, 6]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))
[3, 4, 5, 6]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mp</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dqn</span><span class="o">=</span><span class="n">DQN</span><span class="p">()</span>
<span class="n">ds</span><span class="o">=</span><span class="n">TestDataset</span><span class="p">(</span><span class="n">dqn</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dqn</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">ds</span><span class="o">.</span><span class="n">pids</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span> <span class="nb">print</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">pids</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">ds</span><span class="o">.</span><span class="n">envs</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span> <span class="nb">print</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">envs</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="c1"># print(list(ds.envs))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0290, -0.2360, -0.0339,  0.2361])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0256, -0.2272, -0.0222,  0.2737])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0462, -0.2335,  0.0098,  0.2826])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0214, -0.1958, -0.0264,  0.2993])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0428, -0.2287,  0.0270,  0.3159])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0307, -0.1540,  0.0202,  0.2689])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0435, -0.2423, -0.0322,  0.3182])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0360, -0.1623,  0.0246,  0.2944])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0426, -0.1676, -0.0286,  0.2643])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0416, -0.1694, -0.0244,  0.2964])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0456, -0.1895, -0.0380,  0.2938])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0221, -0.1848,  0.0375,  0.3185])
139977286222800  
pid is:  54
tensor(1)
tensor([-0.0294,  0.1642, -0.0277, -0.3160])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0377, -0.1496, -0.0296,  0.3157])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0032, -0.2257,  0.0449,  0.2742])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0386, -0.1656,  0.0078,  0.3075])
139977286222800  
pid is:  54
tensor(1)
tensor([-0.0482,  0.1807, -0.0107, -0.3208])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0205, -0.1500,  0.0220,  0.3293])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0368, -0.1706, -0.0032,  0.2508])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0395, -0.2452,  0.0116,  0.2529])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0482, -0.2024,  0.0080,  0.3189])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0136, -0.2156,  0.0093,  0.3041])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0284, -0.2286, -0.0391,  0.3200])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0445, -0.2286, -0.0064,  0.3200])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0374, -0.1655,  0.0005,  0.3019])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0055, -0.1880,  0.0478,  0.2920])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0254, -0.1594, -0.0493,  0.2550])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0447, -0.1770,  0.0368,  0.2912])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0286, -0.1521,  0.0040,  0.2611])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0092, -0.2092,  0.0481,  0.3025])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0466, -0.1695, -0.0247,  0.3242])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0166, -0.1892,  0.0351,  0.2813])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0132, -0.2175,  0.0227,  0.3002])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0317, -0.1900, -0.0265,  0.2861])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0459, -0.1617,  0.0486,  0.3340])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0254, -0.2037,  0.0141,  0.2709])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0081, -0.2199, -0.0340,  0.2583])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0339, -0.2199, -0.0094,  0.3366])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0223, -0.1864,  0.0046,  0.2890])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0478, -0.2360, -0.0258,  0.3255])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0252, -0.2285, -0.0198,  0.2982])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0194, -0.1632, -0.0169,  0.3274])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0062, -0.2059,  0.0121,  0.3064])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0124, -0.2099, -0.0107,  0.3382])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0210, -0.1900, -0.0322,  0.3042])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0132, -0.1975,  0.0329,  0.3380])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0420, -0.2327,  0.0239,  0.2711])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0158, -0.2130, -0.0127,  0.2962])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0019, -0.1802,  0.0278,  0.3054])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0206, -0.1978,  0.0063,  0.3172])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0227, -0.1488,  0.0429,  0.2813])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0483, -0.1458,  0.0064,  0.2680])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0412, -0.1852,  0.0481,  0.2690])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0034, -0.1498,  0.0141,  0.3176])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0333, -0.2354,  0.0424,  0.3435])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0045, -0.2349,  0.0313,  0.3231])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0505, -0.1449, -0.0271,  0.3298])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0040, -0.1490, -0.0376,  0.3003])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0221, -0.1711, -0.0072,  0.3308])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0130, -0.1747,  0.0301,  0.3325])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0218, -0.1523,  0.0356,  0.2752])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0129, -0.1904, -0.0369,  0.2728])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0050, -0.2113,  0.0199,  0.2962])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0123, -0.2120, -0.0395,  0.2897])
139977286222800  
pid is:  54
tensor(1)
tensor([-0.0231,  0.1672, -0.0373, -0.3238])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0398, -0.1454, -0.0494,  0.2827])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0244, -0.1743, -0.0075,  0.2853])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0477, -0.2429, -0.0396,  0.2953])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0034, -0.1943, -0.0311,  0.2641])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0266, -0.1765,  0.0076,  0.2492])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0231, -0.1868,  0.0118,  0.3035])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0112, -0.1466, -0.0460,  0.2586])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0416, -0.2125,  0.0144,  0.3087])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0080, -0.1670,  0.0050,  0.3008])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0081, -0.1977,  0.0064,  0.2556])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0462, -0.1481, -0.0070,  0.3083])
139977286222800  
pid is:  54
tensor(1)
tensor([ 0.0028,  0.1636, -0.0325, -0.3391])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0482, -0.2095,  0.0210,  0.2778])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0157, -0.1686,  0.0251,  0.2932])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0373, -0.2379,  0.0100,  0.2559])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0155, -0.2002,  0.0324,  0.2703])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0133, -0.1741, -0.0185,  0.3297])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0135, -0.2444,  0.0033,  0.3252])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0247, -0.2190,  0.0399,  0.3254])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0461, -0.2146, -0.0101,  0.3287])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0327, -0.2412,  0.0263,  0.3118])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0239, -0.1454, -0.0200,  0.3335])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0381, -0.1867,  0.0120,  0.3182])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0152, -0.1700,  0.0342,  0.3143])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0335, -0.1860, -0.0365,  0.3194])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0206, -0.1532, -0.0310,  0.3172])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0074, -0.2299,  0.0343,  0.2992])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0111, -0.1742,  0.0098,  0.3003])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0483, -0.1611, -0.0173,  0.3307])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0329, -0.1640,  0.0063,  0.2619])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0422, -0.2151, -0.0412,  0.2924])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0211, -0.2071,  0.0259,  0.2658])
139977286222800  
pid is:  54
tensor(0)
tensor([-0.0444, -0.1600,  0.0262,  0.2669])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0176, -0.2351, -0.0085,  0.3014])
139977286222800  
pid is:  54
tensor(0)
tensor([ 0.0354, -0.2190,  0.0167,  0.3345])
54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 54 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 139977286222800 </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

