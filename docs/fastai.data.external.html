---

title: External data


keywords: fastai
sidebar: home_sidebar

summary: "Helper functions to download the fastai datasets"
description: "Helper functions to download the fastai datasets"
nb_path: "nbs/02b_fastai.data.external.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02b_fastai.data.external.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To download any of the datasets or pretrained weights, simply run <a href="/fastrl/fastai.data.external.html#untar_data"><code>untar_data</code></a> by passing any dataset name mentioned above like so:</p>
<div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">PETS</span><span class="p">)</span>
<span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
<span class="o">&gt;</span> <span class="o">&gt;</span> <span class="p">(</span><span class="c1">#7393) [Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/keeshond_34.jpg&#39;),...]</span>
</pre></div>
<p>To download model pretrained weights:```python path = untar_data(URLs.PETS)
path.ls()</p>
<blockquote><blockquote><p>(#2) [Path('/home/ubuntu/.fastai/data/wt103-bwd/itos_wt103.pkl'),Path('/home/ubuntu/.fastai/data/wt103-bwd/lstm_bwd.pth')]
```</p>
</blockquote>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Datasets">Datasets<a class="anchor-link" href="#Datasets"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A complete list of datasets that are available by default inside the library are:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Main-datasets">Main datasets<a class="anchor-link" href="#Main-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>ADULT_SAMPLE</strong>: A small of the <a href="https://archive.ics.uci.edu/ml/datasets/Adult">adults dataset</a> to  predict whether income exceeds $50K/yr based on census data. </li>
</ol>
<ul>
<li><strong>BIWI_SAMPLE</strong>: A <a href="https://www.kaggle.com/kmader/biwi-kinect-head-pose-database">BIWI kinect headpose database</a>. The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. </li>
</ul>
<ol>
<li><strong>CIFAR</strong>: The famous <a href="https://www.cs.toronto.edu/~kriz/cifar.html">cifar-10</a> dataset which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.      </li>
<li><strong>COCO_SAMPLE</strong>: A sample of the <a href="http://cocodataset.org/#home">coco dataset</a> for object detection. </li>
<li><strong>COCO_TINY</strong>: A tiny version of the <a href="http://cocodataset.org/#home">coco dataset</a> for object detection.</li>
</ol>
<ul>
<li><strong>HUMAN_NUMBERS</strong>: A synthetic dataset consisting of human number counts in text such as one, two, three, four.. Useful for experimenting with Language Models.</li>
<li><p><strong>IMDB</strong>: The full <a href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDB sentiment analysis dataset</a>.</p>
</li>
<li><p><strong>IMDB_SAMPLE</strong>: A sample of the full <a href="https://ai.stanford.edu/~amaas/data/sentiment/">IMDB sentiment analysis dataset</a>.</p>
</li>
<li><strong>ML_SAMPLE</strong>: A movielens sample dataset for recommendation engines to recommend movies to users.            </li>
<li><strong>ML_100k</strong>: The movielens 100k dataset for recommendation engines to recommend movies to users.             </li>
<li><strong>MNIST_SAMPLE</strong>: A sample of the famous <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> consisting of handwritten digits.        </li>
<li><strong>MNIST_TINY</strong>: A tiny version of the famous <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> consisting of handwritten digits.                   </li>
<li><strong>MNIST_VAR_SIZE_TINY</strong>:  </li>
<li><strong>PLANET_SAMPLE</strong>: A sample of the planets dataset from the Kaggle competition <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">Planet: Understanding the Amazon from Space</a>.</li>
<li><strong>PLANET_TINY</strong>: A tiny version  of the planets dataset from the Kaggle competition <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">Planet: Understanding the Amazon from Space</a> for faster experimentation and prototyping.</li>
<li><strong>IMAGENETTE</strong>: A smaller version of the <a href="http://www.image-net.org/">imagenet dataset</a> pronounced just like 'Imagenet', except with a corny inauthentic French accent. </li>
<li><strong>IMAGENETTE_160</strong>: The 160px version of the Imagenette dataset.      </li>
<li><strong>IMAGENETTE_320</strong>: The 320px version of the Imagenette dataset. </li>
<li><strong>IMAGEWOOF</strong>: Imagewoof is a subset of 10 classes from Imagenet that aren't so easy to classify, since they're all dog breeds.</li>
<li><strong>IMAGEWOOF_160</strong>: 160px version of the ImageWoof dataset.        </li>
<li><strong>IMAGEWOOF_320</strong>: 320px version of the ImageWoof dataset.</li>
<li><strong>IMAGEWANG</strong>: Imagewang contains Imagenette and Imagewoof combined, but with some twists that make it into a tricky semi-supervised unbalanced classification problem</li>
<li><strong>IMAGEWANG_160</strong>: 160px version of Imagewang.        </li>
<li><strong>IMAGEWANG_320</strong>: 320px version of Imagewang. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Kaggle-competition-datasets">Kaggle competition datasets<a class="anchor-link" href="#Kaggle-competition-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>DOGS</strong>: Image dataset consisting of dogs and cats images from <a href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs Cats kaggle competition</a>. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Image-Classification-datasets">Image Classification datasets<a class="anchor-link" href="#Image-Classification-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>CALTECH_101</strong>: Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc 'Aurelio Ranzato.</li>
<li>CARS: The <a href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html">Cars dataset</a> contains 16,185 images of 196 classes of cars.   </li>
<li><strong>CIFAR_100</strong>: The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class.   </li>
<li><strong>CUB_200_2011</strong>: Caltech-UCSD Birds-200-2011 (CUB-200-2011) is an extended version of the CUB-200 dataset, with roughly double the number of images per class and new part location annotations</li>
<li><strong>FLOWERS</strong>: 17 category <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/">flower dataset</a> by gathering images from various websites.</li>
<li><strong>FOOD</strong>:         </li>
<li><strong>MNIST</strong>: <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> consisting of handwritten digits.      </li>
<li><strong>PETS</strong>: A 37 category <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">pet dataset</a> with roughly 200 images for each class.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="NLP-datasets">NLP datasets<a class="anchor-link" href="#NLP-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>AG_NEWS</strong>: The AG News corpus consists of news articles from the AGâ€™s corpus of news articles on the web pertaining to the 4 largest classes. The dataset contains 30,000 training and 1,900 testing examples for each class.</li>
<li><strong>AMAZON_REVIEWS</strong>: This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.</li>
<li><strong>AMAZON_REVIEWS_POLARITY</strong>: Amazon reviews dataset for sentiment analysis.</li>
<li><strong>DBPEDIA</strong>: The DBpedia ontology dataset contains 560,000 training samples and 70,000 testing samples for each of 14 nonoverlapping classes from DBpedia. </li>
<li><strong>MT_ENG_FRA</strong>: Machine translation dataset from English to French.</li>
<li><strong>SOGOU_NEWS</strong>: <a href="http://www.thuir.cn/data-srr/">The Sogou-SRR</a> (Search Result Relevance) dataset was constructed to support researches on search engine relevance estimation and ranking tasks.</li>
<li><strong>WIKITEXT</strong>: The <a href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText language modeling dataset</a> is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.  </li>
<li><strong>WIKITEXT_TINY</strong>: A tiny version of the WIKITEXT dataset.</li>
<li><strong>YAHOO_ANSWERS</strong>: YAHOO's question answers dataset.</li>
<li><strong>YELP_REVIEWS</strong>: The <a href="https://www.yelp.com/dataset">Yelp dataset</a> is a subset of YELP businesses, reviews, and user data for use in personal, educational, and academic purposes</li>
<li><strong>YELP_REVIEWS_POLARITY</strong>: For sentiment classification on YELP reviews.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Image-localization-datasets">Image localization datasets<a class="anchor-link" href="#Image-localization-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>BIWI_HEAD_POSE</strong>: A <a href="https://www.kaggle.com/kmader/biwi-kinect-head-pose-database">BIWI kinect headpose database</a>. The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. </li>
<li><strong>CAMVID</strong>: Consists of driving labelled dataset for segmentation type models.</li>
<li><strong>CAMVID_TINY</strong>: A tiny camvid dataset for segmentation type models.</li>
<li><strong>LSUN_BEDROOMS</strong>: <a href="https://arxiv.org/abs/1506.03365">Large-scale Image Dataset</a> using Deep Learning with Humans in the Loop</li>
<li><strong>PASCAL_2007</strong>: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">Pascal 2007 dataset</a> to recognize objects from a number of visual object classes in realistic scenes.</li>
<li><strong>PASCAL_2012</strong>: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal 2012 dataset</a> to recognize objects from a number of visual object classes in realistic scenes.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Audio-classification">Audio classification<a class="anchor-link" href="#Audio-classification"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>MACAQUES</strong>: <a href="https://datadryad.org/stash/dataset/doi:10.5061/dryad.7f4p9">7285 macaque coo calls</a> across 8 individuals from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806230">Distributed acoustic cues for caller identity in macaque vocalization</a>.</li>
<li><strong>ZEBRA_FINCH</strong>: <a href="https://ndownloader.figshare.com/articles/11905533/versions/1">3405 zebra finch calls</a> classified <a href="https://link.springer.com/article/10.1007/s10071-015-0933-6">across 11 call types</a>. Additional labels include name of individual making the vocalization and its age.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Medical-imaging-datasets">Medical imaging datasets<a class="anchor-link" href="#Medical-imaging-datasets"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>SIIM_SMALL</strong>: A smaller version of the <a href="https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview">SIIM dataset</a> where the objective is to classify pneumothorax from a set of chest radiographic images.</li>
<li><p><strong>TCGA_SMALL</strong>: A smaller version of the <a href="http://doi.org/10.7937/K9/TCIA.2016.NDO1MDFQ">TCGA-OV dataset</a> with subcutaneous and visceral fat segmentations. Citations:</p>
<p>Holback, C., Jarosz, R., Prior, F., Mutch, D. G., Bhosale, P., Garcia, K., â€¦ Erickson, B. J. (2016). Radiology Data from The Cancer Genome Atlas Ovarian Cancer [TCGA-OV] collection. The Cancer Imaging Archive. <a href="http://doi.org/10.7937/K9/TCIA.2016.NDO1MDFQ">http://doi.org/10.7937/K9/TCIA.2016.NDO1MDFQ</a></p>
<p>Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. <a href="https://link.springer.com/article/10.1007/s10278-013-9622-7">https://link.springer.com/article/10.1007/s10278-013-9622-7</a></p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pretrained-models">Pretrained models<a class="anchor-link" href="#Pretrained-models"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><strong>OPENAI_TRANSFORMER</strong>: The GPT2 Transformer pretrained weights.</li>
<li><strong>WT103_FWD</strong>: The WikiText-103 forward language model weights.</li>
<li><strong>WT103_BWD</strong>: The WikiText-103 backward language model weights.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Config">Config<a class="anchor-link" href="#Config"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fastai_cfg" class="doc_header"><code>fastai_cfg</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/external.py#L12" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fastai_cfg</code>()</p>
</blockquote>
<p><code>Config</code> object for fastai's <code>config.ini</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a basic <code>Config</code> file that consists of <code>data</code>, <code>model</code>, <code>storage</code> and <code>archive</code>. 
All future downloads occur at the paths defined in the config file based on the type of download. For example, all future fastai datasets are downloaded to the <code>data</code> while all pretrained model weights are download to <code>model</code> unless the default download location is updated.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cfg</span> <span class="o">=</span> <span class="n">fastai_cfg</span><span class="p">()</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">cfg</span><span class="o">.</span><span class="n">path</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;data&#39;, Path(&#39;/home/fastrl_user/.fastai/data&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fastai_path" class="doc_header"><code>fastai_path</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/external.py#L19" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fastai_path</code>(<strong><code>folder</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Local path to <code>folder</code> in <code>Config</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fastai_path</span><span class="p">(</span><span class="s1">&#39;archive&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path(&#39;/home/fastrl_user/.fastai/archive&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="URLs" class="doc_header"><code>class</code> <code>URLs</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/external.py#L24" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>URLs</code>()</p>
</blockquote>
<p>Global constants for dataset and model URLs.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The default local path is at <code>~/.fastai/archive/</code> but this can be updated by passing a different <code>c_key</code>. Note: <code>c_key</code> should be one of <code>'archive', 'data', 'model', 'storage'</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">PETS</span>
<span class="n">local_path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">path</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">local_path</span><span class="o">.</span><span class="n">parent</span><span class="p">,</span> <span class="n">fastai_path</span><span class="p">(</span><span class="s1">&#39;archive&#39;</span><span class="p">))</span>
<span class="n">local_path</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path(&#39;/home/fastrl_user/.fastai/archive/oxford-iiit-pet.tgz&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">local_path</span> <span class="o">=</span> <span class="n">URLs</span><span class="o">.</span><span class="n">path</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">c_key</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">local_path</span><span class="o">.</span><span class="n">parent</span><span class="p">,</span> <span class="n">fastai_path</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">))</span>
<span class="n">local_path</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path(&#39;/home/fastrl_user/.fastai/models/oxford-iiit-pet.tgz&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="untar_data" class="doc_header"><code>untar_data</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/external.py#L124" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>untar_data</code>(<strong><code>url</code></strong>:<code>str</code>, <strong><code>archive</code></strong>:<code>Path</code>=<em><code>None</code></em>, <strong><code>data</code></strong>:<code>Path</code>=<em><code>None</code></em>, <strong><code>c_key</code></strong>:<code>str</code>=<em><code>'data'</code></em>, <strong><code>force_download</code></strong>:<code>bool</code>=<em><code>False</code></em>)</p>
</blockquote>
<p>Download <code>url</code> to <code>fname</code> if <code>dest</code> doesn't exist, and extract to folder <code>dest</code></p>
<table>
<thead><tr>
<th></th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>url</code></strong></td>
<td><code>str</code></td>
<td></td>
<td>File to download</td>
</tr>
<tr>
<td><strong><code>archive</code></strong></td>
<td><code>Path</code></td>
<td><code>None</code></td>
<td>Optional override for <code>Config</code>'s <code>archive</code> key</td>
</tr>
<tr>
<td><strong><code>data</code></strong></td>
<td><code>Path</code></td>
<td><code>None</code></td>
<td>Optional override for <code>Config</code>'s <code>data</code> key</td>
</tr>
<tr>
<td><strong><code>c_key</code></strong></td>
<td><code>str</code></td>
<td><code>data</code></td>
<td>Key in <code>Config</code> where to extract file</td>
</tr>
<tr>
<td><strong><code>force_download</code></strong></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>Setting to <code>True</code> will overwrite any existing copy of data</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/fastrl/fastai.data.external.html#untar_data"><code>untar_data</code></a> is a thin wrapper for <code>FastDownload.get</code>. It downloads and extracts <code>url</code>, by default to subdirectories of <code>~/.fastai</code>, and returns the path to the extracted data. Setting the <code>force_download</code> flag to 'True' will overwrite any existing copy of the data already present. For an explanation of the <code>c_key</code> parameter, see <a href="/fastrl/fastai.data.external.html#URLs"><code>URLs</code></a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>


