---

title: Loading


keywords: fastai
sidebar: home_sidebar

summary: "Objects using the `Loop` and `DataPipe` API for DataLoading"
description: "Objects using the `Loop` and `DataPipe` API for DataLoading"
nb_path: "nbs/02d_fastai.data.load.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02d_fastai.data.load.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will replicate <a href="https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastai mnist loading</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="n">untar_data</span><span class="p">,</span><span class="n">URLs</span><span class="p">,</span><span class="n">get_image_files</span><span class="p">,</span><span class="n">PILImage</span><span class="p">,</span><span class="n">ToTensor</span>

<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>

<span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/7&#39;),Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/3&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we create the dataset...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TypeTransformLoop" class="doc_header"><code>class</code> <code>TypeTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L21" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TypeTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>MapDataPipe</code></p>
</blockquote>
<p>Map-style DataPipe.</p>
<p>All datasets that represent a map from keys to data samples should subclass this.
Subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given, unique key. Subclasses can also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>MapDataPipe</code> (recommend, available to most but not all DataPipes).</p>
<p>Note:
    :class:<code>~torch.utils.data.DataLoader</code> by default constructs an index
    sampler that yields integral indices. To make it work with a map-style
    DataPipe with non-integral indices/keys, a custom sampler must be provided.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.map import SequenceWrapper, Mapper
&gt;&gt;&gt; dp = SequenceWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; map_dp_2 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; batch_dp = map_dp_1.batch(batch_size=2)
&gt;&gt;&gt; list(batch_dp)
[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ItemTransformLoop" class="doc_header"><code>class</code> <code>ItemTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L32" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ItemTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BatchTransformLoop" class="doc_header"><code>class</code> <code>BatchTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L41" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BatchTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_loader_loop" class="doc_header"><code>default_loader_loop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L50" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_loader_loop</code>(<strong><code>items</code></strong>, <strong><code>splitter</code></strong>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>type_tfms</code></strong>=<em><code>None</code></em>, <strong><code>item_tfms</code></strong>=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>=<em><code>None</code></em>, <strong><code>bs</code></strong>=<em><code>2</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GrandparentSplitter" class="doc_header"><code>GrandparentSplitter</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L75" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GrandparentSplitter</code>(<strong><code>train</code></strong>=<em><code>'train'</code></em>, <strong><code>valid</code></strong>=<em><code>'valid'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">default_loader_loop</span><span class="p">(</span>
    <span class="n">L</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)),</span>
    <span class="n">GrandparentSplitter</span><span class="p">(),</span>
    <span class="n">type_tfms</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span><span class="n">ToTensor</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># default_constructor(</span>
<span class="c1">#     dp.iter.Zipper(*base),</span>
    
<span class="c1"># )</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>DataLoader2<span class="o">?</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea ">
<pre><span class="ansi-red-fg">Init signature:</span>
DataLoader2<span class="ansi-blue-fg">(</span>
    dataset<span class="ansi-blue-fg">,</span>
    batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
    shuffle<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    sampler<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    batch_sampler<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    num_workers<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    collate_fn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    pin_memory<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    drop_last<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    timeout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    worker_init_fn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    prefetch_factor<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span>
    persistent_workers<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    batch_outside_worker<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    parallelism_mode<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;mp&#39;</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>      &lt;no docstring&gt;
<span class="ansi-red-fg">File:</span>           /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader_experimental.py
<span class="ansi-red-fg">Type:</span>           type
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">    177</span>             <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 178</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> elem_type<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    179</span>             <span class="ansi-green-fg">except</span> TypeError<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">&lt;listcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-intense-fg ansi-bold">    177</span>             <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 178</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> elem_type<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    179</span>             <span class="ansi-green-fg">except</span> TypeError<span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">    140</span>             out <span class="ansi-blue-fg">=</span> elem<span class="ansi-blue-fg">.</span>new<span class="ansi-blue-fg">(</span>storage<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>resize_<span class="ansi-blue-fg">(</span>len<span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>list<span class="ansi-blue-fg">(</span>elem<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 141</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> torch<span class="ansi-blue-fg">.</span>stack<span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> out<span class="ansi-blue-fg">=</span>out<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    142</span>     <span class="ansi-green-fg">elif</span> elem_type<span class="ansi-blue-fg">.</span>__module__ <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;numpy&#39;</span> <span class="ansi-green-fg">and</span> elem_type<span class="ansi-blue-fg">.</span>__name__ <span class="ansi-blue-fg">!=</span> <span class="ansi-blue-fg">&#39;str_&#39;</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>

<span class="ansi-red-fg">TypeError</span>: issubclass() arg 2 must be a class or tuple of classes

During handling of the above exception, another exception occurred:

<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_479/2655334691.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">for</span> element <span class="ansi-green-fg">in</span> train_dl<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>     print<span class="ansi-blue-fg">(</span>element<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     <span class="ansi-green-fg">break</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    557</span>                 <span class="ansi-red-fg"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="ansi-green-intense-fg ansi-bold">    558</span>                 self<span class="ansi-blue-fg">.</span>_reset<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># type: ignore[call-arg]</span>
<span class="ansi-green-fg">--&gt; 559</span><span class="ansi-red-fg">             </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_data<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    560</span>             self<span class="ansi-blue-fg">.</span>_num_yielded <span class="ansi-blue-fg">+=</span> <span class="ansi-cyan-fg">1</span>
<span class="ansi-green-intense-fg ansi-bold">    561</span>             <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_dataset_kind <span class="ansi-blue-fg">==</span> _DatasetKind<span class="ansi-blue-fg">.</span>Iterable <span class="ansi-green-fg">and</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">_next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    597</span>     <span class="ansi-green-fg">def</span> _next_data<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    598</span>         index <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_index<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 599</span><span class="ansi-red-fg">         </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_dataset_fetcher<span class="ansi-blue-fg">.</span>fetch<span class="ansi-blue-fg">(</span>index<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-intense-fg ansi-bold">    600</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_pin_memory<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    601</span>             data <span class="ansi-blue-fg">=</span> _utils<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>_pin_memory_device<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py</span> in <span class="ansi-cyan-fg">fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-intense-fg ansi-bold">     38</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     39</span>             data <span class="ansi-blue-fg">=</span> next<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dataset_iter<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 40</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>collate_fn<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     41</span> 
<span class="ansi-green-intense-fg ansi-bold">     42</span> 

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">    179</span>             <span class="ansi-green-fg">except</span> TypeError<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    180</span>                 <span class="ansi-red-fg"># The sequence type may not support `__init__(iterable)` (e.g., `range`).</span>
<span class="ansi-green-fg">--&gt; 181</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> <span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">    182</span> 
<span class="ansi-green-intense-fg ansi-bold">    183</span>     <span class="ansi-green-fg">raise</span> TypeError<span class="ansi-blue-fg">(</span>default_collate_err_msg_format<span class="ansi-blue-fg">.</span>format<span class="ansi-blue-fg">(</span>elem_type<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">&lt;listcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-intense-fg ansi-bold">    179</span>             <span class="ansi-green-fg">except</span> TypeError<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    180</span>                 <span class="ansi-red-fg"># The sequence type may not support `__init__(iterable)` (e.g., `range`).</span>
<span class="ansi-green-fg">--&gt; 181</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> <span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">    182</span> 
<span class="ansi-green-intense-fg ansi-bold">    183</span>     <span class="ansi-green-fg">raise</span> TypeError<span class="ansi-blue-fg">(</span>default_collate_err_msg_format<span class="ansi-blue-fg">.</span>format<span class="ansi-blue-fg">(</span>elem_type<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">    139</span>             storage <span class="ansi-blue-fg">=</span> elem<span class="ansi-blue-fg">.</span>storage<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>_new_shared<span class="ansi-blue-fg">(</span>numel<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    140</span>             out <span class="ansi-blue-fg">=</span> elem<span class="ansi-blue-fg">.</span>new<span class="ansi-blue-fg">(</span>storage<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>resize_<span class="ansi-blue-fg">(</span>len<span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>list<span class="ansi-blue-fg">(</span>elem<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 141</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> torch<span class="ansi-blue-fg">.</span>stack<span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> out<span class="ansi-blue-fg">=</span>out<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    142</span>     <span class="ansi-green-fg">elif</span> elem_type<span class="ansi-blue-fg">.</span>__module__ <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;numpy&#39;</span> <span class="ansi-green-fg">and</span> elem_type<span class="ansi-blue-fg">.</span>__name__ <span class="ansi-blue-fg">!=</span> <span class="ansi-blue-fg">&#39;str_&#39;</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>
<span class="ansi-green-intense-fg ansi-bold">    143</span>             <span class="ansi-green-fg">and</span> elem_type<span class="ansi-blue-fg">.</span>__name__ <span class="ansi-blue-fg">!=</span> <span class="ansi-blue-fg">&#39;string_&#39;</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">TypeError</span>: issubclass() arg 2 must be a class or tuple of classes</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>


