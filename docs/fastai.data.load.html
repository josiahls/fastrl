---

title: Loading


keywords: fastai
sidebar: home_sidebar

summary: "Objects using the `Loop` and `DataPipe` API for DataLoading"
description: "Objects using the `Loop` and `DataPipe` API for DataLoading"
nb_path: "nbs/02d_fastai.data.load.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02d_fastai.data.load.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will replicate <a href="https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastai mnist loading</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="n">untar_data</span><span class="p">,</span><span class="n">URLs</span><span class="p">,</span><span class="n">get_image_files</span><span class="p">,</span><span class="n">PILImage</span><span class="p">,</span><span class="n">ToTensor</span>

<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>

<span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
    <div>
      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.14% [3219456/3214948 00:01<00:00]
    </div>
    </div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/7&#39;),Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/3&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we create the dataset...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TypeTransformLoop" class="doc_header"><code>class</code> <code>TypeTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L21" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TypeTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>MapDataPipe</code></p>
</blockquote>
<p>Map-style DataPipe.</p>
<p>All datasets that represent a map from keys to data samples should subclass this.
Subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given, unique key. Subclasses can also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>MapDataPipe</code> (recommend, available to most but not all DataPipes).</p>
<p>Note:
    :class:<code>~torch.utils.data.DataLoader</code> by default constructs an index
    sampler that yields integral indices. To make it work with a map-style
    DataPipe with non-integral indices/keys, a custom sampler must be provided.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.map import SequenceWrapper, Mapper
&gt;&gt;&gt; dp = SequenceWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; map_dp_2 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; batch_dp = map_dp_1.batch(batch_size=2)
&gt;&gt;&gt; list(batch_dp)
[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ItemTransformLoop" class="doc_header"><code>class</code> <code>ItemTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L32" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ItemTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BatchTransformLoop" class="doc_header"><code>class</code> <code>BatchTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L41" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BatchTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_loader_loop" class="doc_header"><code>default_loader_loop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L50" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_loader_loop</code>(<strong><code>items</code></strong>, <strong><code>splitter</code></strong>, <strong><code>cbs</code></strong>=<em><code>None</code></em>, <strong><code>type_tfms</code></strong>=<em><code>None</code></em>, <strong><code>item_tfms</code></strong>=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>=<em><code>None</code></em>, <strong><code>bs</code></strong>=<em><code>2</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GrandparentSplitter" class="doc_header"><code>GrandparentSplitter</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L75" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GrandparentSplitter</code>(<strong><code>train</code></strong>=<em><code>'train'</code></em>, <strong><code>valid</code></strong>=<em><code>'valid'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">default_loader_loop</span><span class="p">(</span>
    <span class="n">L</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)),</span>
    <span class="n">GrandparentSplitter</span><span class="p">(),</span>
    <span class="n">type_tfms</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span><span class="n">ToTensor</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># default_constructor(</span>
<span class="c1">#     dp.iter.Zipper(*base),</span>
    
<span class="c1"># )</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>DataLoader2<span class="o">?</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea ">
<pre><span class="ansi-red-fg">Init signature:</span>
DataLoader2<span class="ansi-blue-fg">(</span>
    dataset<span class="ansi-blue-fg">,</span>
    batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
    shuffle<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    sampler<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    batch_sampler<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    num_workers<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    collate_fn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    pin_memory<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    drop_last<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    timeout<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    worker_init_fn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    prefetch_factor<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span>
    persistent_workers<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    batch_outside_worker<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    parallelism_mode<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;mp&#39;</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>      &lt;no docstring&gt;
<span class="ansi-red-fg">File:</span>           /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader_experimental.py
<span class="ansi-red-fg">Type:</span>           type
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[TensorImage([[[[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8), TensorImage([[[[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8)]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py:141: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)
  return torch.stack(batch, 0, out=out)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>Transform<span class="o">??</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea ">
<pre><span class="ansi-red-fg">Init signature:</span> Transform<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> enc<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> dec<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> split_idx<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> order<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Source:</span>        
<span class="ansi-green-fg">class</span> Transform<span class="ansi-blue-fg">(</span>metaclass<span class="ansi-blue-fg">=</span>_TfmMeta<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
    <span class="ansi-blue-fg">&#34;Delegates (`__call__`,`decode`,`setup`) to (&lt;code&gt;encodes&lt;/code&gt;,&lt;code&gt;decodes&lt;/code&gt;,&lt;code&gt;setups&lt;/code&gt;) if `split_idx` matches&#34;</span>
    split_idx<span class="ansi-blue-fg">,</span>init_enc<span class="ansi-blue-fg">,</span>order<span class="ansi-blue-fg">,</span>train_setup <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span><span class="ansi-green-fg">None</span>
    <span class="ansi-green-fg">def</span> __init__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> enc<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> dec<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> split_idx<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> order<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        self<span class="ansi-blue-fg">.</span>split_idx <span class="ansi-blue-fg">=</span> ifnone<span class="ansi-blue-fg">(</span>split_idx<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>split_idx<span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> order <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>order<span class="ansi-blue-fg">=</span>order
        self<span class="ansi-blue-fg">.</span>init_enc <span class="ansi-blue-fg">=</span> enc <span class="ansi-green-fg">or</span> dec
        <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> self<span class="ansi-blue-fg">.</span>init_enc<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span>

        self<span class="ansi-blue-fg">.</span>encodes<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>decodes<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>setups <span class="ansi-blue-fg">=</span> TypeDispatch<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>TypeDispatch<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>TypeDispatch<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> enc<span class="ansi-blue-fg">:</span>
            self<span class="ansi-blue-fg">.</span>encodes<span class="ansi-blue-fg">.</span>add<span class="ansi-blue-fg">(</span>enc<span class="ansi-blue-fg">)</span>
            self<span class="ansi-blue-fg">.</span>order <span class="ansi-blue-fg">=</span> getattr<span class="ansi-blue-fg">(</span>enc<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">&#39;order&#39;</span><span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>order<span class="ansi-blue-fg">)</span>
            <span class="ansi-green-fg">if</span> len<span class="ansi-blue-fg">(</span>type_hints<span class="ansi-blue-fg">(</span>enc<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">&gt;</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>input_types <span class="ansi-blue-fg">=</span> first<span class="ansi-blue-fg">(</span>type_hints<span class="ansi-blue-fg">(</span>enc<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>values<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
            self<span class="ansi-blue-fg">.</span>_name <span class="ansi-blue-fg">=</span> _get_name<span class="ansi-blue-fg">(</span>enc<span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> dec<span class="ansi-blue-fg">:</span> self<span class="ansi-blue-fg">.</span>decodes<span class="ansi-blue-fg">.</span>add<span class="ansi-blue-fg">(</span>dec<span class="ansi-blue-fg">)</span>

    <span class="ansi-blue-fg">@</span>property
    <span class="ansi-green-fg">def</span> name<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> getattr<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;_name&#39;</span><span class="ansi-blue-fg">,</span> _get_name<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> __call__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;encodes&#39;</span><span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> decode  <span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;decodes&#39;</span><span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> __repr__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> <span class="ansi-blue-fg">f&#39;{self.name}:\nencodes: {self.encodes}decodes: {self.decodes}&#39;</span>

    <span class="ansi-green-fg">def</span> setup<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> items<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> train_setup<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        train_setup <span class="ansi-blue-fg">=</span> train_setup <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>train_setup <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span> <span class="ansi-green-fg">else</span> self<span class="ansi-blue-fg">.</span>train_setup
        <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>setups<span class="ansi-blue-fg">(</span>getattr<span class="ansi-blue-fg">(</span>items<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;train&#39;</span><span class="ansi-blue-fg">,</span> items<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">if</span> train_setup <span class="ansi-green-fg">else</span> items<span class="ansi-blue-fg">)</span>

    <span class="ansi-green-fg">def</span> _call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> split_idx<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-green-fg">if</span> split_idx<span class="ansi-blue-fg">!=</span>self<span class="ansi-blue-fg">.</span>split_idx <span class="ansi-green-fg">and</span> self<span class="ansi-blue-fg">.</span>split_idx <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> x
        <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>_do_call<span class="ansi-blue-fg">(</span>getattr<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>

    <span class="ansi-green-fg">def</span> _do_call<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> f<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> _is_tuple<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
            <span class="ansi-green-fg">if</span> f <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> x
            ret <span class="ansi-blue-fg">=</span> f<span class="ansi-blue-fg">.</span>returns<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">if</span> hasattr<span class="ansi-blue-fg">(</span>f<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">&#39;returns&#39;</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">else</span> <span class="ansi-green-fg">None</span>
            <span class="ansi-green-fg">return</span> retain_type<span class="ansi-blue-fg">(</span>f<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> ret<span class="ansi-blue-fg">)</span>
        res <span class="ansi-blue-fg">=</span> tuple<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>_do_call<span class="ansi-blue-fg">(</span>f<span class="ansi-blue-fg">,</span> x_<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> x_ <span class="ansi-green-fg">in</span> x<span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">return</span> retain_type<span class="ansi-blue-fg">(</span>res<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">File:</span>           /opt/conda/lib/python3.7/site-packages/fastcore/transform.py
<span class="ansi-red-fg">Type:</span>           _TfmMeta
<span class="ansi-red-fg">Subclasses:</span>     InplaceTransform, DisplayedTransform, ItemTransform, ToTensor, AddMaskCodes, PointScaler, BBoxLabeler
</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>


