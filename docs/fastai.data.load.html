---

title: Loading


keywords: fastai
sidebar: home_sidebar

summary: "Objects using the `Loop` and `DataPipe` API for DataLoading"
description: "Objects using the `Loop` and `DataPipe` API for DataLoading"
nb_path: "nbs/02d_fastai.data.load.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02d_fastai.data.load.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will replicate <a href="https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb">fastai mnist loading</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="n">untar_data</span><span class="p">,</span><span class="n">URLs</span><span class="p">,</span><span class="n">get_image_files</span><span class="p">,</span><span class="n">PILImage</span><span class="p">,</span><span class="n">ToTensor</span><span class="p">,</span><span class="n">PILBase</span>
<span class="kn">from</span> <span class="nn">fastrl.fastai.torch_core</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>

<span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/7&#39;),Path(&#39;/home/fastrl_user/.fastai/data/mnist_sample/train/3&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we create the dataset...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TypeTransformLoop" class="doc_header"><code>class</code> <code>TypeTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L21" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TypeTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>MapDataPipe</code></p>
</blockquote>
<p>Map-style DataPipe.</p>
<p>All datasets that represent a map from keys to data samples should subclass this.
Subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given, unique key. Subclasses can also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>MapDataPipe</code> (recommend, available to most but not all DataPipes).</p>
<p>Note:
    :class:<code>~torch.utils.data.DataLoader</code> by default constructs an index
    sampler that yields integral indices. To make it work with a map-style
    DataPipe with non-integral indices/keys, a custom sampler must be provided.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.map import SequenceWrapper, Mapper
&gt;&gt;&gt; dp = SequenceWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; map_dp_2 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; batch_dp = map_dp_1.batch(batch_size=2)
&gt;&gt;&gt; list(batch_dp)
[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ItemTransformLoop" class="doc_header"><code>class</code> <code>ItemTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L32" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ItemTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BatchTransformLoop" class="doc_header"><code>class</code> <code>BatchTransformLoop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L41" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BatchTransformLoop</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe.</p>
<p><code>IterDataPipe</code> is lazily initialized and its elements are computed only when <code>next()</code> is called
on its iterator.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
&gt;&gt;&gt; dp = IterableWrapper(range(10))
&gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
&gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
&gt;&gt;&gt; list(map_dp_1)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; list(map_dp_2)
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
&gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
&gt;&gt;&gt; list(filter_dp)
[2, 4, 6, 8, 10]</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_loader_loop" class="doc_header"><code>default_loader_loop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L50" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_loader_loop</code>(<strong><code>items</code></strong>:<code>Iterable</code>[<code>+T_co</code>], <strong><code>splitter</code></strong>:<code>typing.Callable</code>, <strong><code>cbs</code></strong>:<code>Optional</code>[<code>List</code>[<a href="/fastrl/fastai.data.loop.core.html#Callback"><code>Callback</code></a>]]=<em><code>None</code></em>, <strong><code>type_tfms</code></strong>:<code>Optional</code>[<code>Transform</code>]=<em><code>None</code></em>, <strong><code>item_tfms</code></strong>:<code>Optional</code>[<code>Transform</code>]=<em><code>None</code></em>, <strong><code>batch_tfms</code></strong>:<code>Optional</code>[<code>Transform</code>]=<em><code>None</code></em>, <strong><code>bs</code></strong>:<code>int</code>=<em><code>2</code></em>, <strong><code>shuffler</code></strong>:<code>Union</code>[<code>IterDataPipe</code>, <code>MapDataPipe</code>, <code>NoneType</code>]=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GrandparentSplitter" class="doc_header"><code>GrandparentSplitter</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/load.py#L79" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GrandparentSplitter</code>(<strong><code>train</code></strong>=<em><code>'train'</code></em>, <strong><code>valid</code></strong>=<em><code>'valid'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">default_loader_loop</span><span class="p">(</span>
    <span class="n">L</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)),</span>
    <span class="n">GrandparentSplitter</span><span class="p">(),</span>
    <span class="n">type_tfms</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">PILImage</span><span class="o">.</span><span class="n">create</span><span class="p">,</span><span class="n">ToTensor</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">default_constructor</span><span class="p">(</span>
    <span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">Zipper</span><span class="p">(</span><span class="o">*</span><span class="n">base</span><span class="p">),</span>   
    <span class="n">cbs</span><span class="o">=</span><span class="n">L</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">DataLoader2</span><span class="p">(</span><span class="n">base</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">element</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dl</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[TensorImage([[[[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8), TensorImage([[[[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]],

              [[0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               ...,
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0],
               [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8)]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py:141: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:295.)
  return torch.stack(batch, 0, out=out)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>


