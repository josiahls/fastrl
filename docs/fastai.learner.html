---

title: Learner


keywords: fastai
sidebar: home_sidebar

summary: "A revised fastai learner that uses DataPipe shimming"
description: "A revised fastai learner that uses DataPipe shimming"
nb_path: "nbs/02f_fastai.learner.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02f_fastai.learner.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="XYSplit" class="doc_header"><code>class</code> <code>XYSplit</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L25" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>XYSplit</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModelPredict" class="doc_header"><code>class</code> <code>ModelPredict</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L40" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModelPredict</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LossCalc" class="doc_header"><code>class</code> <code>LossCalc</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L53" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LossCalc</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="default_train_loop" class="doc_header"><code>default_train_loop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L69" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>default_train_loop</code>(<strong><code>dls</code></strong>:<code>List</code>[<code>DataLoader2</code>], <strong><code>cbs</code></strong>:<code>Optional</code>[<code>List</code>[<a href="/fastrl/fastai.data.pipes.core.html#Callback"><code>Callback</code></a>]]=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="find_pipes" class="doc_header"><code>find_pipes</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/pipes/core.py#L55" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>find_pipes</code>(<strong><code>pipe</code></strong>:<code>Union</code>[<code>MapDataPipe</code>, <code>IterDataPipe</code>], <strong><code>fn</code></strong>, <strong><code>pipe_list</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Epocher" class="doc_header"><code>class</code> <code>Epocher</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L93" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Epocher</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="only_train_loop" class="doc_header"><code>only_train_loop</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L115" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>only_train_loop</code>(<strong><code>dls</code></strong>:<code>List</code>[<code>DataLoader2</code>], <strong><code>epochs</code></strong>:<code>int</code>, <strong><code>learner</code></strong>, <strong><code>cbs</code></strong>:<code>Optional</code>[<code>List</code>[<a href="/fastrl/fastai.data.pipes.core.html#Callback"><code>Callback</code></a>]]=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Learner" class="doc_header"><code>class</code> <code>Learner</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L132" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Learner</code>(<strong><code>model</code></strong>, <strong><code>dls</code></strong>, <strong><code>opt</code></strong>, <strong><code>loss_func</code></strong>, <strong><code>cbs</code></strong>, <strong><code>train_loop</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DQN" class="doc_header"><code>class</code> <code>DQN</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/data/block.py#L265" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DQN</code>(<strong><code>state_sz</code></strong>:<code>int</code>, <strong><code>action_sz</code></strong>:<code>int</code>, <strong><code>hidden</code></strong>=<em><code>512</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="QCalc" class="doc_header"><code>class</code> <code>QCalc</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L172" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>QCalc</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModelLearnCalc" class="doc_header"><code>class</code> <code>ModelLearnCalc</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/fastai/learner.py#L195" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModelLearnCalc</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwds</code></strong>) :: <code>IterDataPipe</code></p>
</blockquote>
<p>Iterable-style DataPipe.</p>
<p>All DataPipes that represent an iterable of data samples should subclass this.
This style of DataPipes is particularly useful when data come from a stream, or
when the number of samples is too large to fit them all in memory. <code>IterDataPipe</code> is lazily initialized and its
elements are computed only when <code>next()</code> is called on the iterator of an <code>IterDataPipe</code>.</p>
<p>All subclasses should overwrite :meth:<code>__iter__</code>, which would return an
iterator of samples in this DataPipe. Calling <code>__iter__</code> of an <code>IterDataPipe</code> automatically invokes its
method <code>reset()</code>, which by default performs no operation. When writing a custom <code>IterDataPipe</code>, users should
override <code>reset()</code> if necessary. The common usages include resetting buffers, pointers,
and various state variables within the custom <code>IterDataPipe</code>.</p>
<p>Note:
    Only <code>one</code> iterator can be valid for each <code>IterDataPipe</code> at a time,
    and the creation a second iterator will invalidate the first one. This constraint is necessary because
    some <code>IterDataPipe</code> have internal buffers, whose states can become invalid if there are multiple iterators.
    The code example below presents details on how this constraint looks in practice.
    If you have any feedback related to this constraint, please see <code>GitHub IterDataPipe Single Iterator Issue</code>_.</p>
<p>These DataPipes can be invoked in two ways, using the class constructor or applying their
functional form onto an existing <code>IterDataPipe</code> (recommended, available to most but not all DataPipes).
You can chain multiple <code>IterDataPipe</code> together to form a pipeline that will perform multiple
operations in succession.</p>
<p>.. _GitHub IterDataPipe Single Iterator Issue:
    <a href="https://github.com/pytorch/data/issues/45">https://github.com/pytorch/data/issues/45</a></p>
<p>Note:
    When a subclass is used with :class:<code>~torch.utils.data.DataLoader</code>, each
    item in the DataPipe will be yielded from the :class:<code>~torch.utils.data.DataLoader</code>
    iterator. When :attr:<code>num_workers &gt; 0</code>, each worker process will have a
    different copy of the DataPipe object, so it is often desired to configure
    each copy independently to avoid having duplicate data returned from the
    workers. :func:<code>~torch.utils.data.get_worker_info</code>, when called in a worker
    process, returns information about the worker. It can be used in either the
    dataset's :meth:<code>__iter__</code> method or the :class:<code>~torch.utils.data.DataLoader</code> 's
    :attr:<code>worker_init_fn</code> option to modify each copy's behavior.</p>
<p>Examples:
    General Usage:</p>

<pre><code>    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; map_dp_1 = Mapper(dp, lambda x: x + 1)  # Using class constructor
    &gt;&gt;&gt; map_dp_2 = dp.map(lambda x: x + 1)  # Using functional form (recommended)
    &gt;&gt;&gt; list(map_dp_1)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; list(map_dp_2)
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    &gt;&gt;&gt; filter_dp = map_dp_1.filter(lambda x: x % 2 == 0)
    &gt;&gt;&gt; list(filter_dp)
    [2, 4, 6, 8, 10]
Single Iterator Constraint Example:
    &gt;&gt;&gt; from torchdata.datapipes.iter import IterableWrapper, Mapper
    &gt;&gt;&gt; dp = IterableWrapper(range(10))
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; list(it1)
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    &gt;&gt;&gt; it1 = iter(source_dp)
    &gt;&gt;&gt; it2 = iter(source_dp)  # The creation of a new iterator invalidates `it1`
    &gt;&gt;&gt; next(it2)
    0
    &gt;&gt;&gt; next(it1)  # Further usage of `it1` will raise a `RunTimeError`</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Dict2TensorBatch</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span> <span class="o">=</span> <span class="n">source_datapipe</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span> <span class="o">=</span> <span class="n">learn</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="nb">dict</span><span class="p">):</span> 
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Batch should be a dict, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ExperienceReplay</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="n">debug</span><span class="o">=</span><span class="kc">False</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learn</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">maxsz</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span> <span class="o">=</span> <span class="n">source_datapipe</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span> <span class="o">=</span> <span class="n">learn</span>
        <span class="k">if</span> <span class="n">learn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">experience_replay</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">bs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxsz</span> <span class="o">=</span> <span class="n">maxsz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span><span class="p">),</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">ifnone</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">),),</span><span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">):</span>
            <span class="c1"># print(f&#39;batch {i}, {b}&#39;)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Experience Replay Adding: &#39;</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span> <span class="k">continue</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">!=</span><span class="mi">2</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="nb">dict</span><span class="p">):</span>   <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span> 
                <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">b</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
                
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">:</span> <span class="k">continue</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d</span><span class="p">:</span><span class="n">Dict</span><span class="p">):</span> 
        <span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">to_np</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">d</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span><span class="o">&gt;</span><span class="bp">self</span><span class="o">.</span><span class="n">maxsz</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">v</span><span class="p">,</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sz_tracker</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReinforcementLearningSimpleCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">add_core_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">after</span><span class="o">=</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterableWrapper</span><span class="p">,</span><span class="n">not_under</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">L</span><span class="p">(</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">ExperienceReplay</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span><span class="n">maxsz</span><span class="o">=</span><span class="mi">100000</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">Dict2TensorBatch</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">XYSplit</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">,</span><span class="n">x_fld</span><span class="o">=</span><span class="s1">&#39;state&#39;</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">ModelPredict</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">QCalc</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">,</span><span class="n">discount</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">nsteps</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">LossCalc</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">ModelLearnCalc</span><span class="p">,</span><span class="n">learn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">),</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">AvgLossMetric</span><span class="p">,</span><span class="n">learner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">)</span>
        <span class="p">)</span>

        
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">AvgLossMetric</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="n">is_metric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">,</span><span class="mi">0</span>
    
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">:</span>
            <span class="n">bs</span> <span class="o">=</span> <span class="n">find_bs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">yb</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">to_detach</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span><span class="o">*</span><span class="n">bs</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">bs</span>
            <span class="k">yield</span> <span class="n">v</span>
        
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span><span class="mi">0</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="s2">&quot;loss&quot;</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">class</span> <span class="nc">RollingRewardMetric</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="n">is_metric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">,</span><span class="n">nrewards</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">nrewards</span> <span class="o">=</span> <span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">,[],</span><span class="n">nrewards</span>
    
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">:</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="s1">&#39;done&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)][</span><span class="s1">&#39;sum_reward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span><span class="o">&gt;</span><span class="bp">self</span><span class="o">.</span><span class="n">nrewards</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">nrewards</span><span class="p">:]</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;after adjust&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
            
            <span class="k">yield</span> <span class="n">v</span>
        
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">pass</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="s2">&quot;rolling_reward&quot;</span>

<span class="k">class</span> <span class="nc">EpsilonMetric</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="n">is_metric</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">,</span><span class="n">epsilon_cb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_cb</span> <span class="o">=</span> <span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">,</span><span class="n">epsilon_cb</span>
    
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_cb</span><span class="o">.</span><span class="n">epsilon</span>
            <span class="k">yield</span> <span class="n">v</span>
        
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">pass</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="s2">&quot;epsilon&quot;</span>


<span class="k">class</span> <span class="nc">RewardCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">epsilon_cb</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_cb</span> <span class="o">=</span> <span class="n">epsilon_cb</span>

    <span class="k">def</span> <span class="nf">add_core_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">after</span><span class="o">=</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterableWrapper</span><span class="p">,</span><span class="n">not_under</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">L</span><span class="p">(</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">RollingRewardMetric</span><span class="p">,</span><span class="n">learner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterableWrapper</span><span class="p">([</span><span class="nb">dict</span><span class="p">(</span><span class="n">test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">ExperienceReplay</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">BatchRecorder</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">source_datapipe</span><span class="p">,</span><span class="n">learner</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span> <span class="o">=</span> <span class="n">source_datapipe</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learner</span> <span class="o">=</span> <span class="n">learner</span>
        
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">pbar</span> <span class="o">=</span> <span class="n">progress_bar</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">n</span><span class="p">)),</span><span class="n">parent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">mbar</span><span class="p">,</span><span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">o</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">pbar</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">source_datapipe</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">o</span>
        
<span class="k">class</span> <span class="nc">RecorderCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">add_core_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">before</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">after</span><span class="o">=</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterableWrapper</span><span class="p">,</span><span class="n">not_under</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">dp</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">IterDataPipe</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">L</span><span class="p">(</span>
            <span class="n">partial</span><span class="p">(</span><span class="n">BatchRecorder</span><span class="p">,</span><span class="n">learner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Setup the agent</span>
<span class="n">agent_base</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">model</span><span class="p">,[])</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">RawOutOfStep</span><span class="p">(</span><span class="n">agent_base</span><span class="p">,</span><span class="n">agent_base</span><span class="p">,</span><span class="s1">&#39;state&#39;</span><span class="p">)</span>
<span class="n">agent_ep</span> <span class="o">=</span> <span class="n">DiscreteEpsilonRandomSelect</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span><span class="n">agent_base</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">min_epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">ArgmaxOfStep</span><span class="p">(</span><span class="n">agent_ep</span><span class="p">,</span><span class="n">agent_base</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">ToDiscrete</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span><span class="n">agent_base</span><span class="p">)</span>
<span class="c1"># Setup the data block </span>
<span class="n">GymTransformBlock</span> <span class="o">=</span> <span class="n">TransformBlock</span><span class="p">(</span>
    <span class="n">type_tfms</span>  <span class="o">=</span> <span class="n">GymTypeTransform</span><span class="p">,</span>
    <span class="n">item_tfms</span>  <span class="o">=</span> <span class="p">(</span><span class="n">GymStepTransform</span><span class="p">(</span><span class="n">agent</span><span class="p">),</span><span class="n">DictToTensor</span><span class="p">),</span>
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="n">DictCollate</span><span class="p">,</span>
    <span class="n">cbs</span> <span class="o">=</span> <span class="n">NStepCallback</span><span class="p">(</span><span class="n">nsteps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1"># Init with supported loader loop</span>
<span class="n">block</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="n">GymTransformBlock</span><span class="p">,</span>
    <span class="n">loader_loop</span><span class="o">=</span><span class="n">simple_iter_loader_loop</span>
<span class="p">)</span>
<span class="c1"># Init the loader(s)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">([</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">],</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">n_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dls</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span> 
                <span class="n">loss_func</span><span class="o">=</span><span class="n">MSELoss</span><span class="p">(),</span><span class="n">train_loop</span><span class="o">=</span><span class="n">only_train_loop</span><span class="p">,</span>
               <span class="n">cbs</span><span class="o">=</span><span class="n">L</span><span class="p">(</span><span class="n">RewardCallback</span><span class="p">(</span><span class="n">agent_ep</span><span class="p">),</span>
                     <span class="n">RecorderCallback</span><span class="p">,</span><span class="n">ReinforcementLearningSimpleCallback</span><span class="p">))</span>
<span class="n">learn</span><span class="o">.</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ExperienceReplay</span><span class="o">.</span><span class="n">debug</span><span class="o">=</span><span class="kc">False</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     dls:List[DataLoader2],</span>
<span class="c1">#     epochs:int,</span>
<span class="c1">#     learner,</span>
<span class="c1">#     cbs:Optional[List[Callback]]=None,</span>
<span class="c1"># ):</span>
    
<span class="c1">#     train = dp.iter.IterableWrapper(dls,deepcopy=False).add_cbs(cbs)</span>
<span class="c1">#     train = Epocher(train,epochs,learner=learner).add_cbs(cbs)</span>

<span class="c1">#     # train = </span>
    
<span class="c1">#     return train</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchdata.datapipes.utils</span> <span class="kn">import</span> <span class="n">to_graph</span>
<span class="n">to_graph</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">train_pipe</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.40.1 (20161225.0304)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="139pt" height="632pt"
 viewBox="0.00 0.00 139.00 632.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 628)">
<title>%3</title>
<polygon fill="#ffffff" stroke="transparent" points="-4,4 -4,-628 135,-628 135,4 -4,4"/>
<!-- QCalc&#45;8747992103361 -->
<g id="node1" class="node">
<title>QCalc&#45;8747992103361</title>
<polygon fill="#d3d3d3" stroke="#000000" points="92.5,-239 38.5,-239 38.5,-220 92.5,-220 92.5,-239"/>
<text text-anchor="middle" x="65.5" y="-227" font-family="monospace" font-size="10.00" fill="#000000">QCalc</text>
</g>
<!-- LossCalc&#45;8747992103365 -->
<g id="node2" class="node">
<title>LossCalc&#45;8747992103365</title>
<polygon fill="#d3d3d3" stroke="#000000" points="98,-184 33,-184 33,-165 98,-165 98,-184"/>
<text text-anchor="middle" x="65.5" y="-172" font-family="monospace" font-size="10.00" fill="#000000">LossCalc</text>
</g>
<!-- QCalc&#45;8747992103361&#45;&gt;LossCalc&#45;8747992103365 -->
<g id="edge1" class="edge">
<title>QCalc&#45;8747992103361&#45;&gt;LossCalc&#45;8747992103365</title>
<path fill="none" stroke="#000000" d="M65.5,-219.9197C65.5,-212.9083 65.5,-203.1442 65.5,-194.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-194.3408 65.5,-184.3408 62.0001,-194.3409 69.0001,-194.3408"/>
</g>
<!-- ModelLearnCalc&#45;8747992103369 -->
<g id="node4" class="node">
<title>ModelLearnCalc&#45;8747992103369</title>
<polygon fill="#d3d3d3" stroke="#000000" points="116,-129 15,-129 15,-110 116,-110 116,-129"/>
<text text-anchor="middle" x="65.5" y="-117" font-family="monospace" font-size="10.00" fill="#000000">ModelLearnCalc</text>
</g>
<!-- LossCalc&#45;8747992103365&#45;&gt;ModelLearnCalc&#45;8747992103369 -->
<g id="edge2" class="edge">
<title>LossCalc&#45;8747992103365&#45;&gt;ModelLearnCalc&#45;8747992103369</title>
<path fill="none" stroke="#000000" d="M65.5,-164.9197C65.5,-157.9083 65.5,-148.1442 65.5,-139.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-139.3408 65.5,-129.3408 62.0001,-139.3409 69.0001,-139.3408"/>
</g>
<!-- Epocher&#45;8748127251365 -->
<g id="node3" class="node">
<title>Epocher&#45;8748127251365</title>
<polygon fill="#caff70" stroke="#000000" points="95,-19 36,-19 36,0 95,0 95,-19"/>
<text text-anchor="middle" x="65.5" y="-7" font-family="monospace" font-size="10.00" fill="#000000">Epocher</text>
</g>
<!-- AvgLossMetric&#45;8747992103373 -->
<g id="node7" class="node">
<title>AvgLossMetric&#45;8747992103373</title>
<polygon fill="#d3d3d3" stroke="#000000" points="113,-74 18,-74 18,-55 113,-55 113,-74"/>
<text text-anchor="middle" x="65.5" y="-62" font-family="monospace" font-size="10.00" fill="#000000">AvgLossMetric</text>
</g>
<!-- ModelLearnCalc&#45;8747992103369&#45;&gt;AvgLossMetric&#45;8747992103373 -->
<g id="edge4" class="edge">
<title>ModelLearnCalc&#45;8747992103369&#45;&gt;AvgLossMetric&#45;8747992103373</title>
<path fill="none" stroke="#000000" d="M65.5,-109.9197C65.5,-102.9083 65.5,-93.1442 65.5,-84.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-84.3408 65.5,-74.3408 62.0001,-84.3409 69.0001,-84.3408"/>
</g>
<!-- IterableWrapper&#45;8748070737317 -->
<g id="node5" class="node">
<title>IterableWrapper&#45;8748070737317</title>
<polygon fill="#add8e6" stroke="#000000" points="119,-624 12,-624 12,-605 119,-605 119,-624"/>
<text text-anchor="middle" x="65.5" y="-612" font-family="monospace" font-size="10.00" fill="#000000">IterableWrapper</text>
</g>
<!-- RollingRewardMetric&#45;8747992103341 -->
<g id="node6" class="node">
<title>RollingRewardMetric&#45;8747992103341</title>
<polygon fill="#d3d3d3" stroke="#000000" points="131,-569 0,-569 0,-550 131,-550 131,-569"/>
<text text-anchor="middle" x="65.5" y="-557" font-family="monospace" font-size="10.00" fill="#000000">RollingRewardMetric</text>
</g>
<!-- IterableWrapper&#45;8748070737317&#45;&gt;RollingRewardMetric&#45;8747992103341 -->
<g id="edge3" class="edge">
<title>IterableWrapper&#45;8748070737317&#45;&gt;RollingRewardMetric&#45;8747992103341</title>
<path fill="none" stroke="#000000" d="M65.5,-604.9197C65.5,-597.9083 65.5,-588.1442 65.5,-579.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-579.3408 65.5,-569.3408 62.0001,-579.3409 69.0001,-579.3408"/>
</g>
<!-- BatchRecorder&#45;8747992103337 -->
<g id="node10" class="node">
<title>BatchRecorder&#45;8747992103337</title>
<polygon fill="#d3d3d3" stroke="#000000" points="113,-514 18,-514 18,-495 113,-495 113,-514"/>
<text text-anchor="middle" x="65.5" y="-502" font-family="monospace" font-size="10.00" fill="#000000">BatchRecorder</text>
</g>
<!-- RollingRewardMetric&#45;8747992103341&#45;&gt;BatchRecorder&#45;8747992103337 -->
<g id="edge8" class="edge">
<title>RollingRewardMetric&#45;8747992103341&#45;&gt;BatchRecorder&#45;8747992103337</title>
<path fill="none" stroke="#000000" d="M65.5,-549.9197C65.5,-542.9083 65.5,-533.1442 65.5,-524.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-524.3408 65.5,-514.3408 62.0001,-524.3409 69.0001,-524.3408"/>
</g>
<!-- AvgLossMetric&#45;8747992103373&#45;&gt;Epocher&#45;8748127251365 -->
<g id="edge7" class="edge">
<title>AvgLossMetric&#45;8747992103373&#45;&gt;Epocher&#45;8748127251365</title>
<path fill="none" stroke="#000000" d="M65.5,-54.9197C65.5,-47.9083 65.5,-38.1442 65.5,-29.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-29.3408 65.5,-19.3408 62.0001,-29.3409 69.0001,-29.3408"/>
</g>
<!-- ExperienceReplay&#45;8747992103321 -->
<g id="node8" class="node">
<title>ExperienceReplay&#45;8747992103321</title>
<polygon fill="#d3d3d3" stroke="#000000" points="122,-459 9,-459 9,-440 122,-440 122,-459"/>
<text text-anchor="middle" x="65.5" y="-447" font-family="monospace" font-size="10.00" fill="#000000">ExperienceReplay</text>
</g>
<!-- Dict2TensorBatch&#45;8747992103349 -->
<g id="node9" class="node">
<title>Dict2TensorBatch&#45;8747992103349</title>
<polygon fill="#d3d3d3" stroke="#000000" points="122,-404 9,-404 9,-385 122,-385 122,-404"/>
<text text-anchor="middle" x="65.5" y="-392" font-family="monospace" font-size="10.00" fill="#000000">Dict2TensorBatch</text>
</g>
<!-- ExperienceReplay&#45;8747992103321&#45;&gt;Dict2TensorBatch&#45;8747992103349 -->
<g id="edge5" class="edge">
<title>ExperienceReplay&#45;8747992103321&#45;&gt;Dict2TensorBatch&#45;8747992103349</title>
<path fill="none" stroke="#000000" d="M65.5,-439.9197C65.5,-432.9083 65.5,-423.1442 65.5,-414.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-414.3408 65.5,-404.3408 62.0001,-414.3409 69.0001,-414.3408"/>
</g>
<!-- XYSplit&#45;8747992103353 -->
<g id="node12" class="node">
<title>XYSplit&#45;8747992103353</title>
<polygon fill="#d3d3d3" stroke="#000000" points="95,-349 36,-349 36,-330 95,-330 95,-349"/>
<text text-anchor="middle" x="65.5" y="-337" font-family="monospace" font-size="10.00" fill="#000000">XYSplit</text>
</g>
<!-- Dict2TensorBatch&#45;8747992103349&#45;&gt;XYSplit&#45;8747992103353 -->
<g id="edge10" class="edge">
<title>Dict2TensorBatch&#45;8747992103349&#45;&gt;XYSplit&#45;8747992103353</title>
<path fill="none" stroke="#000000" d="M65.5,-384.9197C65.5,-377.9083 65.5,-368.1442 65.5,-359.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-359.3408 65.5,-349.3408 62.0001,-359.3409 69.0001,-359.3408"/>
</g>
<!-- BatchRecorder&#45;8747992103337&#45;&gt;ExperienceReplay&#45;8747992103321 -->
<g id="edge6" class="edge">
<title>BatchRecorder&#45;8747992103337&#45;&gt;ExperienceReplay&#45;8747992103321</title>
<path fill="none" stroke="#000000" d="M65.5,-494.9197C65.5,-487.9083 65.5,-478.1442 65.5,-469.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-469.3408 65.5,-459.3408 62.0001,-469.3409 69.0001,-469.3408"/>
</g>
<!-- ModelPredict&#45;8747992103345 -->
<g id="node11" class="node">
<title>ModelPredict&#45;8747992103345</title>
<polygon fill="#d3d3d3" stroke="#000000" points="110,-294 21,-294 21,-275 110,-275 110,-294"/>
<text text-anchor="middle" x="65.5" y="-282" font-family="monospace" font-size="10.00" fill="#000000">ModelPredict</text>
</g>
<!-- ModelPredict&#45;8747992103345&#45;&gt;QCalc&#45;8747992103361 -->
<g id="edge9" class="edge">
<title>ModelPredict&#45;8747992103345&#45;&gt;QCalc&#45;8747992103361</title>
<path fill="none" stroke="#000000" d="M65.5,-274.9197C65.5,-267.9083 65.5,-258.1442 65.5,-249.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-249.3408 65.5,-239.3408 62.0001,-249.3409 69.0001,-249.3408"/>
</g>
<!-- XYSplit&#45;8747992103353&#45;&gt;ModelPredict&#45;8747992103345 -->
<g id="edge11" class="edge">
<title>XYSplit&#45;8747992103353&#45;&gt;ModelPredict&#45;8747992103345</title>
<path fill="none" stroke="#000000" d="M65.5,-329.9197C65.5,-322.9083 65.5,-313.1442 65.5,-304.4652"/>
<polygon fill="#000000" stroke="#000000" points="69.0001,-304.3408 65.5,-294.3408 62.0001,-304.3409 69.0001,-304.3408"/>
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
taking step
found:  [AvgLossMetric, RollingRewardMetric]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:98: UserWarning: <span class="ansi-yellow-fg">WARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html</span>
  &#34;We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) &#34;
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea "><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>loss</th>
      <th>rolling_reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>None</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.482128</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.409861</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.373620</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.356339</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.329100</td>
      <td>nan</td>
    </tr>
    <tr>
      <td>0.317722</td>
      <td>nan</td>
    </tr>
  </tbody>
</table></div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis)
/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>


