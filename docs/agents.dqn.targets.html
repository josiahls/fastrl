---

title: DQN Targets + N-Step


keywords: fastai
sidebar: home_sidebar

summary: "A Bare-Bones DQN is usually extremely unstable. Target models can eleviate this. We also support First-Last N steps better."
description: "A Bare-Bones DQN is usually extremely unstable. Target models can eleviate this. We also support First-Last N steps better."
nb_path: "nbs/10b_agents.dqn.targets.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/10b_agents.dqn.targets.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">GAMMA</span><span class="o">=</span><span class="mf">0.99</span>
<span class="k">def</span> <span class="nf">calc_target</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">local_reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">next_state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">d</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">local_reward</span>
    <span class="n">state_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">next_state</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
    <span class="n">next_q_v</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">state_v</span><span class="p">)</span>
    <span class="n">best_q</span> <span class="o">=</span> <span class="n">next_q_v</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">local_reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">best_q</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # learn.opt.zero_grad()</span>

<span class="c1"># learn.state_action_values = learn.model.model(learn.xb[&#39;state&#39;]).gather(1, learn.xb[&#39;action&#39;]).squeeze(-1)</span>
<span class="c1"># learn.next_state_values = learn.target_model(learn.xb[&#39;next_state&#39;]).max(1)[0]</span>
<span class="c1"># learn.next_state_values[learn.xb[&#39;done&#39;].squeeze(-1)] = 0.0</span>

<span class="c1"># learn.expected_state_action_values = learn.next_state_values.detach() * (0.99**3) + learn.xb[&#39;reward&#39;].squeeze(-1)</span>
<span class="c1"># learn.loss= nn.MSELoss()(learn.state_action_values,learn.next_state_values)</span>

<span class="c1"># # learn.loss.backward()</span>
<span class="c1"># # learn.opt.step()</span>

<span class="c1"># print(learn.loss)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DQNTargetTrainer" class="doc_header"><code>class</code> <code>DQNTargetTrainer</code><a href="https://github.com/josiahls/fastrl/tree/master/fastrl/agents/dqn/targets.py#L27" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DQNTargetTrainer</code>(<strong><code>n_batch</code></strong>=<em><code>0</code></em>, <strong><code>target_sync</code></strong>=<em><code>300</code></em>, <strong><code>discount</code></strong>=<em><code>0.99</code></em>, <strong><code>n_steps</code></strong>=<em><code>1</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Basic class handling tweaks of the training loop by changing a <code>Learner</code> in various events</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><p><strong><code>n_batch</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>target_sync</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>discount</code></strong> : <em><code>&lt;class 'float'&gt;</code></em>, <em>optional</em></p>
</li>
<li><p><strong><code>n_steps</code></strong> : <em><code>&lt;class 'int'&gt;</code></em>, <em>optional</em></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dqn</span><span class="o">=</span><span class="n">DQN</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">agent</span><span class="o">=</span><span class="n">Agent</span><span class="p">(</span><span class="n">dqn</span><span class="p">,</span><span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">ArgMaxFeed</span><span class="p">,</span><span class="n">DiscreteEpsilonRandomSelect</span><span class="p">(</span><span class="n">min_epsilon</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)])</span>
<span class="n">source</span><span class="o">=</span><span class="n">Source</span><span class="p">(</span><span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">GymLoop</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">,</span><span class="n">agent</span><span class="p">,</span><span class="n">steps_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="c1">#mode=&#39;rgb_array&#39;,</span>
                           <span class="n">steps_delta</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">FirstLast</span><span class="c1">#,ResReduce(reduce_by=4)</span>
                  <span class="p">])</span>
<span class="n">dls</span><span class="o">=</span><span class="n">SourceDataBlock</span><span class="p">()</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">([</span><span class="n">source</span><span class="p">],</span><span class="n">n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">bs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">er_tb</span><span class="o">=</span><span class="n">ExperienceReplayTensorboard</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s1">&#39;_dqn_target&#39;</span><span class="p">,</span><span class="n">every_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># opt=optim.Adam(dqn.parameters(), lr=0.0001)</span>
<span class="n">learn</span><span class="o">=</span><span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span><span class="n">agent</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">MSELoss</span><span class="p">(),</span>
              <span class="n">opt_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">OptimWrapper</span><span class="p">,</span><span class="n">dqn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">),</span>
              <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">ExperienceReplayCallback</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">max_sz</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">warmup_sz</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">freeze_at_max</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                   <span class="n">DQNTargetTrainer</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">target_sync</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
                   <span class="p">,</span><span class="n">er_tb</span>
                  <span class="p">],</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">Reward</span><span class="p">,</span><span class="n">Epsilon</span><span class="p">,</span><span class="n">NEpisodes</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Could not do one pass in your dataloader, there is something wrong in it
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A few things, the loss should be &lt;1. If it is not, there is something majorly wrong iwth the training.</p>
<p>It is alright for the actual values twe are comparing to be way more than &gt;1 though, but it is expected that the loss should not be all that high at all.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">slow</span><span class="o">=</span><span class="kc">True</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">slow</span> <span class="k">else</span> <span class="mi">47</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span><span class="n">wd</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>train_reward</th>
      <th>train_epsilon</th>
      <th>train_n_episodes</th>
      <th>valid_loss</th>
      <th>valid_reward</th>
      <th>valid_epsilon</th>
      <th>valid_n_episodes</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.002783</td>
      <td>17.960784</td>
      <td>0.600000</td>
      <td>51</td>
      <td>00:35</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.001929</td>
      <td>19.730000</td>
      <td>0.200000</td>
      <td>102</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.001884</td>
      <td>20.880000</td>
      <td>0.020000</td>
      <td>145</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.001725</td>
      <td>20.520000</td>
      <td>0.020000</td>
      <td>191</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.001655</td>
      <td>21.570000</td>
      <td>0.020000</td>
      <td>268</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.040002</td>
      <td>20.930000</td>
      <td>0.020000</td>
      <td>331</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.036043</td>
      <td>19.920000</td>
      <td>0.020000</td>
      <td>378</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.036505</td>
      <td>19.940000</td>
      <td>0.020000</td>
      <td>429</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.037018</td>
      <td>19.190000</td>
      <td>0.020000</td>
      <td>481</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.034429</td>
      <td>18.440000</td>
      <td>0.020000</td>
      <td>529</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.083630</td>
      <td>19.020000</td>
      <td>0.020000</td>
      <td>591</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.087661</td>
      <td>19.160000</td>
      <td>0.020000</td>
      <td>633</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.073809</td>
      <td>18.220000</td>
      <td>0.020000</td>
      <td>689</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.066275</td>
      <td>18.270000</td>
      <td>0.020000</td>
      <td>736</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.072148</td>
      <td>19.090000</td>
      <td>0.020000</td>
      <td>791</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.131040</td>
      <td>18.700000</td>
      <td>0.020000</td>
      <td>848</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.131858</td>
      <td>18.880000</td>
      <td>0.020000</td>
      <td>895</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.111032</td>
      <td>18.970000</td>
      <td>0.020000</td>
      <td>939</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.135014</td>
      <td>19.550000</td>
      <td>0.020000</td>
      <td>984</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.112437</td>
      <td>20.010000</td>
      <td>0.020000</td>
      <td>1039</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.155983</td>
      <td>20.380000</td>
      <td>0.020000</td>
      <td>1090</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.158334</td>
      <td>19.700000</td>
      <td>0.020000</td>
      <td>1131</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.157344</td>
      <td>18.800000</td>
      <td>0.020000</td>
      <td>1182</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.146119</td>
      <td>19.530000</td>
      <td>0.020000</td>
      <td>1243</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.140432</td>
      <td>18.590000</td>
      <td>0.020000</td>
      <td>1293</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.196181</td>
      <td>17.000000</td>
      <td>0.020000</td>
      <td>1346</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.195283</td>
      <td>19.370000</td>
      <td>0.020000</td>
      <td>1406</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.209266</td>
      <td>20.890000</td>
      <td>0.020000</td>
      <td>1451</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.166778</td>
      <td>20.540000</td>
      <td>0.020000</td>
      <td>1493</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.143278</td>
      <td>18.530000</td>
      <td>0.020000</td>
      <td>1549</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.249149</td>
      <td>18.000000</td>
      <td>0.020000</td>
      <td>1594</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.244026</td>
      <td>18.180000</td>
      <td>0.020000</td>
      <td>1653</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.222805</td>
      <td>19.440000</td>
      <td>0.020000</td>
      <td>1695</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.273831</td>
      <td>20.000000</td>
      <td>0.020000</td>
      <td>1737</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.198564</td>
      <td>20.240000</td>
      <td>0.020000</td>
      <td>1788</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.266650</td>
      <td>19.970000</td>
      <td>0.020000</td>
      <td>1846</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.284163</td>
      <td>20.200000</td>
      <td>0.020000</td>
      <td>1901</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.234000</td>
      <td>21.640000</td>
      <td>0.020000</td>
      <td>1953</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.291279</td>
      <td>21.660000</td>
      <td>0.020000</td>
      <td>2002</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.213886</td>
      <td>21.420000</td>
      <td>0.020000</td>
      <td>2059</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.299499</td>
      <td>21.160000</td>
      <td>0.020000</td>
      <td>2096</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.306723</td>
      <td>20.750000</td>
      <td>0.020000</td>
      <td>2147</td>
      <td>00:24</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.350722</td>
      <td>19.570000</td>
      <td>0.020000</td>
      <td>2193</td>
      <td>00:24</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.330054</td>
      <td>19.380000</td>
      <td>0.020000</td>
      <td>2253</td>
      <td>00:21</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.289513</td>
      <td>19.900000</td>
      <td>0.020000</td>
      <td>2298</td>
      <td>00:22</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.295217</td>
      <td>19.260000</td>
      <td>0.020000</td>
      <td>2348</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.377988</td>
      <td>18.760000</td>
      <td>0.020000</td>
      <td>2401</td>
      <td>00:23</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/fastrl_user/fastrl/fastrl/memory/experience_replay.py:134: UserWarning: image is missing from the experience replay. Image section of the replay will not be logged.
  warn(&#39;image is missing from the experience replay. Image section of the replay will not be logged.&#39;)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The loss should be practically zero:  TensorBatch(0.9907, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-1.0440, -1.0236, -1.0498, -0.0246, -1.0358, -1.0149, -1.0102, -0.9955,
        -1.0295, -1.0671, -1.0393, -1.0625, -1.0461, -1.0243, -1.0161, -0.0478,
        -1.0099, -1.0370, -1.0111, -1.0322, -0.9760, -1.0351, -1.0100, -1.0388,
        -0.9724, -1.0122, -1.0463, -1.0477, -1.0482, -1.0032, -1.0263, -1.0659],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(0.9941, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.7403, -1.0462, -1.0342, -1.0266, -1.0569, -1.0326, -1.0578, -1.0104,
        -0.8361, -1.0350, -1.0262, -0.9537, -1.0112, -1.0029, -1.0225, -1.0659,
        -1.0202, -0.7528, -0.9697, -1.0647, -1.0191, -0.8861, -0.9843, -0.9945,
        -1.1034, -0.9715, -0.9514, -0.9955, -1.0290, -1.0312, -1.0427, -1.0244],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(0.9553, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-1.0185, -0.9584,  0.8990,  0.9528, -1.0355, -1.0327, -1.0092, -1.0775,
        -0.9754, -0.8838, -0.9632, -1.0346, -1.0088, -1.0477, -0.8991, -1.0187,
        -1.0642, -1.0263, -0.9478, -1.0115, -1.0032, -1.0402, -0.9881, -0.7587,
        -1.0443, -0.7896, -1.0099, -0.9547, -0.8350, -1.0178, -1.0601, -0.7973],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(0.9588, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.8241, -1.0244, -1.0118, -1.0135,  1.0298, -0.9325, -0.9494, -1.0288,
        -1.0205, -0.9891, -0.5950, -0.9397, -0.7888, -0.9894, -0.8533, -1.0553,
        -1.0736, -1.0246, -1.0122, -0.9711, -1.0122, -0.8550, -1.0351, -1.0126,
        -1.1418, -1.0311, -1.0379, -0.9990, -1.0669, -0.9197, -1.0025, -0.9266],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(1.0159, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.9738, -0.9136, -0.9633, -0.9090, -1.0005, -1.0263, -1.0132, -0.7311,
        -1.0020, -0.9532, -0.8858, -0.7669, -1.0376, -1.0519, -1.0595,  0.8889,
        -0.9872, -1.0615, -1.1490, -0.7453,  1.7700, -1.0987, -1.0165, -0.9961,
        -1.0295, -0.9991, -0.4255, -0.9966, -1.1306, -0.8779, -1.1215, -1.0460],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(0.9412, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.2848, -0.9861, -0.9301, -1.0298, -0.8576, -1.0366, -1.0627, -0.7327,
        -1.0125, -0.6575, -1.0085, -1.1769, -0.9276, -1.2366, -1.0257, -0.9779,
        -1.0205, -0.9124, -1.0567, -0.8535, -1.0326, -1.0437, -0.1402, -1.0329,
        -0.9745, -0.9426, -1.0366, -1.0727, -1.0830, -1.0256, -1.0922, -0.9812],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(0.9559, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.9958, -0.9480, -0.9336, -0.9065, -1.0435,  1.2460, -0.6082, -1.3118,
        -1.1426, -1.1379, -0.8994, -1.2348, -1.0665, -0.8708, -0.9839, -0.9996,
        -1.0454, -1.0811, -0.8362, -0.9554, -0.3836, -0.9011, -0.8181, -1.0746,
         0.1780, -1.0884, -1.2873, -0.6144, -0.8937, -1.0146, -0.9254, -0.9402],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(1.0946, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-1.1649, -1.0036, -1.4222, -0.9720, -1.4222, -1.1738, -1.0411, -1.0109,
        -1.2337, -0.8495, -0.6120, -1.0750, -0.8990, -0.9928, -0.4254, -0.9056,
        -0.8031, -1.0797, -1.4249, -1.2271, -1.0268, -0.9696, -1.1557,  1.0655,
        -0.6010, -1.0378, -1.0100, -0.9253, -1.1409, -0.9776, -1.0320, -1.0832],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
The loss should be practically zero:  TensorBatch(1.5047, device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
TensorBatch([-0.9195, -1.0776, -1.0518, -1.4092, -0.9261, -1.2703, -1.0708, -0.8932,
        -0.9700, -0.2185, -1.0065, -0.6596, -0.9664, -0.8329, -0.9280,  4.2690,
        -0.9530, -0.9550, -0.9449, -1.0306, -0.5597, -0.8922, -1.1217, -1.1185,
        -0.9802, -0.9308, -1.2203, -0.9336, -0.7687, -0.9156, -1.2860, -0.9073],
       device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

