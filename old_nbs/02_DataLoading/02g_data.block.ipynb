{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "import inspect\n",
    "from typing import Any,Callable,Generator\n",
    "from inspect import isfunction,ismethod\n",
    "import pickle\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "from torchdata.dataloader2.dataloader2 import DataLoader2\n",
    "from torchdata.dataloader2.graph import find_dps,traverse,DataPipe,IterDataPipe,MapDataPipe\n",
    "from fastai.torch_core import *\n",
    "from fastai.data.transforms import *\n",
    "import torchdata.datapipes as dp\n",
    "from collections import deque\n",
    "from fastai.imports import *\n",
    "# Local modules\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.core import *\n",
    "from fastrl.data.dataloader2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# Data Block\n",
    "> High level API to quickly get your data in a `DataLoader`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63392058-2c52-4282-94f7-3c01400845d9",
   "metadata": {},
   "source": [
    "## Transform Block\n",
    "> Loosely similar to the fastai==2.* `TransformBlock`, only this time, just like the fastrl **Agent** and **Learner**, is simply a *DataPipe* construction\n",
    "function with augmentation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f5942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "DataPipeOrDataLoader = Union[DataPipe,DataLoader2]\n",
    "TransformBlock = Callable[[Union[Iterable,DataPipe]],DataPipeOrDataLoader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52424cc1",
   "metadata": {},
   "source": [
    "`DataBlock` as defined below expects single or tuples of `TransformBlock` callables. These functions \n",
    "need to have the above signatures. \n",
    "\n",
    "Note that a `TransformBlock` **must** take params `source` and `as_dataloader` at minimum. \n",
    "Additional params are up to the developer / user.\n",
    "\n",
    "\n",
    "The simplest example would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b675b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestTransformBlock(\n",
    "    # Pipeline Parameters\n",
    "    a:int=1,\n",
    "    b:str='_test',\n",
    "    # Additional pipelines to insert, replace, remove\n",
    "    dp_augmentation_fns:Tuple[DataPipeAugmentationFn]=None\n",
    ") -> TransformBlock:\n",
    "    \"This function returns a pipeline builder that either will return a DataPipe or a DataLoader\"\n",
    "    def _TestTransformBlock(\n",
    "        # `source` likely will be an iterable that gets pushed into the pipeline when an \n",
    "        # experiment is actually being run.\n",
    "        source:Any,\n",
    "        # Any parameters needed for the dataloader\n",
    "        num_workers:int=0,\n",
    "        # This param must exist: as_dataloader for the datablock to create dataloaders\n",
    "        as_dataloader:bool=False\n",
    "    ) -> DataPipeOrDataLoader:\n",
    "        \"This is the function that is actually run by `DataBlock`\"\n",
    "        # This is where the template pipeline gets outlined. Notice that we\n",
    "        # are feeding source into the pipeline.\n",
    "        pipe = dp.iter.IterableWrapper(source) # In this example, probably a list of numbers\n",
    "        pipe = pipe.map(lambda o:o+a)          # Add `a` to them\n",
    "        pipe = pipe.map(lambda o:str(o))       # Convert the numbers to str\n",
    "        pipe = pipe.map(lambda o:o+b)          # Concat `b` into the str\n",
    "        # Once the base pipeline is constructed, we give the user the opportinuty to augment the \n",
    "        # pipeline however they like.\n",
    "        pipe = apply_dp_augmentation_fns(pipe,ifnone(dp_augmentation_fns,()))\n",
    "        # The transform block must be able to return a `DataLoader2` instance\n",
    "        if as_dataloader:\n",
    "            pipe = DataLoader2(\n",
    "                datapipe=pipe,\n",
    "                reading_service=PrototypeMultiProcessingReadingService(\n",
    "                    num_workers = num_workers,\n",
    "                    protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "                    protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "                    pipe_type = item_input_pipe_type,\n",
    "                    eventloop = SpawnProcessForDataPipeline\n",
    "                ) if num_workers>0 else None\n",
    "            )\n",
    "        return pipe \n",
    "    return _TestTransformBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1a6a4",
   "metadata": {},
   "source": [
    "Check that we can return a `DataPipe` and that an iteration through it is what we \n",
    "expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c46762",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_block = TestTransformBlock()\n",
    "pipe = tfm_block([1,2,3])\n",
    "test_eq(type(pipe),dp.iter.Mapper)\n",
    "test_eq(list(pipe),['2_test', '3_test', '4_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458b8fe",
   "metadata": {},
   "source": [
    "Check that we can return a `DataLoader2` and that an iteration through it is what we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4490218",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_block = TestTransformBlock()\n",
    "pipe = tfm_block([1,2,3],as_dataloader=True)\n",
    "test_eq(type(pipe),DataLoader2)\n",
    "test_eq(list(pipe),['2_test', '3_test', '4_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06d3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class InvalidTransformBlock(Exception):pass\n",
    "\n",
    "def validate_transform_block(block:TransformBlock):\n",
    "    msg = f\"Checked {block}:\"\n",
    "    failed = False \n",
    "    kwargs = dict(inspect.signature(block).parameters)\n",
    "    msg += f'\\nGiven kwargs: {kwargs}'\n",
    "    msg += f'\\nGiven return: {inspect.signature(block).return_annotation}'\n",
    "    if 'source' not in kwargs:\n",
    "        failed = True\n",
    "        msg += f'\\n`source:Any` is missing from the arguments'\n",
    "    if 'as_dataloader' not in kwargs:\n",
    "        failed = True\n",
    "        msg += f'\\n`as_dataloader:bool=False` is missing from the arguments'\n",
    "    if inspect.signature(block).return_annotation != DataPipeOrDataLoader:\n",
    "        failed = True\n",
    "        msg += f'\\n`DataPipeOrDataLoader` missing from return signature'\n",
    "    if failed: raise InvalidTransformBlock(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7593049",
   "metadata": {},
   "source": [
    "Check that `TestTransformBlock` is infact valid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d29363",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_transform_block(tfm_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdafc79",
   "metadata": {},
   "source": [
    "And check that invalid `TransformBlock`s get caught..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46634d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked <function invalid_transform_block.<locals>._invalid_transform_block at 0x7fddc6d60b90>:\n",
      "Given kwargs: {}\n",
      "Given return: <class 'inspect._empty'>\n",
      "`source:Any` is missing from the arguments\n",
      "`as_dataloader:bool=False` is missing from the arguments\n",
      "`DataPipeOrDataLoader` missing from return signature\n"
     ]
    }
   ],
   "source": [
    "def invalid_transform_block():\n",
    "    def _invalid_transform_block():pass\n",
    "    return _invalid_transform_block\n",
    "\n",
    "invalid_tfm_block = invalid_transform_block()\n",
    "with ExceptionExpected(InvalidTransformBlock):\n",
    "    try: validate_transform_block(invalid_tfm_block)\n",
    "    except InvalidTransformBlock as e:\n",
    "        print(str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DataPipeWrapperTransformBlock(\n",
    "    dp_cls:DataPipe, # The `DataPipe` to wrap into a `TransformBlock`,\n",
    "    **dp_kwargs\n",
    ") -> TransformBlock:\n",
    "    \"Used by `DataBlock` to support converting `DataPipe`s to `TransformBlock`s on the fly.\"\n",
    "    def _DataPipeWrapperTransformBlock(\n",
    "        # `source` likely will be an iterable that gets pushed into the pipeline when an \n",
    "        # experiment is actually being run.\n",
    "        source:Any,\n",
    "        # Any parameters needed for the dataloader\n",
    "        num_workers:int=0,\n",
    "        # If True, returns a `DataLoader2` instead of `DataPipe`\n",
    "        as_dataloader:bool=False\n",
    "    ) -> DataPipeOrDataLoader:\n",
    "\n",
    "        pipe = dp_cls(source,**dp_kwargs) \n",
    "        if as_dataloader:\n",
    "            pipe = DataLoader2(\n",
    "                datapipe=pipe,\n",
    "                reading_service=PrototypeMultiProcessingReadingService(\n",
    "                    num_workers = num_workers,\n",
    "                    protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "                    protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "                    pipe_type = item_input_pipe_type,\n",
    "                    eventloop = SpawnProcessForDataPipeline\n",
    "                ) if num_workers>0 else None\n",
    "            )\n",
    "        return pipe \n",
    "    return _DataPipeWrapperTransformBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1a6a4",
   "metadata": {},
   "source": [
    "Check that we can return a `DataPipe` and that an iteration through it is what we \n",
    "expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c46762",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_block = TestTransformBlock()\n",
    "pipe = tfm_block([1,2,3])\n",
    "test_eq(type(pipe),dp.iter.Mapper)\n",
    "test_eq(list(pipe),['2_test', '3_test', '4_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458b8fe",
   "metadata": {},
   "source": [
    "Check that we can return a `DataLoader2` and that an iteration through it is what we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd34f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_DataBlock_msg = \"\"\"Interpreting `blocks` input as %s, resulting in %s dataloaders\"\"\"\n",
    "\n",
    "class DataBlock(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A tuple of `TransformBlock`s to convert to `DataPipe`s or `DataLoader2`s.\n",
    "        *blocks:Tuple[Union[Tuple[TransformBlock],TransformBlock]],\n",
    "        # Debug mode for verbose output\n",
    "        debug:bool=False\n",
    "    ):\n",
    "        self.blocks = blocks \n",
    "\n",
    "        if debug:\n",
    "            block_types = [['datapipe','datapipe_group'][type(b)==tuple] for b in blocks]\n",
    "            print(_DataBlock_msg%(block_types, len(blocks)))\n",
    "\n",
    "    def blocks2dp_or_dl(self,\n",
    "            # Passed into the `blocks`, likely as an iterable.\n",
    "            source:Any,\n",
    "            # Single `TransformBlock` or tuples of `TransformBlock`s that are\n",
    "            # executed and chained together into a single `DataPipe`.\n",
    "            blocks:Union[TransformBlock,Tuple[TransformBlock]],\n",
    "            # If True, a `DataLoader2` instance is returned instead of a `DataPipe`  \n",
    "            as_dataloader:bool=False,\n",
    "            # Number of workers to use for the dataloader.\n",
    "            # Requires `as_dataloader=True`\n",
    "            num_workers:int=0\n",
    "        ) -> DataPipeOrDataLoader:\n",
    "        if type(blocks)!=tuple:\n",
    "            validate_transform_block(blocks)\n",
    "            pipe = blocks(source,as_dataloader=as_dataloader,num_workers=num_workers)\n",
    "        elif len(blocks)==1:\n",
    "            validate_transform_block(blocks[0])\n",
    "            pipe = blocks[0](source,as_dataloader=as_dataloader,num_workers=num_workers)\n",
    "        else:\n",
    "            for b in blocks: validate_transform_block(b)\n",
    "            pipe = blocks[0](source)\n",
    "            for sub_block in blocks[1:-1]: pipe = sub_block(pipe)\n",
    "            pipe = blocks[-1](pipe,as_dataloader=as_dataloader,num_workers=num_workers)\n",
    "        return pipe\n",
    "\n",
    "\n",
    "    def datapipes(self,source:Any):\n",
    "        return tuple(self.blocks2dp_or_dl(source,b) for b in self.blocks)\n",
    "\n",
    "    def dataloaders(self,source:Any,num_workers=0):\n",
    "        return tuple(\n",
    "            self.blocks2dp_or_dl(source,b,as_dataloader=True,num_workers=num_workers) \n",
    "            for b in self.blocks\n",
    "        )\n",
    "\n",
    "\n",
    "add_docs(DataBlock,\n",
    "\"\"\"`DataBlock` is a single object for constructing datapipes and dataloaders from `blocks`.\n",
    "Below are examples on how `blocks` eventually get converted to dataloaders.\n",
    "\n",
    "Example 1: Simplest\n",
    "blocks = (\n",
    "    TestTransformBlock,\n",
    "    TestTransformBlock\n",
    ") -> (\n",
    "    DataLoader2(TestTransformBlock(as_dataloader=True)),\n",
    "    DataLoader2(TestTransformBlock(as_dataloader=True))\n",
    ")\n",
    "\n",
    "Example 2: Nested Blocks\n",
    "blocks = (\n",
    "    (TestTransformBlock,TestTransformBlock2),\n",
    "    TestTransformBlock\n",
    ") -> (\n",
    "    DataLoader2(TestTransformBlock -> TestTransformBlock2(as_dataloader=True)),\n",
    "    DataLoader2(TestTransformBlock)\n",
    ")\n",
    "\n",
    "In example 2, we can nest the blocks, thus chaining them together. The last\n",
    "one in the chain is used to create the dataloader that is required.\n",
    "\"\"\",\n",
    "# wrap_dps=\"Wrap any `DataPipe`s in `DataPipeWrapperTransformBlock` in `self.blocks`\",\n",
    "blocks2dp_or_dl=\"\"\"Passes `source` into single `TransformBlock`s or passes `source`\n",
    "to chained `TransformBlock` outputs. In either case, it results in a single `DataPipe`. \n",
    "\n",
    "If `as_dataloader` is True, then a `DataLoader2` instance is returned instead.\n",
    "\"\"\",\n",
    "datapipes=\"\"\"Combines `self.blocks` with `source` where `bs` can be defined. `n=None` means \n",
    "that the datapipes are infinite / lengthless. If `n` is an integer then the datapipes will have \n",
    "an expected max len.\n",
    "\"\"\",\n",
    "dataloaders=\"Returns a dataloader for each respoctive combination of blocks.\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8686a",
   "metadata": {},
   "source": [
    "In the below example we want 2 dataloaders, so we the len(blocks) will be 2. However,\n",
    "for the second dataloader we want to change the output, and also cycle twice. We can easily do this\n",
    "by using a tuple instead of a single `TestTransformBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting `blocks` input as ['datapipe', 'datapipe_group'], resulting in 2 dataloaders\n"
     ]
    }
   ],
   "source": [
    "block = DataBlock(\n",
    "    TestTransformBlock(),\n",
    "    (TestTransformBlock(b='_test2'),DataPipeWrapperTransformBlock(dp.iter.Cycler,count=2)),\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60aae1",
   "metadata": {},
   "source": [
    "The resulting datapipes are in the format that we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = block.datapipes([1,2,3])\n",
    "traverse(pipes[0])\n",
    "test_eq(type(pipes[0]),dp.iter.Mapper)\n",
    "test_eq(list(pipes[0]),['2_test', '3_test', '4_test'])\n",
    "# Second pipe has _test2 as a postfix and cycles the dataset twice\n",
    "test_eq(type(pipes[1]),dp.iter.Cycler)\n",
    "test_eq(list(pipes[1]),['2_test2', '3_test2', '4_test2', '2_test2', '3_test2', '4_test2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6c78c",
   "metadata": {},
   "source": [
    "We can easily do the same for the dataloaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dece84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapipe\n",
      "Got pickle error:  Can't pickle local object 'TestTransformBlock.<locals>._TestTransformBlock.<locals>.<lambda>'  for key  datapipe\n",
      "_adapted\n",
      "_datapipe_iter\n",
      "Got pickle error:  can't pickle generator objects  for key  _datapipe_iter\n",
      "_reset_iter\n",
      "datapipe_adapter_fns\n",
      "reading_service\n",
      "reading_service_state\n",
      "_terminated\n",
      "valid_iterator_id\n",
      "_datapipe_before_reading_service_adapt\n",
      "Got pickle error:  Can't pickle local object 'TestTransformBlock.<locals>._TestTransformBlock.<locals>.<lambda>'  for key  _datapipe_before_reading_service_adapt\n"
     ]
    }
   ],
   "source": [
    "from shutil import ExecError\n",
    "\n",
    "\n",
    "pipes = block.dataloaders([1,2,3])\n",
    "test_eq(type(pipes[0]),DataLoader2)\n",
    "test_eq(list(pipes[0]),['2_test', '3_test', '4_test'])\n",
    "# Second pipe has _test2 as a postfix and cycles the dataset twice\n",
    "test_eq(type(pipes[1]),DataLoader2)\n",
    "test_eq(list(pipes[1]),['2_test2', '3_test2', '4_test2', '2_test2', '3_test2', '4_test2'])\n",
    "with ExceptionExpected(TypeError):\n",
    "    traverse(dp.iter.IterableWrapper(pipes))\n",
    "    print('torchdata dataloaders are not traverseable once started.')\n",
    "\n",
    "# TODO: Kind of what I was a afraid of for the transform blocks. In reality,\n",
    "# I think they should have their inner functions already returned before any\n",
    "# pickling happens, so this technically shouldn't be happening.\n",
    "# There are other issues with the dataloader itself though that can only be fixed \n",
    "# in torch data.\n",
    "for k in pipes[0].__dict__:\n",
    "    try:\n",
    "        print(k)\n",
    "        pickle.dumps(pipes[0].__dict__[k])\n",
    "    except Exception as e:\n",
    "        print('Got pickle error: ',str(e),' for key ',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8d8e7-e153-41a3-ac31-22f936720d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
