{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastrl['dev'] pyvirtualdisplay && \\\n",
    "                     apt-get install -y xvfb python-opengl > /dev/null 2>&1 \n",
    "# NOTE: IF YOU SEE VERSION ERRORS, IT IS SAFE TO IGNORE THEM. COLAB IS BEHIND IN SOME OF THE PACKAGE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "viral-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev.showdoc import *\n",
    "    from nbdev.imports import *\n",
    "    if not os.environ.get(\"IN_TEST\", None):\n",
    "        assert IN_NOTEBOOK\n",
    "        assert not IN_COLAB\n",
    "        assert IN_IPYTHON\n",
    "else:\n",
    "    # Virutual display is needed for colab\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp agents.dqn.asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Python native modules\n",
    "import os\n",
    "from collections import deque\n",
    "# Third party libs\n",
    "from fastcore.all import *\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.datapipes._typing import _DataPipeMeta, _IterDataPipeMeta\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "\n",
    "from fastai.torch_basics import *\n",
    "from fastai.torch_core import *\n",
    "from torchdata.dataloader2.graph import find_dps,traverse\n",
    "# Local modules\n",
    "\n",
    "from fastrl.core import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.pipes.core import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.memory.experience_replay import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.agents.discrete import *\n",
    "from fastrl.loggers.core import *\n",
    "from fastrl.loggers.jupyter_visualizers import *\n",
    "from fastrl.learner.core import *\n",
    "from fastrl.agents.dqn.basic import *\n",
    "from fastrl.data.dataloader2 import *\n",
    "from torchdata.dataloader2 import DataLoader2,DataLoader2Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce3394-5e44-4dba-b196-0ab324359b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lesser-innocent",
   "metadata": {},
   "source": [
    "# DQN Async\n",
    "> Components that allow for syncing multiple dqn agents on multiple processes to calcualtions on the\n",
    "main process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50939a1-a607-411a-8506-421517ab1c29",
   "metadata": {},
   "source": [
    "There is a little weirdness using cuda with spawn. pytorch has a bug: https://github.com/pytorch/pytorch/issues/30401 so queue usage isnt so simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c98be0-6288-443a-b4ab-9390fbe3081c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Training DataPipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf265e3-d03f-4514-8de6-a9aa568701cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ModelSubscriber(dp.iter.IterDataPipe):\n",
    "    \"If an agent is passed to another process and 'spawn' start method is used, then this module is needed.\"\n",
    "    def __init__(self,source_datapipe): \n",
    "        super().__init__()\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.model = find_dp(traverse(self.source_datapipe,only_datapipe=True),AgentBase).model\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for x in self.source_datapipe:\n",
    "            if type(x)==GetInputItemRequest and x.key.startswith('model_state_dict_publish_'):\n",
    "                self.model.load_state_dict(x.value)\n",
    "                continue\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8c66df-7063-4c77-915b-2c2c8c1b4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ModelPublisher(dp.iter.IterDataPipe):\n",
    "    def __init__(self,\n",
    "            source_datapipe,\n",
    "            publish_freq:int=1,\n",
    "            # Sometimes its not possible to share current model due to cuda issues.\n",
    "            # `do_deepcopy` will copy and move the model to cpu in order to publish it.\n",
    "            do_deepcopy:bool=False\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.source_datapipe = source_datapipe\n",
    "        self.model = find_dp(traverse(self,only_datapipe=True),LearnerBase).model\n",
    "        self.publish_freq = publish_freq\n",
    "        self.protocol_clients = []\n",
    "        self._expect_response = []\n",
    "        self.initialized = False\n",
    "        self.do_deepcopy = do_deepcopy\n",
    " \n",
    "    def _reset(self):\n",
    "        for dl in find_dp(traverse(self,only_datapipe=True),LearnerBase).iterable:\n",
    "            for q_wrapper in dl.datapipe.iterable.datapipes:\n",
    "                self.protocol_clients.append(q_wrapper.protocol)\n",
    "                self._expect_response.append(False)\n",
    "        self.initialized = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i,batch in enumerate(self.source_datapipe):\n",
    "            # print('Got batch: ',batch)\n",
    "            if not self.initialized: self._reset()\n",
    "            #  (this batch we should publish) and (there are protocols) and (there are some that are ready)\n",
    "            if type(batch)==GetInputItemResponse and batch.value.startswith('model_state_dict_publish_'): \n",
    "                client_num = int(batch.value.replace('model_state_dict_publish_',''))\n",
    "\n",
    "                if self._expect_response[client_num]:\n",
    "                    self._expect_response[client_num] = False\n",
    "\n",
    "                continue\n",
    "            if i%self.publish_freq==0 and self.protocol_clients and not all(self._expect_response):\n",
    "                with torch.no_grad():\n",
    "                    # We need to deepcopy the model itself since `cpu` is an inplace op.\n",
    "                    # We cant keep the model in cuda because mp.Manager passes around the \n",
    "                    # tensors too much and causes errors ref: https://github.com/pytorch/pytorch/issues/30401\n",
    "                    # This is alos why we cant just call state_dict directly. It returns references\n",
    "                    # to cuda tensors.\n",
    "                    if self.do_deepcopy:\n",
    "                        state = deepcopy(self.model).to(device=self.device).state_dict()\n",
    "                    else:\n",
    "                        state = self.model.state_dict()\n",
    "\n",
    "                        \n",
    "                    for client_id,client in enumerate(self.protocol_clients):\n",
    "                        if not self._expect_response[client_id]: \n",
    "                            # print('PUBLISHING!!!!')\n",
    "                            client.request_input_item(\n",
    "                                key=f'model_state_dict_publish_{client_id}',value=state\n",
    "                            )\n",
    "                            self._expect_response[client_id] = True\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976d9e60-340c-48a8-98fc-7ab06d91617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def DQNLearner(\n",
    "    model,\n",
    "    dls,\n",
    "    agent,\n",
    "    logger_bases=None,\n",
    "    loss_func=MSELoss(),\n",
    "    opt=AdamW,\n",
    "    lr=0.005,\n",
    "    bs=128,\n",
    "    max_sz=10000,\n",
    "    nsteps=1,\n",
    "    device=None,\n",
    "    batches=1000,\n",
    "    publish_freq=1\n",
    ") -> LearnerHead:\n",
    "    learner = LearnerBase(model,dls,batches=batches,loss_func=MSELoss(),opt=opt(model.parameters(),lr=lr))\n",
    "    learner = ModelPublisher(learner,publish_freq=publish_freq)\n",
    "    learner = BatchCollector(learner,logger_bases=logger_bases,batch_on_pipe=LearnerBase)\n",
    "    learner = EpocherCollector(learner,logger_bases=logger_bases)\n",
    "    for logger_base in L(logger_bases): learner = logger_base.connect_source_datapipe(learner)\n",
    "    if logger_bases: \n",
    "        learner = RollingTerminatedRewardCollector(learner,logger_bases)\n",
    "        learner = EpisodeCollector(learner,logger_bases)\n",
    "    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz)\n",
    "    learner = StepBatcher(learner,device=device)\n",
    "    learner = QCalc(learner,nsteps=nsteps)\n",
    "    learner = ModelLearnCalc(learner)\n",
    "    if logger_bases: \n",
    "        learner = LossCollector(learner,logger_bases)\n",
    "    learner = LearnerHead(learner)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e13a364-9dff-4677-ab77-417e2b85ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CacheLoggerBase(LoggerBase):\n",
    "    \"Short lived logger base meant to dump logs\"\n",
    "    def reset(self):\n",
    "        # This logger will be exhausted frequently if used in an agent.\n",
    "        # We need to get the buffer alive so we dont lose reference\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        print('Iterating through buffer of len: ',len(self.buffer))\n",
    "        yield from self.buffer\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee8c966-7e72-4168-a65e-0076ab752a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export   \n",
    "def DQNAgent(\n",
    "    model,\n",
    "    logger_bases=None,\n",
    "    min_epsilon=0.02,\n",
    "    max_epsilon=1,\n",
    "    max_steps=1000,\n",
    "    device='cpu'\n",
    ")->AgentHead:\n",
    "    agent_base = AgentBase(model,logger_bases=ifnone(logger_bases,[CacheLoggerBase()]))\n",
    "    agent = StepFieldSelector(agent_base,field='state')\n",
    "    agent = InputInjester(agent)\n",
    "    agent = ModelSubscriber(agent)\n",
    "    agent = SimpleModelRunner(agent,device=device)\n",
    "    agent = ArgMaxer(agent)\n",
    "    selector = EpsilonSelector(agent,min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps,device=device)\n",
    "\n",
    "    agent = EpsilonCollector(selector,agent_base.logger_bases)\n",
    "\n",
    "    agent = ArgMaxer(agent,only_idx=True)\n",
    "    agent = NumpyConverter(agent)\n",
    "    agent = PyPrimativeConverter(agent)\n",
    "    agent = AgentHead(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f9ed8-fb05-40a1-ac0d-d4cafee8fa07",
   "metadata": {},
   "source": [
    "Try training with basic defaults..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5c9d13-2426-4543-9f2f-ec572cf8a859",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16517/2049894555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Setup up the core NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# model.share_memory() # This will not work in spawn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Setup the Agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \"\"\"\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \"\"\"\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import *\n",
    "import torch.nn.functional as F\n",
    "from fastrl.loggers.core import *\n",
    "from fastrl.loggers.jupyter_visualizers import *\n",
    "from fastrl.learner.core import *\n",
    "from fastrl.data.block import *\n",
    "from fastrl.envs.gym import *\n",
    "from fastrl.agents.core import *\n",
    "from fastrl.agents.discrete import *\n",
    "from torch.utils.data.dataloader_experimental import DataLoader2\n",
    "\n",
    "logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                 batch_on_pipe=BatchCollector)\n",
    "\n",
    "# Setup up the core NN\n",
    "torch.manual_seed(0)\n",
    "model = DQN(4,2).cuda()\n",
    "# model.share_memory() # This will not work in spawn\n",
    "# Setup the Agent\n",
    "agent = DQNAgent(model,max_steps=4000,device='cuda')\n",
    "# Setup the DataBlock\n",
    "block = DataBlock(\n",
    "    blocks = GymTransformBlock(agent=agent,\n",
    "                               nsteps=1,nskips=1,firstlast=False,\n",
    "                               # dl_type=partial(DataLoader2,persistent_workers=True)\n",
    "                              )\n",
    ")\n",
    "# pipes = L(block.datapipes(['CartPole-v1']*1,n=10))\n",
    "dls = L(block.dataloaders(['CartPole-v1']*1,bs=1,num_workers=0))\n",
    "# # Setup the Learner\n",
    "learner = DQNLearner(model,dls,[agent],batches=1000,logger_bases=[logger_base],bs=128,max_sz=100_000,device='cuda')\n",
    "# learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ec6fd-0126-4dc6-a5d8-a8c6b01a302b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "find_dp(traverse(dls[0].dataset),AgentBase).logger_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2d318-92e6-4ab4-95f2-b4dae6660e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%writefile external_run_scripts/agents_dqn_async_35.py\n",
    "# %%python\n",
    "\n",
    "if __name__=='__main__':\n",
    "    from torch.multiprocessing import Pool, Process, set_start_method\n",
    "    \n",
    "    try:\n",
    "        set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    \n",
    "    from fastcore.all import *\n",
    "    import torch\n",
    "    from torch.nn import *\n",
    "    import torch.nn.functional as F\n",
    "    from fastrl.loggers.core import *\n",
    "    from fastrl.loggers.jupyter_visualizers import *\n",
    "    from fastrl.learner.core import *\n",
    "    from fastrl.data.block import *\n",
    "    from fastrl.envs.gym import *\n",
    "    from fastrl.agents.core import *\n",
    "    from fastrl.agents.discrete import *\n",
    "    from fastrl.agents.dqn.basic import *\n",
    "    from fastrl.agents.dqn.asynchronous import *\n",
    "    \n",
    "    from torchdata.dataloader2 import DataLoader2\n",
    "    from fastrl.data.dataloader2 import *\n",
    "    \n",
    "    logger_base = ProgressBarLogger(epoch_on_pipe=EpocherCollector,\n",
    "                     batch_on_pipe=BatchCollector)\n",
    "    \n",
    "    # RollingTerminatedRewardCollector.debug=True\n",
    "\n",
    "    # Setup up the core NN\n",
    "    torch.manual_seed(0)\n",
    "    model = DQN(4,2).cuda()\n",
    "    # model.share_memory() # This will not work in spawn\n",
    "    # Setup the Agent\n",
    "    agent = DQNAgent(model,max_steps=8000,device='cuda')\n",
    "    # Setup the DataBlock\n",
    "    block = DataBlock(\n",
    "        blocks = GymTransformBlock(agent=agent,\n",
    "                                   nsteps=1,nskips=1,firstlast=False\n",
    "                                  )\n",
    "    )\n",
    "    pipe = L(block.datapipes(['CartPole-v1']*1))\n",
    "    \n",
    "    dl = DataLoader2(\n",
    "        pipe[0],\n",
    "        reading_service=PrototypeMultiProcessingReadingService(\n",
    "            num_workers = 5,\n",
    "            # persistent_workers=True,\n",
    "            protocol_client_type = InputItemIterDataPipeQueueProtocolClient,\n",
    "            protocol_server_type = InputItemIterDataPipeQueueProtocolServer,\n",
    "            pipe_type = item_input_pipe_type,\n",
    "            eventloop = SpawnProcessForDataPipeline\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dls = [dl]\n",
    "    \n",
    "    # from torchdata.dataloader2.graph import find_dps,traverse\n",
    "    # print(traverse(dls[0].datapipe))\n",
    "    \n",
    "    # dls = L(block.dataloaders(['CartPole-v1']*1,n=1000,bs=1,num_workers=1))\n",
    "    # print('persistent workers: ',dls[0].persistent_workers)\n",
    "    # # Setup the Learner\n",
    "    learner = DQNLearner(model,dls,[agent],batches=1000,logger_bases=[logger_base],\n",
    "                         publish_freq=100,\n",
    "                         bs=128,max_sz=100_000,device='cuda')\n",
    "    learner.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-pilot",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from fastcore.imports import in_colab\n",
    "\n",
    "# Since colab still requires tornado<6, we don't want to import nbdev if we don't have to\n",
    "if not in_colab():\n",
    "    from nbdev import nbdev_export\n",
    "    nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b33237-1b21-4680-9ebb-03f21b7e0356",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
