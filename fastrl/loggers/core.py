# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/05_Logging/09a_loggers.core.ipynb.

# %% auto 0
__all__ = ['not_record', 'LoggerBase', 'LogCollector', 'EpocherCollector', 'BatchCollector', 'ProgressBarLogger',
           'RewardCollector', 'EpisodeCollector', 'RollingTerminatedRewardCollector', 'CacheLoggerBase']

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 2
# Python native modules
from typing import Optional,List,Any,Iterable
from collections import deque
# Third party libs
from fastcore.all import add_docs,merge,ifnone
# from torch.multiprocessing import Pool,Process,set_start_method,Manager,get_start_method,Queue
import torchdata.datapipes as dp
from fastprogress.fastprogress import master_bar,progress_bar
from torchdata.dataloader2.graph import find_dps,traverse_dps
# from torch.utils.data.datapipes._hook_iterator import _SnapshotState
import numpy as np
# Local modules
# from fastrl.core import *
from ..pipes.core import find_dp,find_dps
from ..core import StepType,Record
# from fastrl.torch_core import *

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 4
def not_record(data:Any):
    "Intended for use with dp.iter.Filter"
    return type(data)!=Record

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 7
class LoggerBase(object):
    debug:bool
    buffer:list
        
    def connect(self,pipe:dp.iter.IterDataPipe):
        self.source_datapipe = pipe
        return self
    
    def dequeue(self): 
        while self.buffer: yield self.buffer.pop(0)
    
    # def reset(self):
        # Note: trying to decide if this is really needed.
        # if self.debug:
        #     print(self,' resetting buffer.')
        # if self._snapshot_state!=_SnapshotState.Restored:
        #     self.buffer = []
        
add_docs(
    LoggerBase,
    """The `LoggerBase` class is an iterface for datapipes that also collect `Record` objects
    for logging purposes.
    """,
    connect_source_datapipe="""`LoggerBase` does not need to be part of a `DataPipeGraph` 
    when its initialized, so this method allows for inserting into a `DataPipeGraph` later on.""",
    dequeue="Empties the `self.buffer` yielding each of its contents."
)        

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 11
class LogCollector(dp.iter.IterDataPipe):
    debug:bool=False
    title:Optional[str]=None

    def __init__(self,
         source_datapipe, # The parent datapipe, likely the one to collect metrics from
        ):
        self.source_datapipe = source_datapipe
        self.main_buffers = None
        
    def __iter__(self): raise NotImplementedError

    def push_title(
            self,
            key:str
        ):
        # self.reset()
        for q in self.main_buffers: q.append(Record(key,None))

    def reset(self):
        if self.main_buffers is None:
            if self.debug: print(f'Resetting {self}')
            logger_bases = find_dps(traverse_dps(self),LoggerBase,include_subclasses=True)
            self.main_buffers = [o.buffer for o in logger_bases]
            self.push_title(self.title)

add_docs(
LogCollector,
"""`LogCollector` specifically manages finding and attaching itself to
`LoggerBase`s found earlier in the pipeline.""",
reset="Grabs buffers from all logger bases in the pipeline.",
push_title="""Should be called after the pipeline is initialized. Sends header
`Record`s to the `LoggerBase`s so they know what logs are going to be sent to them."""
)  


# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 13
class EpocherCollector(LogCollector):
    debug:bool=False
    title:str='epoch'

    def __init__(self,
            source_datapipe,
            # Epochs is the number of times we iterate, and exhaust `source_datapipe`.
            # This is expected behavior of more traditional dataset iteration where
            # an epoch is a single full run through of a dataset.
            epochs:int=0,
            # `LoggerBase`s that we want to send metrics to
            logger_bases:List[LoggerBase]=None 
        ):
        self.source_datapipe = source_datapipe
        self.main_buffers = None
        self.iteration_started = False
        self.epochs = epochs
        self.epoch = 0

    def __iter__(self): 
        for i in range(self.epochs): 
            self.epoch = i
            if self.main_buffers is not None:
                for q in self.main_buffers: q.append(Record('epoch',self.epoch))
            yield from self.source_datapipe
            
add_docs(
EpocherCollector,
"""Tracks the number of epochs that the pipeline is currently on.""",
reset="Grabs buffers from all logger bases in the pipeline."
)

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 16
class BatchCollector(LogCollector):
    title:str='batch'

    def __init__(self,
            source_datapipe,
            batches:Optional[int]=None,
            # If `batches` is None, `BatchCollector` with try to find: `batch_on_pipe` instance
            # and try to grab a `batches` field from there.
            batch_on_pipe:dp.iter.IterDataPipe=None 
        ):
        self.source_datapipe = source_datapipe
        self.main_buffers = None
        self.iteration_started = False
        self.batches = (
            batches if batches is not None else self.batch_on_pipe_get_batches(batch_on_pipe)
        )
        self.batch = 0
        
    def batch_on_pipe_get_batches(self,batch_on_pipe):
        pipe = find_dp(traverse_dps(self.source_datapipe),batch_on_pipe)
        if hasattr(pipe,'batches'):
            return pipe.batches
        elif hasattr(pipe,'limit'):
            return pipe.limit
        else:
            raise RuntimeError(f'Pipe {pipe} isnt recognized as a batch tracker.')

    def __iter__(self): 
        self.batch = 0
        for batch,record in enumerate(self.source_datapipe): 
            yield record
            if type(record)!=Record:
                self.batch += 1
                if self.main_buffers is not None:
                    # print('posting batch values: ',Record('batch',self.batch))
                    for q in self.main_buffers: q.append(Record('batch',self.batch))
            if self.batch>self.batches: 
                break

add_docs(
BatchCollector,
"""Tracks the number of batches that the pipeline is currently on.""",
batch_on_pipe_get_batches="Gets the number of batches from `batch_on_pipe`",
reset="Grabs buffers from all logger bases in the pipeline."
)

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 18
class ProgressBarLogger(LoggerBase):
    debug:bool=False

    def __init__(self,
                 # This does not need to be immediately set since we need the `LogCollectors` to 
                 # first be able to reference its queues.
                 source_datapipe=None, 
                 # For automatic pipe attaching, we can designate which pipe this should be
                 # referneced for information on which epoch we are on
                 epoch_on_pipe:dp.iter.IterDataPipe=EpochCollector,
                 # For automatic pipe attaching, we can designate which pipe this should be
                 # referneced for information on which batch we are on
                 batch_on_pipe:dp.iter.IterDataPipe=BatchCollector
                ):
        super().__init__(source_datapipe=source_datapipe)
        self.epoch_on_pipe = epoch_on_pipe
        self.batch_on_pipe = batch_on_pipe
        
        self.collector_keys = None
        self.attached_collectors = None
    
    def __iter__(self):
        epocher = find_dp(traverse_dps(self),self.epoch_on_pipe)
        batcher = find_dp(traverse_dps(self),self.batch_on_pipe)
        mbar = master_bar(range(epocher.epochs)) 
        pbar = progress_bar(range(batcher.batches),parent=mbar,leave=False)

        mbar.update(0)
        i = 0
        for record in self.source_datapipe:
            if self.filter_record(record):
                self.buffer.append(record)
                # We only want to start setting up logging when the data loader starts producing 
                # real data.
                continue
   
            if i==0:
                self.attached_collectors = {o.name:o.value for o in self.dequeue()}
                if self.debug: print('Got initial values: ',self.attached_collectors)
                mbar.write(self.attached_collectors, table=True)
                self.collector_keys = list(self.attached_collectors)
                pbar.update(0)
                    
            attached_collectors = {o.name:o.value for o in self.dequeue()}
            if self.debug: print('Got running values: ',self.attached_collectors)

            if attached_collectors:
                self.attached_collectors = merge(self.attached_collectors,attached_collectors)
            
            if 'batch' in attached_collectors: 
                pbar.update(attached_collectors['batch'])
                
            if 'epoch' in attached_collectors:
                mbar.update(attached_collectors['epoch'])
                collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}
                mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)
                
            i+=1  
            yield record

        attached_collectors = {o.name:o.value for o in self.dequeue()}
        if attached_collectors: self.attached_collectors = merge(self.attached_collectors,attached_collectors)

        collector_values = {k:self.attached_collectors.get(k,None) for k in self.collector_keys}
        mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in collector_values.values()], table=True)

        pbar.on_iter_end()
        mbar.on_iter_end()
            

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 20
class RewardCollector(LogCollector):
    title:str='reward'

    def __iter__(self):
        for i,steps in enumerate(self.source_datapipe):
            # if i==0: self.push_title('reward')
            if isinstance(steps,dp.DataChunk):
                for step in steps:
                    for q in self.main_buffers: q.append(Record('reward',step.reward.detach().numpy()))
            else:
                for q in self.main_buffers: q.append(Record('reward',steps.reward.detach().numpy()))
            yield steps

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 23
class EpisodeCollector(LogCollector):
    title:str='episode'
    
    def episode_detach(self,step): 
        try:
            v = step.episode_n.cpu().detach().numpy()
            if len(v.shape)==0: return int(v)
            return v[0]
        except IndexError:
            print(f'Got IndexError getting episode_n which is unexpected: \n{step}')
            raise
    
    def __iter__(self):
        for i,steps in enumerate(self.source_datapipe):
            # if i==0: self.push_title('episode')
            if isinstance(steps,dp.DataChunk):
                for step in steps:
                    for q in self.main_buffers: q.append(Record('episode',self.episode_detach(step)))
            else:
                for q in self.main_buffers: q.append(Record('episode',self.episode_detach(steps)))
            yield steps

add_docs(
EpisodeCollector,
"""Collects the `episode_n` field from steps.""",
episode_detach="Moves the `episode_n` tensor to numpy.",
)

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 24
class RollingTerminatedRewardCollector(LogCollector):
    debug:bool=False
    title:str='rolling_reward'

    def __init__(self,
         source_datapipe, # The parent datapipe, likely the one to collect metrics from
         rolling_length:int=100
        ):
        self.source_datapipe = source_datapipe
        self.main_buffers = None
        self.rolling_rewards = deque([],maxlen=rolling_length)
        
    def step2terminated(self,step): return bool(step.terminated)

    def reward_detach(self,step): 
        try:
            v = step.total_reward.cpu().detach().numpy()
            if len(v.shape)==0: return float(v)
            return v[0]
        except IndexError:
            print(f'Got IndexError getting reward which is unexpected: \n{step}')
            raise

    def __iter__(self):
        for i,steps in enumerate(self.source_datapipe):
            if self.debug: print(f'RollingTerminatedRewardCollector: ',steps)
            if isinstance(steps,dp.DataChunk):
                for step in steps:
                    if self.step2terminated(step):
                        self.rolling_rewards.append(self.reward_detach(step))
                        for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))
            elif self.step2terminated(steps):
                self.rolling_rewards.append(self.reward_detach(steps))
                for q in self.main_buffers: q.append(Record('rolling_reward',np.average(self.rolling_rewards)))
            yield steps

add_docs(
RollingTerminatedRewardCollector,
"""Collects the `total_reward` field from steps if `terminated` is true and 
logs a rolling average of size `rolling_length`.""",
reward_detach="Moves the `total_reward` tensor to numpy.",
step2terminated="Casts the `terminated` field in steps to a bool"
)

# %% ../../nbs/05_Logging/09a_loggers.core.ipynb 30
class CacheLoggerBase(LoggerBase):
    "Short lived logger base meant to dump logs"
    def reset(self):
        # This logger will be exhausted frequently if used in an agent.
        # We need to get the buffer alive so we dont lose reference
        pass
    
    def __iter__(self):
        print('Iterating through buffer of len: ',len(self.buffer))
        yield from self.buffer
        self.buffer.clear()
