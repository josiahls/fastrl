# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02e_fastai.data.block.ipynb (unless otherwise specified).

__all__ = ['TransformBlock', 'simple_iter_loader_loop', 'DataBlock', 'NStepPipe', 'NSkipPipe', 'NStepCallback',
           'DictCollate', 'DictToTensor', 'GymTypeTransform', 'GymStepTransform', 'DQN', 'Agent', 'RawOutOfStep',
           'ArgmaxOfStep', 'ToDiscrete', 'DiscreteEpsilonRandomSelect']

# Cell
# Python native modules
import os
from typing import Any,Callable
from inspect import isfunction,ismethod
# Third party libs
from fastcore.all import *
from torch.utils.data.dataloader_experimental import DataLoader2
from fastai.torch_core import *
from fastai.data.transforms import *
import torchdata.datapipes as dp
from collections import deque
from fastai.imports import *
# Local modules
from .load import *
from .pipes.core import *
from ...core import *

# Cell
def _merge_grouper(o):
    if isinstance(o, LambdaType): return id(o)
    elif isinstance(o, type): return o
    elif (isfunction(o) or ismethod(o)): return o.__qualname__
    return o.__class__

def _merge_tfms(*tfms):
    "Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating"
    g = groupby(concat(*tfms), _merge_grouper)
    return L(v[-1] for k,v in g.items()).map(instantiate)

def _zip(x): return L(x).zip()

# Cell
class TransformBlock():
    "A basic wrapper that links defaults transforms for the data block API"
    def __init__(self,
        type_tfms:list=None, # One or more `Transform`s for converting types. These will be re-called if workers!=0 for the dataloader.
        item_tfms:list=None, # `ItemTransform`s, applied on an item
        batch_tfms:list=None, # `Transform`s or `RandTransform`s, applied by batch
        cbs:list=None, # `Callback`s for use in dataloaders
        dl_type:DataLoader2=None, # Task specific `TfmdDL`, defaults to `TfmdDL`
        dls_kwargs:dict=None, # Additional arguments to be passed to `DataLoaders`
    ):
        self.type_tfms  =            L(type_tfms)
        self.item_tfms  =            L(item_tfms)
        self.batch_tfms =            L(batch_tfms)
        self.cbs        =            L(cbs)
        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)

# Cell
def simple_iter_loader_loop(
    items:Iterable,
    cbs:Optional[List[Callback]]=None,
    type_tfms:Optional[Transform]=None,
    item_tfms:Optional[Transform]=None,
    batch_tfms:Optional[Transform]=None,
    bs:int=1,
    n:int=1
):
    pipe = dp.map.SequenceWrapper(items)
    pipe = TypeTransformLoop(pipe, type_tfms=ifnone(type_tfms,L()))
    pipe = dp.map.InMemoryCacheHolder(pipe)
    pipe = dp.iter.MapToIterConverter(pipe) # Will intialize the gym object, which will be an issue when doing multiproc
    pipe = dp.iter.ShardingFilter(pipe)
    pipe = pipe.cycle(count=n)
    pipe = ItemTransformLoop(pipe, item_tfms=ifnone(item_tfms,L()))
    pipe = dp.iter.Batcher(pipe,bs)
    pipe = BatchTransformLoop(pipe, batch_tfms=ifnone(batch_tfms,L()))
    pipe = add_cbs_to_pipes(pipe,cbs)
    return pipe

# Cell
class DataBlock(object):
    def __init__(
        self,
        blocks:List[TransformBlock]=None, # Transform blocks to use
        loader_loop:Callable=None,
        dl_type=None
    ):
        store_attr(but='loader_loop')
        self.loader_loop = ifnone(loader_loop,default_loader_loop)
        blocks = L(self.blocks if blocks is None else blocks)
        blocks = L(b() if callable(b) else b for b in blocks)
        self.type_tfms = blocks.attrgot('type_tfms', L())

        self.cbs = blocks.attrgot('cbs', L())
        self.item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))
        self.batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))
        for b in blocks:
            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type
        if dl_type is not None: self.dl_type = dl_type
        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)
        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))

    def datapipes(
        self,
        source:Any,
        bs=1,
        n=1,
        **kwargs,
    ):
        return L(self.loader_loop(
            source,
            cbs=cbs,
            type_tfms=type_tfms,
            item_tfms=self.item_tfms,
            batch_tfms=self.batch_tfms,
            bs=bs,
            n=n,
            **kwargs
        ) for type_tfms,cbs in zip(self.type_tfms,self.cbs))

    def dataloaders(
        self,
        source:Any,
        n_workers=0,
        **kwargs
    ):
        pipes = self.datapipes(source,**kwargs)
        return L(pipes).map(DataLoader2,num_workers=n_workers,**self.dls_kwargs)

# Cell
import gym


class NStepPipe(dp.iter.IterDataPipe):

    def __init__(self, source_datapipe, n=1, **kwargs) -> None:
        self.source_datapipe = source_datapipe
        self.n = n
        self.kwargs = kwargs

    def __iter__(self):
        buffer = []
        for o in self.source_datapipe:
            if not type(o)==dict:
                raise Exception(f'Expected dict object generated from `make_step` got {type(o)}\n{o}')

            buffer.append(o)
            if not o['done'] and len(buffer)<self.n: continue

            while o['done'] and len(buffer)!=0:
                yield tuple(buffer)
                buffer.pop(0)

            if not o['done']:
                yield tuple(buffer)
                buffer.pop(0)

class NSkipPipe(dp.iter.IterDataPipe):

    def __init__(self, source_datapipe, n=1, **kwargs) -> None:
        self.source_datapipe = source_datapipe
        self.n = n
        self.kwargs = kwargs

    def __iter__(self):
        skip_idx = 0
        for o in self.source_datapipe:
            if not type(o)==dict:
                raise Exception(f'Expected dict object generated from `make_step` got {type(o)}\n{o}')

            skip_idx += 1 # Be aware of the ordering here. we want to always show the first step when we can.
            if skip_idx%self.n==0 or o['done']:
                yield o
                if o['done']: skip_idx = 0

# Cell

class NStepCallback(Callback):
    def __init__(self,nsteps=1,nskip=1):
        store_attr()

    def add_nstep_pipes(self,before=None,after=ItemTransformLoop,not_under=None) -> List[dp.iter.IterDataPipe]:
        return L(partial(NSkipPipe,n=self.nskip),
                 partial(NStepPipe, n=self.nsteps),
                 Flattener
               )

# Cell
class DictCollate(Transform):
    def encodes(self,o): return L(o).map(BD).sum()

class DictToTensor(Transform):
    def encodes(self,o:dict):
        for k,v in o.items():
            v = TensorBatch(v)
            if len(v.shape)==0: v = v.unsqueeze(0)
            o[k] = v
        return o

# Cell

class GymTypeTransform(Transform):
    def encodes(self,o): return gym.make(o)

class GymStepTransform(Transform):
    @delegates(Transform)
    def __init__(self,agent=None,**kwargs):
        self.agent = agent
        super().__init__(**kwargs)

    def encodes(self,o:gym.Env):

        if getattr(o,'is_done',True):
            state = o.reset(seed=getattr(self,'seed',0))
            o.is_done = False
            o.sum_reward = 0
            episode_n = getattr(o,'step_info',{'episode_n':-1})['episode_n']+1
            o.step_info = make_step(state,None,False,None,0,None,env_id=id(o),step_n=-1,
                                    episode_n=episode_n)
        else:
            state = o.state

        self.agent.agent_base.iterator.append(o.step_info)
        for action in self.agent:
            next_state,reward,done,_ = o.step(action)
            o.sum_reward += reward

        if done: o.is_done = True
        o.step_info = make_step(state,next_state,done,reward,o.sum_reward,action,env_id=id(o),
                                step_n=o.step_info['step_n']+1,episode_n=o.step_info['episode_n'])

        return o.step_info



# Cell
from torch.nn import *
from torch.optim import *
from fastai.torch_basics import *
from fastai.torch_core import *

class DQN(Module):
    def __init__(self,state_sz:int,action_sz:int,hidden=512):
        self.layers=Sequential(
            Linear(state_sz,hidden),
            ReLU(),
            Linear(hidden,action_sz),
        )
    def forward(self,x): return self.layers(x)

class Agent(dp.iter.IterDataPipe):
    def __init__(self,model,iterator):
        self.model = model
        self.iterator = iterator
        self.agent_base = self

    def __iter__(self):
        while self.iterator:
            yield self.iterator.pop(0)


class RawOutOfStep(dp.iter.IterDataPipe):
    def __init__(self,source_datapipe,agent_base,key):
        self.source_datapipe = source_datapipe
        self.key = key
        self.agent_base = agent_base

    def __iter__(self):
        for o in self.source_datapipe:
            x = tensor(o[self.key])
            x = self.agent_base.model(x)
            yield x

class ArgmaxOfStep(dp.iter.IterDataPipe):
    def __init__(self,source_datapipe,agent_base):
        self.source_datapipe = source_datapipe
        self.agent_base = agent_base

    def __iter__(self):
        for o in self.source_datapipe:
            yield torch.argmax(o)

class ToDiscrete(dp.iter.IterDataPipe):
    def __init__(self,source_datapipe,agent_base):
        self.source_datapipe = source_datapipe
        self.agent_base = agent_base

    def __iter__(self):
        for x in self.source_datapipe:
            if isinstance(x,Tensor):
                if len(x.shape)==0:
                    yield int(x)
                else:
                    yield x.long()
            else:
                raise Exception(f'Cant convert to discrete: {x}')

# Cell
class DiscreteEpsilonRandomSelect(dp.iter.IterDataPipe):
    debug=False
    def __init__(self,source_datapipe,agent_base,n_actions,idx=0,min_epsilon=0.2,max_epsilon=1,max_steps=5000):
        self.n_actions = n_actions
        self.source_datapipe = source_datapipe
        self.agent_base = agent_base
        self.agent_base.epislon_selector = self
        self.min_epsilon = min_epsilon
        self.epsilon = max_epsilon
        self.max_epsilon = max_epsilon
        self.max_steps = max_steps
        self.idx = idx

    def __iter__(self):
        for action in self.source_datapipe:
            mask = np.random.random(size=self.n_actions) < self.epsilon
            rand_actions = np.random.choice(self.n_actions, sum(mask))
            action = action.cpu().detach().numpy().reshape((-1,))
            action[mask] = rand_actions
            action=Tensor(action).long().reshape(-1,1)

            if self.agent_base.model.training:
                self.idx += 1
                self.epsilon = max(self.min_epsilon,self.max_epsilon-self.idx/self.max_steps)
            yield action