# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb.

# %% auto 0
__all__ = ['init_xavier_uniform_weights', 'init_kaiming_normal_weights', 'Critic', 'Actor', 'OrnsteinUhlenbeck',
           'ExplorationComparisonLogger', 'ActionUnbatcher', 'ActionClip', 'DDPGAgent', 'BasicOptStepper',
           'LossCollector', 'SoftTargetUpdater', 'get_target_model', 'CriticLossProcessor', 'ActorLossProcessor',
           'DDPGLearner']

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 3
# Python native modules
import os
from typing import *
from copy import deepcopy
# Third party libs
from fastcore.all import *
import torchdata.datapipes as dp
from torch import nn
import torch
from  torchdata.dataloader2.graph import DataPipe,traverse
import numpy as np
import pandas as pd
from torch.optim import AdamW,Adam
# Local modules
from ..core import *
from ..torch_core import *
from ..pipes.core import *
from ..data.block import *
from ..data.dataloader2 import *
from .core import *
from ..memory.experience_replay import ExperienceReplay
from ..learner.core import *
from ..loggers.core import *

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 7
def init_xavier_uniform_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 8
def init_kaiming_normal_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.kaiming_normal_(m.weight)
        # torch.nn.init.kaiming_normal_(m.bias)
        m.bias.data.fill_(0.01)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 9
class Critic(Module):
    def __init__(
            self,
            state_sz:int,  # The input dim of the state
            action_sz:int, # The input dim of the actions
            hidden1=400,    # Number of neurons connected between the 2 input/output layers
            hidden2=300,    # Number of neurons connected between the 2 input/output layers
            head_layer:Module=nn.Linear, # Output layer
            activition_fn:Module=nn.ReLU # The activation function
        ):
        # TODO: Add batch normalization
        self.action_sz = action_sz
        self.state_sz = state_sz
        self.layers = nn.Sequential(
            nn.Linear(state_sz+action_sz,hidden1),
            activition_fn(),
            nn.Linear(hidden1,hidden2),
            activition_fn(),
            head_layer(hidden2,1),
        )
        self.layers.apply(init_kaiming_normal_weights)

    def forward(
            self,
            x:torch.Tensor # A single tensor of shape [Batch,`state_sz`+`action_sz`]
            # A single tensor of shape [B,1] representing the cumulative value estimate of state+action combinations  
        ) -> torch.Tensor: 
        return self.layers(x)

add_docs(
Critic,
"Takes a single tensor of size [B,`state_sz`+`action_sz`] -> [B,1] outputs a 1d tensor repersenting the value",
forward="""Takes in a single tensor of a state tensor and action tensor and output
 the culative value estimates of that state,action combination"""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 12
class Actor(Module):
    def __init__(self,
                 state_sz:int,  # The input dim of the state
                 action_sz:int, # The output dim of the actions
                 hidden1=400,    # Number of neurons connected between the 2 input/output layers
                 hidden2=300,    # Number of neurons connected between the 2 input/output layers
                 head_layer:Module=nn.Linear, # Output layer
                 activition_fn:Module=nn.ReLU # The activiation function
                ):
        # TODO: Add batch normalization
        self.action_sz = action_sz
        self.state_sz = state_sz
        self.layers = nn.Sequential(
            nn.Linear(state_sz,hidden1),
            activition_fn(),
            nn.Linear(hidden1,hidden2),
            activition_fn(),
            head_layer(hidden2,action_sz),
            nn.Tanh()
        )
        self.layers.apply(init_kaiming_normal_weights)

    def forward(self,x): return self.layers(x)

add_docs(
Actor,
"Takes a single tensor of size [B,`state_sz`] -> [B,`action_sz`] and outputs a tensor of actions.",
forward="""Takes in a state tensor and output
 the actions value mappings"""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 16
class OrnsteinUhlenbeck(dp.iter.IterDataPipe):
	def __init__(
			self, 
			source_datapipe:DataPipe, # a datapipe whose next(source_datapipe) -> `Tensor` 
			action_sz:int, # The action dimension
			mu:float=0., # Used in preturbing continuous actions
			theta:float=0.15, # Used in preturbing continuous actions
			sigma:float=0.2, # Used in preturbing continuous actions
            min_epsilon:float=0.2, # The minimum epsilon to drop to
            # The max/starting epsilon if `epsilon` is None and used for calculating epislon decrease speed.
            max_epsilon:float=1, 
            # Determines how fast the episilon should drop to `min_epsilon`. This should be the number
            # of steps that the agent was run through.
            max_steps:int=100,
            # The starting epsilon which determines how much exploration to do.
			# epislon close to 1 does maximal exploration, while close to 0
			# does very little.
            epsilon:float=None,
            # Based on the `base_agent.model.training`, by default no decrement or step tracking will
            # occur during validation steps.
            decrement_on_val:bool=False,
            # Based on the `base_agent.model.training`, by default random actions will not be attempted
            explore_on_val:bool=False,
            # Also return the original action prior to exploratory noise
            ret_original:bool=False,
            # The device to create the masks one
            device='cpu'
		):
		self.source_datapipe = source_datapipe
		self.min_epsilon = min_epsilon
		self.max_epsilon = max_epsilon
		self.max_steps = max_steps
		self.epsilon = epsilon
		self.decrement_on_val = decrement_on_val
		self.explore_on_val = explore_on_val
		self.ret_original = ret_original
		self.agent_base = None
		self.step = 0
		self.device = torch.device(device)
		self.sigma = sigma
		self.theta = theta
		self.mu = mu
		self.normal_dist = torch.distributions.Normal(0,1)
		self.x = torch.full((action_sz,),1).float().to(device=self.device)
		if not (self.decrement_on_val and self.explore_on_val):
			self.agent_base = find_dp(traverse(self.source_datapipe),AgentBase)

	def __iter__(self):
		for action in self.source_datapipe:
			# TODO: Support tuples of actions also
			if not issubclass(action.__class__,torch.Tensor):
				raise Exception(f'Expected Tensor, got {type(action)}\n{action}')
			if action.dtype not in (torch.float32,torch.float64):
				raise ValueError(f'Expected Tensor of dtype float32,float64, got: {action.dtype} from {self.source_datapipe}')

			if self.decrement_on_val or self.agent_base.model.training:
				self.step+=1
				self.epsilon = max(self.min_epsilon,self.max_epsilon-self.step/self.max_steps)

			# Add a batch dim if missing
			if len(action.shape)==1: action.unsqueeze_(0)

			if self.explore_on_val or self.agent_base.model.training:
				dist = self.normal_dist.sample((len(self.x),)).to(device=self.device)
				self.x += self.theta*(self.mu-self.x)+self.sigma*dist

				if self.ret_original: yield (self.epsilon*self.x+action,action)
				else:                 yield self.epsilon*self.x+action
			else:
				yield action

add_docs(
OrnsteinUhlenbeck,
"""Used for exploration in continuous action domains via temporaly correlated noise.

[1] From https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab

[2] Cumulatively based on [Uhlenbeck et al., 1930](http://www.entsphere.com/pub/pdf/1930%20Uhlenbeck,%20on%20the%20theory%20of%20the%20Brownian%20motion.pdf)"""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 17
class ExplorationComparisonLogger(LoggerBase):
    def __iter__(self):
        for element in self.source_datapipe:
            if isinstance(element,Record) and element.name=='exploration-compare': 
                self.buffer.append(element)
            elif isinstance(element,tuple) and len(element)==2:
                action,original_action = element
                self.buffer.append(
                    Record(
                        'exploration-compare',
                        tuple((action.detach().cpu(),original_action.detach().cpu()))
                    )
                )
            yield element
    
    def show(self,title='Explored Actions vs Original Actions'):
        import plotly.express as px
        import plotly.io as pio
        pio.renderers.default = "plotly_mimetype+notebook_connected"

        action,original_action = zip(*[o.value for o in self.buffer])
        difference = torch.sub(torch.vstack(action),torch.vstack(original_action)).abs()

        fig = px.scatter(
            pd.DataFrame(difference.numpy()),
            title=title,
            labels={
                "index": "N Steps",
                "value": "Difference between Original/Explored Action"
            },
        )
        return fig.show()

add_docs(
ExplorationComparisonLogger,
"""Allows for quickly doing a "what if" on exploration methods by comparing
the actions selected via exploration with the ones chosen by the model.
""",
show="""Shows the absolute difference between explored actions and original actions.
We would expect as the number of steps increase, the difference between the 
explored and original actions would get smaller. In other words, if there is no
exploration, then the explored actions and the original actions should be almost
identical."""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 21
class ActionUnbatcher(dp.iter.IterDataPipe):
    def __init__(self,source_datapipe):
        self.source_datapipe = source_datapipe

    def __iter__(self):
        for action in self.source_datapipe:
            if len(action.shape)==2: yield action.squeeze(0)
            else:                    yield action

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 22
class ActionClip(dp.iter.IterDataPipe):
    def __init__(
        self,
        source_datapipe,
        clip_min:float=-1,
        clip_max:float=1
    ):
        self.source_datapipe = source_datapipe
        self.clip_min = clip_min
        self.clip_max = clip_max

    def __iter__(self):
        for action in self.source_datapipe:
            yield torch.clip(action,self.clip_min,self.clip_max)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 23
def DDPGAgent(
    model:Actor, # The actor to use for mapping states to actions
    # LoggerBases push logs to. If None, logs will be collected and output
    # by the dataloader.
    logger_bases:Optional[LoggerBase]=None, 
    min_epsilon:float=0.2, # The minimum epsilon to drop to
    # The max/starting epsilon if `epsilon` is None and used for calculating epislon decrease speed.
    max_epsilon:float=1, 
    # Determines how fast the episilon should drop to `min_epsilon`. This should be the number
    # of steps that the agent was run through.
    max_steps:int=100,
    # Any augmentations to the DDPG agent.
    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None
)->AgentHead:
    agent_base = AgentBase(model,logger_bases=ifnone(logger_bases,[CacheLoggerBase()]))
    agent = StepFieldSelector(agent_base,field='state')
    agent = InputInjester(agent)
    agent = SimpleModelRunner(agent)
    agent = OrnsteinUhlenbeck(
        agent,
        action_sz=model.action_sz,
        min_epsilon=min_epsilon,max_epsilon=max_epsilon,max_steps=max_steps
    )
    
    agent = ActionClip(agent)
    agent = NumpyConverter(agent)
    agent = ActionUnbatcher(agent)
    agent = AgentHead(agent)
    
    agent = apply_dp_augmentation_fns(agent,dp_augmentation_fns)

    return agent



# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 29
class BasicOptStepper(dp.iter.IterDataPipe):
    def __init__(self,
        # The parent datapipe that should produce a dict of format `{'loss':tensor(...)}`
        # all other types will be passed through.
        source_datapipe:DataPipe, 
        # The model to attach
        model:nn.Module,
        # The learning rate
        lr:float,
        # The optimizer to use
        opt:torch.optim.Optimizer=AdamW,
        # If an input is loss, catch it and prevent it from proceeding to the
        # rest of the pipeline.
        filter:bool=False,
        # Whether to `zero_grad()` before doing backward/step
        do_zero_grad:bool=True,
        # kwargs to be passed to the `opt`
        **opt_kwargs
    ):
        self.source_datapipe = source_datapipe
        self.lr = lr
        self.model = model
        self.opt = opt
        self.opt_kwargs = opt_kwargs
        self.do_zero_grad = do_zero_grad
        self.filter = filter
        self._opt = self.opt(self.model.parameters(),lr=self.lr,**self.opt_kwargs)

    def __iter__(self):
        for x in self.source_datapipe:
            if isinstance(x,dict) and 'loss' in x:
                if self.do_zero_grad: self._opt.zero_grad()
                x['loss'].backward()
                self._opt.step()
                if self.filter: continue 
            yield x

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 30
class LossCollector(LogCollector):
    def __init__(self,
            source_datapipe:DataPipe, # The parent datapipe, likely the one to collect metrics from
            header:str='loss', # Name of the record. Change if using multiple instances.
            # If an input is loss, catch it and prevent it from proceeding to the
            # rest of the pipeline.
            filter:bool=False,
            # By default, LossCollector will search the pipeline for logger bases
            # and attach them here. However we can directly attach them here if
            # we need. This must be a list of lists/queues.
            main_buffers:Optional[List[List]]=None 
        ):
        self.source_datapipe = source_datapipe
        self.main_buffers = main_buffers
        self.header = header
        self.filter = filter
        
    def __iter__(self):
        for x in self.source_datapipe:
            if isinstance(x,dict) and 'loss' in x:
                for q in self.main_buffers: 
                    q.append(Record(self.header,x['loss'].cpu().detach().numpy()))
                if self.filter: continue
            yield x

    def show(self,title='Loss over N-Steps'):
        import plotly.express as px
        import plotly.io as pio
        pio.renderers.default = "plotly_mimetype+notebook_connected"

        losses = {i:[o.value for o in ls] for i,ls in enumerate(self.main_buffers)}

        fig = px.line(
            pd.DataFrame(losses),
            title=title,
            labels={
                "index": "N Steps",
                "value": "Loss"
            },
        )
        return fig.show()

add_docs(
LossCollector,
"""Itercepts dictionary results generated from `source_datapipe` that are in the 
format: `{'loss':tensor(...)}`. All other elements will be ignored and passed through.

If `filter=true`, then intercepted dictionaries will filtered out by this pipe, and will
not be propagated to the rest of the pipeline. 
""",
show="""Shows the loss over n-steps/n-batchs depending on how the loss values are loaded 
into the `main_buffers`. If there is no `LoggerBase`s, then 
`LossCollector(...,main_buffers=[[]],...)` must be passed so that 
losses can be cached for showing."""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 31
class SoftTargetUpdater(dp.iter.IterDataPipe):
    def __init__(
            self,
            source_datapipe,
            model,
            target_sync:int=1,
            tau:int=0.001
        ):
        self.source_datapipe = source_datapipe
        self.model = model
        self.target_model = deepcopy(model)
        self.target_sync = target_sync
        self.tau = tau
        self.n_batch = 0
        
    def __iter__(self):
        for batch in self.source_datapipe:
            yield batch
            if self.n_batch%self.target_sync==0:
                for tp, fp in zip(self.target_model.parameters(), self.model.parameters()):
                    tp.data.copy_(self.tau * fp.data + (1.0 - self.tau) * tp.data)

            self.n_batch+=1


# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 32
def get_target_model(
        model,
        pipe,
        model_cls,
        target_updater_cls=(SoftTargetUpdater,),
        debug:bool=False
    ):
        if model is not None: return model
        target_updaters = []
        for target_updater in target_updater_cls:
            target_updaters.extend(find_dps(traverse(pipe),target_updater))
        if debug: print(target_updaters)
        target_updaters = [o for o in target_updaters if isinstance(o.target_model,model_cls)]
        if debug: print(f'After filtering on {model_cls}: {target_updaters}')
        if len(target_updaters)==0: raise RuntimeError('target_updaters is empty')
        elif len(target_updaters)>1: 
            warn(f'Found multiple target updaters with {model_cls}, {target_updaters}')
        return target_updaters[0].target_model

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 33
class CriticLossProcessor(dp.iter.IterDataPipe):
    debug:bool=False

    def __init__(self,
            source_datapipe:DataPipe, # The parent datapipe that should yield step types
            critic:Critic,
            t_actor:Optional[Actor]=None,
            t_critic:Optional[Critic]=None,
            loss:nn.Module=nn.MSELoss,
            discount:float=0.99,
            nsteps:int=1
        ):
        self.source_datapipe = source_datapipe
        self.critic = critic
        self.t_critic = get_target_model(t_critic,source_datapipe,Critic,debug=self.debug)
        self.t_actor = get_target_model(t_actor,source_datapipe,Actor,debug=self.debug)
        self.loss = loss()
        self.discount = discount
        self.nsteps = nsteps

    def __iter__(self) -> Union[Dict,SimpleStep]:
        for batch in self.source_datapipe:
            done_mask = batch.terminated
            with torch.no_grad():
                t_actions = self.t_actor(batch.next_state)
                q = self.t_critic(torch.hstack((batch.next_state,t_actions)))

            self.critic.zero_grad()
            targets = batch.reward+q*(self.discount**self.nsteps)*(~done_mask)
            pred = self.critic(torch.hstack((batch.state,batch.action)))
            yield {'loss':self.loss(pred,targets)}
            yield batch

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 35
class ActorLossProcessor(dp.iter.IterDataPipe):
    def __init__(self,
            source_datapipe:DataPipe, # The parent datapipe that should yield step types
            critic:Critic,
            actor:Actor,
        ):
        self.source_datapipe = source_datapipe
        self.critic = critic
        self.actor = actor

    def __iter__(self) -> Union[Dict,SimpleStep]:
        for batch in self.source_datapipe:
            self.actor.zero_grad()
            q = self.critic(torch.hstack((batch.state,self.actor(batch.state))))
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 5)

            loss = (-q).mean()
            yield {'loss':loss}
            yield batch

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 37
def DDPGLearner(
    actor,
    critic,
    dls,
    logger_bases=None,
    actor_lr=10e-4,
    actor_opt=Adam,
    critic_lr=10e-3,
    critic_opt=AdamW,
    target_copy_freq=1,
    tau=0.001,
    bs=128,
    max_sz=10000,
    nsteps=1,
    device=None,
    batches=None,
    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None
) -> LearnerHead:
    learner = LearnerBase(actor,dls,batches=batches)
    learner = LoggerBasePassThrough(learner,logger_bases)
    learner = BatchCollector(learner,batch_on_pipe=LearnerBase)
    learner = EpocherCollector(learner)
    for logger_base in L(logger_bases): learner = logger_base.connect_source_datapipe(learner)
    if logger_bases: 
        learner = RollingTerminatedRewardCollector(learner)
        learner = EpisodeCollector(learner)
    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz)
    learner = StepBatcher(learner,device=device)

    learner = SoftTargetUpdater(learner,critic,target_sync=target_copy_freq,tau=tau)
    learner = SoftTargetUpdater(learner,actor,target_sync=target_copy_freq,tau=tau)
    learner = CriticLossProcessor(learner,critic,actor)
    learner = LossCollector(learner,header='critic-loss')
    learner = BasicOptStepper(learner,critic,critic_lr,opt=critic_opt,filter=True,do_zero_grad=False)
    learner = ActorLossProcessor(learner,critic,actor)
    learner = LossCollector(learner,header='actor-loss')
    learner = BasicOptStepper(learner,actor,actor_lr,opt=actor_opt,filter=True,do_zero_grad=False)
    learner = LearnerHead(learner)
    
    learner = apply_dp_augmentation_fns(learner,dp_augmentation_fns)
    
    return learner
