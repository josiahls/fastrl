# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb.

# %% auto 0
__all__ = ['Critic', 'Actor', 'DDPGAgent']

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 3
# Python native modules
import os
from typing import *
# Third party libs
from fastcore.all import *
import torchdata.datapipes as dp
from torch import nn
import torch
from  torchdata.dataloader2.graph import DataPipe
# Local modules
from ..torch_core import *
from ..pipes.core import *
from ..data.block import *
from ..data.dataloader2 import *
from .core import *
from ..learner.core import *
from ..loggers.core import *

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 7
class Critic(Module):
    def __init__(
            self,
            state_sz:int,  # The input dim of the state
            action_sz:int, # The input dim of the actions
            hidden=512,    # Number of neurons connected between the 2 input/output layers
            head_layer:Module=nn.Linear, # Output layer
            activition_fn:Module=nn.ReLU # The activation function
        ):
        # TODO: Add batch normalization
        self.layers = nn.Sequential(
            nn.Linear(state_sz+action_sz,hidden),
            activition_fn(),
            head_layer(hidden,1),
        )
    def forward(
            self,
            x:torch.Tensor # A single tensor of shape [Batch,`state_sz`+`action_sz`]
            # A single tensor of shape [B,1] representing the cumulative value estimate of state+action combinations  
        ) -> torch.Tensor: 
        return self.layers(x)

add_docs(
Critic,
"Takes a single tensor of size [B,`state_sz`+`action_sz`] -> [B,1] outputs a 1d tensor repersenting the value",
forward="""Takes in a single tensor of a state tensor and action tensor and output
 the culative value estimates of that state,action combination"""
)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 10
class Actor(Module):
    def __init__(self,
                 state_sz:int,  # The input dim of the state
                 action_sz:int, # The output dim of the actions
                 hidden=512,    # Number of neurons connected between the 2 input/output layers
                 head_layer:Module=nn.Linear, # Output layer
                 activition_fn:Module=nn.ReLU # The activiation function
                ):
        # TODO: Add batch normalization
        self.layers = nn.Sequential(
            nn.Linear(state_sz,hidden),
            activition_fn(),
            head_layer(hidden,action_sz),
        )
    def forward(self,x): return self.layers(x)

# %% ../../nbs/07_Agents/02_Continuous/12s_agents.ddpg.ipynb 14
def DDPGAgent(
    model,
    logger_bases=None,
    dp_augmentation_fns:Optional[List[DataPipeAugmentationFn]]=None
)->AgentHead:
    agent_base = AgentBase(model,logger_bases=ifnone(logger_bases,[CacheLoggerBase()]))
    agent = StepFieldSelector(agent_base,field='state')
    agent = InputInjester(agent)
    agent = SimpleModelRunner(agent)

    agent = AgentHead(agent)
    
    agent = apply_dp_augmentation_fns(agent,dp_augmentation_fns)

    return agent
