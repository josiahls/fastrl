# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12k_agents.dqn.double.ipynb.

# %% auto 0
__all__ = ['DoubleQCalc', 'DQNLearner']

# %% ../nbs/12k_agents.dqn.double.ipynb 3
# Python native modules
import os
from collections import deque
# Third party libs
from fastcore.all import *
import torchdata.datapipes as dp
from torch.utils.data.dataloader_experimental import DataLoader2
from torch.utils.data.datapipes._typing import _DataPipeMeta, _IterDataPipeMeta
# Local modules
import torch
from torch.nn import *
import torch.nn.functional as F
from torch.optim import *
from fastai.torch_basics import *
from fastai.torch_core import *

from ...core import *
from ..core import *
from ...pipes.core import *
from ...fastai.data.block import *
from ...memory.experience_replay import *
from ..core import *
from ..discrete import *
from ...loggers.core import *
from ...loggers.jupyter_visualizers import *
from ...learner.core import *
from .basic import *

# %% ../nbs/12k_agents.dqn.double.ipynb 6
class DoubleQCalc(dp.iter.IterDataPipe):
    def __init__(self,source_datapipe,discount=0.99,nsteps=1,target_sync=300):
        self.source_datapipe = source_datapipe
        self.discount = discount
        self.nsteps = nsteps
        self.learner = find_pipe_instance(self,LearnerBase)
        self.learner.target_model=deepcopy(self.learner.model)
        self.target_sync = target_sync
        self.n_batch = 0
                
    def __iter__(self):
        for batch in self.source_datapipe:
            try:
                self.learner.done_mask = batch.terminated.reshape(-1,)
                with torch.no_grad():
                    # self.learner.next_q = self.learner.target_model(batch.next_state)
                # self.learner.next_q = self.learner.next_q.max(dim=1).values.reshape(-1,1)
                
                
                    chosen_actions = self.learner.model(batch.next_state).argmax(dim=1).reshape(-1,1)
                    self.learner.next_q = self.learner.target_model(batch.next_state).gather(1,chosen_actions)

                
                self.learner.next_q[self.learner.done_mask] = 0 #xb[done_mask]['reward']
                self.learner.targets = batch.reward+self.learner.next_q*(self.discount**self.nsteps)
                self.learner.pred = self.learner.model(batch.state)

                t_q=self.learner.pred.clone()
                t_q.scatter_(1,batch.action.long(),self.learner.targets)

                self.learner.loss_grad = self.learner.loss_func(self.learner.pred, t_q)
                
                if self.n_batch%self.target_sync==0:
                    self.learner.target_model.load_state_dict(self.learner.model.state_dict())
                self.n_batch+=1
                
                yield batch
            except RuntimeError as e:
                print(f'Failed on batch: {batch}')
                raise

# %% ../nbs/12k_agents.dqn.double.ipynb 7
def DQNLearner(
    model,
    dls,
    logger_bases=None,
    loss_func=MSELoss(),
    opt=AdamW,
    lr=0.005,
    bs=128,
    max_sz=10000,
    nsteps=1
) -> LearnerHead:
    learner = LearnerBase(model,dls,loss_func=MSELoss(),opt=opt(model.parameters(),lr=lr))
    learner = BatchCollector(learner,logger_bases=logger_bases,batch_on_pipe=LearnerBase)
    learner = EpocherCollector(learner,logger_bases=logger_bases)
    for logger_base in logger_bases: learner = logger_base.connect_source_datapipe(learner)
    learner = RollingTerminatedRewardCollector(learner,logger_bases)
    learner = EpisodeCollector(learner,logger_bases)
    learner = ExperienceReplay(learner,bs=bs,max_sz=max_sz,clone_detach=dls[0].num_workers>0)
    learner = StepBatcher(learner)
    learner = DoubleQCalc(learner,nsteps=nsteps)
    learner = ModelLearnCalc(learner)
    learner = LossCollector(learner,logger_bases)
    learner = LearnerHead(learner)
    return learner
