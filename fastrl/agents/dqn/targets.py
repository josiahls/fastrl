# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10b_agents.dqn.targets.ipynb (unless otherwise specified).

__all__ = ['DQNTargetTrainer', 'get_background_foreground', 'encoded_tb_gif2rgb_img', 'tfeventimages2npimage',
           'scalars2value', 'tb_scalar_over_epochs', 'figures_to_html']

# Cell
# Python native modules
import os
from collections import deque
from typing import *
from warnings import warn
import logging
# Third party libs
import torch
from torch.nn import *
from torch import optim
from fastcore.all import *
from fastai.learner import *
from fastai.torch_basics import *
from fastai.torch_core import *
from fastai.optimizer import OptimWrapper
from fastai.callback.all import *
# Local modules
from ...data.block import *
from ...data.gym import *
from ...agent import *
from ...core import *
from .core import *
from ...memory.experience_replay import *

_logger=logging.getLogger()

# Cell
class DQNTargetTrainer(Callback):

    def __init__(self,n_batch=0,target_sync=300,discount=0.99,n_steps=1):
        store_attr()
        self._xb=None

    def before_fit(self):
        self.learn.target_model=deepcopy(self.learn.model.model)
        self.n_batch=0

    def after_pred(self):
        self._xb=self.yb
        self.learn.yb=[]



        self.learn.opt.zero_grad()
        with torch.no_grad():
            s=self.learn.xb['state']
            a=self.learn.xb['action']
            ns=self.xb['next_state']
            r=self.xb['reward']
            d=self.xb['done']


        self.learn.state_action_values = self.learn.model.model(s).gather(1,a).squeeze(-1)
        with torch.no_grad():
            self.learn.next_state_values = self.target_model(ns).max(1)[0]
            self.learn.next_state_values[d.squeeze(-1)] = 0.0

            self.learn.expected_state_action_values = self.learn.next_state_values.detach() * (self.discount**self.n_steps) + r.squeeze(-1)
        self.learn.loss= nn.MSELoss()(self.learn.state_action_values,self.learn.expected_state_action_values)

        if (self.n_batch-1)%self.target_sync==0:
            print('The loss should be practically zero: ',self.loss)
            print(self.learn.state_action_values-self.learn.expected_state_action_values)

        # raise Exception
        self.learn.loss.backward()
        self.learn.opt.step()
#         self.learn.batch_targets = torch.cat([calc_target(self.learn.model.model, r, ns.cpu().numpy(),d)
#                          for r,ns,d in zip(self.learn.xb['reward'],self.learn.xb['next_state'],self.learn.xb['done'])])

#         self.learn.opt.zero_grad()
#         self.learn.states_v = self.xb['state'].to(default_device()).float()
#         self.learn.net_q_v = self.learn.model.model(self.learn.states_v)
#         # print(net_q_v)
#         self.learn.target_q = self.learn.net_q_v.cpu().data.numpy().copy()

#         # print(batch_targets,target_q)
#         self.learn.target_q[range(self.learn.xb.bs()), self.xb['action'].cpu()] = self.learn.batch_targets.cpu()
#         self.learn.target_q_v = torch.tensor(self.learn.target_q)
#         # print(net_q_v, target_q_v)
#         loss_v = self.learn.loss_func(self.learn.net_q_v.cpu(), self.learn.target_q_v.cpu())
#         loss_v.backward()
#         self.learn.loss=loss_v.cpu()
#         # print(loss_v)
#         self.learn.opt.step()

#         self.learn.yb=self.xb
#         self.learn.xb=self.xb[0]
#         self._xb=({k:v.clone() for k,v in self.xb.items()},)
#         self.learn.done_mask=self.xb['done'].reshape(-1,)
#         self.learn.next_q=self.target_model(self.xb['next_state']).max(dim=1).values.reshape(-1,1)
#         self.learn.next_q[self.done_mask]=1
#         self.learn.targets=self.xb['reward']+self.learn.next_q*(self.discount**self.n_steps)
#         self.learn.pred=self.learn.model.model(self.xb['state'])

#         t_q=self.pred.clone()
#         t_q.scatter_(1,self.xb['action'],self.targets)
#         self.learn.yb=(t_q,)
        with torch.no_grad():
            self.learn.td_error=(self.learn.state_action_values.cpu()-self.learn.expected_state_action_values.cpu()).reshape(-1,1)**2

    def before_backward(self): self.learn.yb=self._xb

    def after_batch(self):
        if self.n_batch%self.target_sync==0:
            self.target_model.load_state_dict(self.learn.model.state_dict())
            # if self.n_batch>1:raise Exception
        self.n_batch+=1

# Cell
def get_background_foreground(images,foreground=False,remove_empty=True,background_hint=255):
    "Gets the `get_background_foreground`"
    stacked_imgs=np.stack([o for o in images],axis=3)

    # apply_along_axis
    def get_unique_px(p):
        unique_vals,counts=np.unique(p,return_counts=True)
        count=(counts.min() if foreground else counts.max())
        mode=unique_vals[counts.argmin() if foreground else counts.argmax()]

        if foreground:
            if mode==background_hint and len(unique_vals)>1:
                unique_vals=unique_vals[unique_vals!=mode]
                counts=counts[counts!=count]

                count=(counts.min() if foreground else counts.max())
                mode=unique_vals[counts.argmin() if foreground else counts.argmax()]

        return mode

    if not remove_empty or not foreground:
        return np.apply_along_axis(get_unique_px, 3,stacked_imgs)
    elif remove_empty and foreground:
        foreground=False
        background=np.apply_along_axis(get_unique_px, 3,stacked_imgs)
        foreground=True
        stacked_imgs=np.stack(images+[background],axis=3)
        return np.apply_along_axis(get_unique_px, 3,stacked_imgs)

# Cell
def encoded_tb_gif2rgb_img(img):
    return Image.open(BytesIO(img.encoded_image_string)).convert('RGB')

def tfeventimages2npimage(tag:str, # The event tag to load the images from.
                          ea:event_accumulator.EventAccumulator, # The event accumulator to load the images
                          start=0, # Where in the list of images to start loading
                          end:Optional[int]=None, # Optionally where to stop loading images
                          step=1 # Number of steps betwee image reads
                         )->np.array:
    "Takes `start`->`end` images from `tag` and merges them into a composite np image."
    imgs=ea.Images(tag)
    if not imgs:
        warn(f'There are no images in {tag}')
        return None

    images=[]
    np_img=np.array(encoded_tb_gif2rgb_img(imgs[0]))

    for i in range(start+1,ifnone(end,len(imgs)),step):
        images.append(np_img)
        try:               img=imgs[i]
        except IndexError: break
        np_img=np.array(encoded_tb_gif2rgb_img(img)).copy()

    background=get_background_foreground(images,foreground=True)

    return images,(np_img,background)

# Cell

# export
from IPython.display import HTML
import plotly.express as px
from plotly.express._core import configure_animation_controls
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.graph_objects import layout
from math import ceil

# Cell
import plotly.express as px
from fnmatch import fnmatch


def scalars2value(scalars:list): return list(map(lambda o:o.value,scalars))

def tb_scalar_over_epochs(
                          tag_pattern, # Tags that will match this pattern to be displayed.
                          ea:event_accumulator.EventAccumulator, # The event accumulator to load the images
                          start=0, # Where in the list of images to start loading
                          end:Optional[int]=None, # Optionally where to stop loading images
                          step=1 # Number of steps betwee image reads
        ):
    tags=[t for t in ea.Tags()['scalars'] if fnmatch(t,tag_pattern)]
    if not tags: warn(f'There are not tags with pattern {tag_pattern}, there are: \n{ea.Tags()["scalars"]}')
    # slice the scalars
    sliced={t:scalars2value(ea.Scalars(t)[start:end:step]) for t in tags}
    # Create the data frame
    df=pd.DataFrame(
        data={'values':np.array(list(sliced.values())).reshape(-1,),
              'tags':np.array([[k]*len(v) for k,v in sliced.items()]).reshape(-1,),
              'steps':np.array([np.arange(len(v)) for v in sliced.values()]).reshape(-1,)}
    )

    value_array=np.array(list(sliced.values()))
    return px.line(data_frame=df,x='steps',y='values',animation_frame="tags",
            range_y=[value_array.min(),value_array.max()])

# Cell
def figures_to_html(figs):
    html="<html><head></head><body>" + "\n"
    for fig in figs: html+=fig.to_html().split('<body>')[1].split('</body>')[0]
    html+="</body></html>" + "\n"
    display(HTML(html))