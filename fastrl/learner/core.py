# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/06_Learning/10a_learner.core.ipynb.

# %% auto 0
__all__ = ['LearnerBase', 'LearnerHead', 'StepBatcher']

# %% ../../nbs/06_Learning/10a_learner.core.ipynb 2
# Python native modules
import os
from contextlib import contextmanager
from typing import List,Union,Dict,Optional,Iterable,Tuple
# Third party libs
from fastcore.all import add_docs
import torchdata.datapipes as dp
from torchdata.dataloader2.graph import list_dps 
import torch
from torch import nn
from torchdata.dataloader2 import DataLoader2
from torchdata.dataloader2.graph import traverse_dps,DataPipeGraph,DataPipe
# Local modules
from ..torch_core import evaluating
from ..pipes.core import find_dp
from ..loggers.core import Record,EpochCollector,BatchCollector

# %% ../../nbs/06_Learning/10a_learner.core.ipynb 4
class LearnerBase(dp.iter.IterDataPipe):
    def __init__(self,
            # The base NN that we getting raw action values out of.
            # This can either be a `nn.Module` or a dict of multiple `nn.Module`s
            # For multimodel training
            model:Union[nn.Module,Dict[str,nn.Module]], 
            # The dataloaders to read data from for training. This can be a single
            # DataLoader2 or an iterable that yields from a DataLoader2.
            dls:Union[DataLoader2,Iterable], 
            # By default for reinforcement learning, we want to keep the workers
            # alive so that simluations are not being shutdown / restarted.
            # Epochs are expected to be handled semantically via tracking the number 
            # of batches.
            infinite_dls:bool=True
    ):
        self.model = model
        self.iterable = dls
        self.learner_base = self
        self.infinite_dls = infinite_dls
        self._dls = None
        self._ended = False

    def __getstate__(self):
        state = {k:v for k,v in self.__dict__.items() if k not in ['_dls']}
        # TODO: Needs a better way to serialize / deserialize states.
        # state['iterable'] = [d.state_dict() for d in state['iterable']]
        if dp.iter.IterDataPipe.getstate_hook is not None:
            return dp.iter.IterDataPipe.getstate_hook(state)

        return state

    def __setstate__(self, state):
        # state['iterable'] = [d.from_state_dict() for d in state['iterable']]
        for k,v in state.items():
            setattr(self,k,v)

    def end(self):
        self._ended = True
   
    def __iter__(self):
        self._ended = False
        for data in self.iterable:
            if self._ended:
                break
            yield data

add_docs(
LearnerBase,
"Combines models,dataloaders, and optimizers together for running a training pipeline.",
reset="""If `infinite_dls` is false, then all dls will be reset, otherwise they will be
kept alive.""",
end="When called, will cause the Learner to stop iterating and cleanup."
)

# %% ../../nbs/06_Learning/10a_learner.core.ipynb 5
class LearnerHead(dp.iter.IterDataPipe):
    def __init__(
            self,
            source_datapipes:Tuple[dp.iter.IterDataPipe],
            model
        ):
        if not isinstance(source_datapipes,tuple):
            self.source_datapipes = (source_datapipes,)
        else:
            self.source_datapipes = source_datapipes
        self.dp_idx = 0
        self.model = model

    def __iter__(self): yield from self.source_datapipes[self.dp_idx]
    
    def fit(self,epochs):
        self.dp_idx = 0
        epocher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),EpochCollector)
        epocher.epochs = epochs
        self.model.train()
        for _ in self: pass

    def validate(self,epochs=1,batches=100,show=True) -> DataPipe:
        self.dp_idx = 1
        epocher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),EpochCollector)
        epocher.epochs = epochs
        batcher = find_dp(traverse_dps(self.source_datapipes[self.dp_idx]),BatchCollector)
        batcher.batches = batches
        with evaluating(self.model):
            for _ in self: pass
            if show:
                pipes = list_dps(traverse_dps(self.source_datapipes[self.dp_idx]))
                for pipe in pipes:
                    if hasattr(pipe,'show'):
                        return pipe.show() 
        
add_docs(
LearnerHead,
"""
""",
fit="Runs the `LearnerHead` pipeline for `epochs`",
validate="""If there is more than 1 dl, then run 1 epoch of that dl based on 
`dl_idx` and returns the original datapipe for displaying."""
)  

# %% ../../nbs/06_Learning/10a_learner.core.ipynb 18
class StepBatcher(dp.iter.IterDataPipe):
    def __init__(self,
            source_datapipe,
            device=None
        ):
        self.source_datapipe = source_datapipe
        self.device = device
        
    def vstack_by_fld(self,batch,fld):
        try:
            t = torch.vstack(tuple(getattr(step,fld) for step in batch))
            if self.device is not None:
                t = t.to(torch.device(self.device))
            t.requires_grad = False
            return t
        except RuntimeError as e:
            print(f'Failed to stack {fld} given batch: {batch}')
            raise
        
    def __iter__(self):
        for batch in self.source_datapipe:
            cls = batch[0].__class__
            batched_step = cls(**{fld:self.vstack_by_fld(batch,fld) for fld in cls._fields})
            yield batched_step 

add_docs(
StepBatcher,
"Converts multiple `StepType` into a single `StepType` with the fields concated.",
vstack_by_fld="vstacks a `fld` in `batch`"
)
