# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_DataLoading/00_core.ipynb.

# %% auto 0
__all__ = ['dataloaders']

# %% ../../nbs/02_DataLoading/00_core.ipynb 2
# Python native modules
from typing import Tuple,Union,List
# Third party libs
import torchdata.datapipes as dp
from torchdata.dataloader2 import MultiProcessingReadingService,DataLoader2
from fastcore.all import delegates
# Local modules

# %% ../../nbs/02_DataLoading/00_core.ipynb 4
@delegates(MultiProcessingReadingService)
def dataloaders(
    # A tuple of iterable datapipes to generate dataloaders from.
    pipes:Union[Tuple[dp.iter.IterDataPipe],dp.iter.IterDataPipe],
    # Concat the dataloaders together
    do_concat:bool = False,
    # Multiplex the dataloaders
    do_multiplex:bool = False,
    # Number of workers the dataloaders should run in
    num_workers: int = 0,
    **kwargs
) -> Union[dp.iter.IterDataPipe,List[dp.iter.IterDataPipe]]:
    "Function that creates dataloaders based on `pipes` with different ways of combing them."
    if not isinstance(pipes,tuple):
        pipes = (pipes,)

    dls = []
    for pipe in pipes:
        dl = DataLoader2(
            datapipe=pipe,
            reading_service=MultiProcessingReadingService(
                num_workers = num_workers,
                **kwargs
            ) if num_workers > 0 else None
        )
        dl = dp.iter.IterableWrapper(dl,deepcopy=False)
        dls.append(dl)
    #TODO(josiahls): Not sure if this is needed tbh.. Might be better to just
    # return dls, and have the user wrap them if they want. Then try can do more complex stuff.
    if do_concat:
        return dp.iter.Concater(*dls)
    elif do_multiplex:
        return dp.iter.Multiplexer(*dls)
    else:
        return dls

